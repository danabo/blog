<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dan&#39;s Notepad</title>
    <link>https://danabo.github.io/blog/</link>
    <description>Recent content on Dan&#39;s Notepad</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Â©2021 Daniel Abolafia.</copyright>
    <lastBuildDate>Fri, 14 May 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://danabo.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Physical Information</title>
      <link>https://danabo.github.io/blog/posts/physical-information/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/physical-information/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\es}{\emptyset}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\N}{\mb{N}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\I}{\mb{I}}&lt;br&gt;
\newcommand{\H}{\mb{H}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\Env}{\mf{E}}&lt;br&gt;
\newcommand{\expt}[2]{\mb{E}_{#1}\left[#2\right]}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[1]{_{\mid #1}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\Dt}{{\Delta t}}&lt;br&gt;
\newcommand{\tr}{\rightarrowtail}&lt;br&gt;
\newcommand{\tx}{\prec}&lt;br&gt;
\newcommand{\qed}{\ \ \blacksquare}&lt;br&gt;
\newcommand{\c}{\overline}&lt;br&gt;
\newcommand{\A}{\mf{A}}&lt;br&gt;
\newcommand{\cA}{\c{\mf{A}}}&lt;br&gt;
\newcommand{\dg}{\dagger}&lt;br&gt;
\newcommand{\lgfr}[2]{\lg\par{\frac{#1}{#2}}}&lt;br&gt;
\newcommand{\rv}{\boldsymbol}&lt;br&gt;
\require{cancel}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\newcommand{\sys}[2]{\left[#2\right]_{#1}}$&lt;/p&gt;
&lt;p&gt;I will apply to abstract physics the same information algebra I introduced in &lt;a href=&#34;https://danabo.github.io/blog/posts/bayesian-information-theory/#defining-information&#34;&gt;Bayesian information theory#defining-information&lt;/a&gt; and further developed in &lt;a href=&#34;https://danabo.github.io/blog/posts/information-algebra/&#34;&gt;Information Algebra&lt;/a&gt;. &lt;em&gt;Bayesian&lt;/em&gt; information is just information from the perspective of an agent that may have or not have particular information. Below, I will introduce the notion of a physical system having or not having information about itself or other systems (whether or not it has &lt;em&gt;agenty&lt;/em&gt; attributes), and the same information algebra will apply. The only difference is a shift from the 1st person to 3rd person perspective.&lt;/p&gt;
&lt;h1 id=&#34;information-preliminaries&#34;&gt;Information Preliminaries&lt;/h1&gt;
&lt;p&gt;For sets $\O$ and $A \subseteq \O$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\O\tr A&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;is&lt;/em&gt; information. This denotes the narrowing down of possibility space $\O$ to possibility space $A$ containing the &lt;em&gt;true&lt;/em&gt; possibility $\o^*\in A$.&lt;/p&gt;
&lt;p&gt;The information $\O\tr A$ implies a domain restriction. For some other set $B \subseteq \O$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
B\dom{A} \df B \cap A&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is the domain restriction operation on $B$, which makes clear which set is the domain and which set is being restricted.&lt;/p&gt;
&lt;p&gt;Let $\mf{P}$ be a partition of $\O$. Then&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mf{P}\dom{A} &amp;amp;\df \set{p\dom{A} \mid p\in\mf{P}} \\&lt;br&gt;
&amp;amp;= \set{p\cap A \mid p\in\mf{P}}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is the domain restriction of partition $\mf{P}$ to domain $A$, s.t. $\bigcup\mf{P} = A$.&lt;/p&gt;
&lt;h1 id=&#34;information-theory-of-systems&#34;&gt;Information Theory Of Systems&lt;/h1&gt;
&lt;p&gt;I will use the abstraction of physics that I introduced in &lt;a href=&#34;https://danabo.github.io/blog/posts/causality-for-physics/#abstract-physics&#34;&gt;Causality For Physics#abstract-physics&lt;/a&gt;. Let $\O$ be a set of possible states and $\t_\Dt : \O\to\O$, $\Dt\in\R$, be a family of bijective time-evolution functions on $\O$. In general, time-evolution forms the group $(\set{\t_\Dt \mid \Dt\in\R}, \circ)$, where $\t_{\Dt+\Dt&#39;} = \t_\Dt\circ\t_{\Dt&#39;}$ and $\t_{-\Dt}=\t^{-1}_\Dt$, and $\t_0:\o\mapsto\o$ is the identity function.&lt;/p&gt;
&lt;p&gt;I will regard $\O$ as the state-space of an entire universe (i.e. a closed system). The universe may contain any number of systems labeled &amp;ldquo;A&amp;rdquo;, &amp;ldquo;B&amp;rdquo;, &amp;ldquo;C&amp;rdquo;, etc., with respective state-spaces $A, B, C,\dots$, so that $\O\subseteq A\m B\m C\m \dots$ and states are tuples, $\o = (a, b, c, \dots) \in \O$. Then the time-evolution function&lt;/p&gt;
&lt;p&gt;$$\t_\Dt : (a, b, c, \dots) \mapsto \t_\Dt(a, b, c, \dots)$$&lt;/p&gt;
&lt;p&gt;jointly time-evolves all the systems simultaneously, which allows $\t_\Dt$ to incorporate arbitrary interactions between systems. This also means that the time evolution of, say, system A, depends on not just A&amp;rsquo;s state, but the state of all systems, i.e. the universe&amp;rsquo;s state $\o$.&lt;/p&gt;
&lt;p&gt;There is an alternative way to describe systems using partitions. Let $\mf{A}, \mf{B}, \mf{C},\dots$ each be a partition on $\O$. Partition $\mc{A}$ is a representation of system A&amp;rsquo;s state space, partition $\mf{B}$ is a representation of system B&amp;rsquo;s state space, and so on. I&amp;rsquo;ll denote elements of a partition with lowercase letters, e.g. $a\in\mf{A},\ b\in\mf{B},\ c\in\mf{C},\ \dots$&lt;/p&gt;
&lt;p&gt;In the state-space view, $a\in A$ is a featureless element of which universal states $\o$ are composed. In the partition view, on the other hand, $a\in\mf{A}$ is a subset of $\O$, corresponding to all the states of the universe that are indistinguishable to system A, i.e. the set of all universal states $\o\in\O$ for which system A is in the same state $a$. You can think of $\mf{A}$ as the set of equivalence classes for the relation &amp;ldquo;same state from system A&amp;rsquo;s perspective&amp;rdquo;. Let $\sys{\mf{A}}{\o}$ be the equivalence class containing $\o$, i.e. $\sys{\mf{A}}{\o} = a\in\mf{A}$ s.t. $\o\in a$.&lt;/p&gt;
&lt;p&gt;From here on I will treat the states of systems as subsets of $\O$, and the state spaces of systems will be partitions of $\O$.&lt;/p&gt;
&lt;h2 id=&#34;systems-have-information&#34;&gt;Systems Have Information&lt;/h2&gt;
&lt;p&gt;Suppose the universe is in state $\o\up{t}\in\O$ at time $t$. Then system A, with state space $\mf{A}$, is in state $a\up{t} = \sys{\mf{A}}{\o\up{t}}$. From system A&amp;rsquo;s perspective, $a\up{t}$ is the set of states the universe can be in.&lt;/p&gt;
&lt;p&gt;System A has the information $\O \tr a\up{t}$, which reads &amp;ldquo;$\O$ is narrowed down to $a\up{t}$.&amp;rdquo; System A possesses this information in a purely physical sense. System A need not have awareness or understanding that it posses information, or even the capacity for awareness or understanding of anything. Merely as a physical description, I define any system with state space $\mf{A}$ to have the information $\O \tr \sys{\mf{A}}{\o\up{t}}$ at time $t$.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514153113.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
As a corollary, the universe has the information $\O\tr\set{\o\up{t}}$ at time $t$. The universe can be viewed as the &lt;em&gt;supersystem&lt;/em&gt; with state space $\mf{O} = \set{\set{\o} \mid \o\in\O}$, the singleton partition. In this sense, the universe has total information, corresponding to narrowing down to a single state.&lt;/p&gt;
&lt;p&gt;This definition of what it means for a system to have information will allow us to talk about what information the system has about itself and other systems at various times, as well as the information the system gains or losses about them as time evolves.&lt;/p&gt;
&lt;h2 id=&#34;interactions&#34;&gt;Interactions&lt;/h2&gt;
&lt;p&gt;If $a\up{t}$ is the state of system A at time $t$, then $\t_{\Dt}(a\up{t})$ is NOT the time-evolution of system A&amp;rsquo;s state. A&amp;rsquo;s state at time $t+\Dt$ is $a\up{t+\Dt}=\sys{\mf{A}}{\o\up{t+\Dt}}$ where $\o\up{t+\Dt} = \t_\Dt(\o\up{t})$. If $\t_{\Dt}(a\up{t}) \neq a\up{t+\Dt}$, then system A &lt;strong&gt;interacted&lt;/strong&gt; with another system (or the environment) in the time interval $(t, t+\Dt)$. Note that by necessity, $\o\up{t+\Dt} \in \t_{\Dt}(a\up{t})\cap a\up{t+\Dt}$.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514153146.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
Then what is $\t_{\Dt}(a\up{t})$? At time $t$, system A has information not just about time $t$, but about all other points in time. Specifically, at time $t$, system A has the information $\O\tr \t_{\Dt}(a\up{t})$ about time $t+\Dt$. It is important to distinguish between the time when a system has information and the time it has information about. So at time $t+\Dt$, system A has information $\O\tr \t_{-\Dt}(a\up{t+\Dt})$ about time $t$.&lt;/p&gt;
&lt;p&gt;If $\t_{\Dt}(a\up{t}) \neq a\up{t+\Dt}$, then system A does not have complete information about its own future state, which is the necessary result of interaction. Furthermore, $\t_{\Dt}(a\up{t}) \neq a\up{t+\Dt} \iff \t_{-\Dt}(a\up{t+\Dt}) \neq a\up{t}$, and so after system A interacted, it has forgotten information about its previous state at time $t$. That is to say, interaction causes a system to lose information about its past.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514153301.png&#34; alt=&#34;&#34;&gt;
&lt;/p&gt;
&lt;h1 id=&#34;conservation-of-information&#34;&gt;Conservation Of Information&lt;/h1&gt;
&lt;p&gt;Recapping &lt;a href=&#34;https://danabo.github.io/blog/posts/information-algebra/&#34;&gt;Information Algebra&lt;/a&gt;, suppose we are given some measure $\mu$ on $\O$. This measure need not be normalized. Then for measureable set $R\subseteq\O$, the quantity of the information $\O\tr R$ is given by&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
h(\O\tr R) = h(R) = \lg\par{\frac{\mu(\O)}{\mu(R)}}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $h(R)$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Information_content&#34;target=&#34;_blank&#34;&gt;information content&lt;/a&gt; (or pointwise entropy) of $R$, and $h(\O\tr R)$ is my own shorthand notation to make it clear what information is being quantified.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conservation of information&lt;/strong&gt; is the property of any bijective time-evolution, whereby the information $\O\tr \t_\Dt(a\up{t})$ is enough to recover the information $\O\tr a\up{t}$, because $a\up{t} = \t^{-1}_\Dt(\t_\Dt(a\up{t}))$, for all $\Dt\in\R$. That is to say, time-evolution of arbitrary state sets $R\subseteq\O$ does not destroy the information $\O\tr R$ (this is distinct from the time-evolution of systems which, as we saw, can lose information).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conservation of information quantity&lt;/strong&gt; is a property of &lt;strong&gt;measure-preserving&lt;/strong&gt; time-evolution. Let $\mu$ additionally be a $\t_\Dt$-&lt;a href=&#34;https://en.wikipedia.org/wiki/Invariant_measure&#34;target=&#34;_blank&#34;&gt;invariant measure&lt;/a&gt; on $\O$, i.e. $\mu(\t_\Dt^{-1}(A)) = \mu(A)$ for all measurable $A\subseteq \O$ and for all $\Dt\in\R$. Because $\t_\Dt$ is bijective, &lt;a href=&#34;https://encyclopediaofmath.org/wiki/Invariant_measure&#34;target=&#34;_blank&#34;&gt;this is equivalent to&lt;/a&gt; requiring $\mu(\t_\Dt(A)) = \mu(A)$. Then $h(\O\tr a\up{t}) = h(\O\tr \t_\Dt(a\up{t}))$ for all $t, \Dt\in\R$.&lt;/p&gt;
&lt;p&gt;Conservation of information quantity is a stronger property that requires a $\t$-invariant measure in addition to bijective time-evolution. In classical mechanics, &lt;a href=&#34;https://en.wikipedia.org/wiki/Liouville%27s_theorem_%28Hamiltonian%29&#34;target=&#34;_blank&#34;&gt;Liouville&amp;rsquo;s theorem&lt;/a&gt; shows that any Newtonian time-evolution preserves uniform measures on phase space. This result is sometimes referred to as &lt;em&gt;conservation of information&lt;/em&gt; by physicists, who are referring to conservation of information quantity within my nomenclature. For details, see&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://physicstravelguide.com/theorems/liouvilles_theorem&#34;target=&#34;_blank&#34;&gt;Liouville&amp;rsquo;s theorem - physicstravelguide.com&lt;/a&gt; (Davis &amp;amp; Schwichtenberg)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://theoreticalminimum.com/courses/statistical-mechanics/2013/spring/lecture-1&#34;target=&#34;_blank&#34;&gt;Entropy and conservation of information - theoreticalminimum.com&lt;/a&gt; - (Susskind)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://philsci-archive.pitt.edu/15985/1/gibbsliouville.pdf&#34;target=&#34;_blank&#34;&gt;On the Gibbs-Liouville theorem in classical mechanics&lt;/a&gt; (Henriksson)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.11569&#34;target=&#34;_blank&#34;&gt;Hamiltonian mechanics is conservation of information entropy&lt;/a&gt; (Carcassi &amp;amp; Aidala)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;information-about-systems&#34;&gt;Information About Systems&lt;/h1&gt;
&lt;p&gt;Suppose there are at least two systems, A and B, with respective state spaces $\mf{A}$ and $\mf{B}$. Let system A be in state $a\up{t}$ at time $t$. System A (at $t$) has the information $\O\tr \t_{\Dt}(a\up{t})$ about time $t+\Dt$. What information does A have about B&amp;rsquo;s state at time $t+\Dt$?&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://danabo.github.io/blog/posts/information-algebra/#mutual-information&#34;&gt;Information Algebra#mutual-information&lt;/a&gt;, I showed that pointwise mutual information (PMI) quantifies &amp;ldquo;information about&amp;rdquo;. Specifically,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
i(a\up{t}, b) = i(b, a\up{t}) = \lg\par{\frac{\mu(a\up{t}\cap b)\mu(\O)}{\mu(a\up{t})\mu(b)}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;quantifies how much information system A, being in state $a\up{t}$, has about whether system B is in state $b\in\mf{B}$ at time $t$.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514153829.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
If $i(a\up{t}, b) = 0$, then $a\up{t}$ and $b$ are &lt;strong&gt;orthogonal&lt;/strong&gt; states. State spaces $\mf{A}$ and $\mf{B}$ are called orthogonal iff $i(a,b) = 0$ for all $a\in\mf{A}$ and $b\in\mf{B}$. Usually, systems are defined to be orthogonal, meaning that the state of one system can be chosen totally independently of the state of the other. Non-orthogonal systems have information about each other &lt;em&gt;by construction&lt;/em&gt;, regardless of their mutual time-evolution. On the other hand, orthogonal systems always have zero information about each other at the present moment. That is to say, $i(a\up{t}, b) = 0$ for all $\mf{B}$, i.e. system A has no information about the state of system B at time $t$. However, system A may have information about system B&amp;rsquo;s past or future state.&lt;/p&gt;
&lt;p&gt;$\O\tr \t_\Dt(a\up{t})$ is the information system A has &lt;em&gt;at time $t$&lt;/em&gt; about time $t+\Dt$. Then it follows that $i(\t_\Dt(a\up{t}), b)$ is the quantity of information system A has, &lt;em&gt;at time $t$&lt;/em&gt;, about whether system B is in state $b$ &lt;em&gt;at time $t+\Dt$&lt;/em&gt;. If $\t_\Dt(a\up{t}) \in b$, then $i(\t_\Dt(a\up{t}), b) = h(b) = h(\O\tr b)$ and system A (at time $t$) has certainty that system B is in state $b$ at time $t+\Dt$. On the other hand, if $b \in \t_\Dt(a\up{t})$, then $i(\t_\Dt(a\up{t}), b) = h(\t_\Dt(a\up{t})) = h(\O\tr \t_\Dt(a\up{t}))$ which is just the total quantity of information that system A has about time $t+\Dt$.&lt;/p&gt;
&lt;p&gt;To fully lay out what information system A has (at time $t$) about system B&amp;rsquo;s state at time $t+\Dt$, we need to look at all the quantities of information A has about every state $b\in\mf{B}$, which can be given as the vector&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\vtup{i(\t_\Dt(a\up{t}), b) \mid b\in\mf{B}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Note that $i(\t_\Dt(a\up{t}), b)$ can be negative (and has no lower bound), which means that system A has needs more quantity of information than $h(\O\tr b)$ to have certainty that system B is in state $b$. If system A has information that rules out state $b$ with certainty, i.e. $b\cap \t_\Dt(a\up{t}) = \es$, then $i(\t_\Dt(a\up{t}), b) = -\infty$. &lt;a href=&#34;https://danabo.github.io/blog/posts/information-algebra/#mutual-information&#34;&gt;Information Algebra#mutual-information&lt;/a&gt; goes into further detail about the interpretation of PMI quantities.&lt;/p&gt;
&lt;h2 id=&#34;information-gain&#34;&gt;Information Gain&lt;/h2&gt;
&lt;p&gt;The difference&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
i(\t_{t&#39;-t-\Dt}(a\up{t+\Dt}), b) - i(\t_{t&#39;-t}(a\up{t}), b)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is &lt;strong&gt;information gain&lt;/strong&gt;, i.e. the information system A gained (or lost if negative) about whether system B is in state $b$ at time $t&#39;$ due to the time-evolution of system A&amp;rsquo;s state over the time interval $(t,t+\Dt)$.&lt;/p&gt;
&lt;p&gt;Likewise, $i(\t_{t&#39;-t-\Dt}(a\up{t+\Dt}), a) - i(\t_{t&#39;-t}(a\up{t}), a)$ is system A&amp;rsquo;s information gain (or loss) about itself, specifically whether it is in state $a$ at time $t&#39;$.&lt;/p&gt;
&lt;h1 id=&#34;sum-conservation-laws&#34;&gt;Sum-Conservation Laws&lt;/h1&gt;
&lt;p&gt;Let $\O\tr R$ be the information some unspecified system at some unspecified time has about some other time. Then that system has the quantity of information $h(\O\tr R)$.&lt;/p&gt;
&lt;p&gt;Let $b\in\mf{B}$ be a possible state of system B. Suppose $i(b, R) &amp;lt; h(\O\tr R)$. Then you might ask, &amp;ldquo;where did the remaining information go?&amp;rdquo; That is to say, does it make sense to think that the complete quantity of information $h(\O\tr R)$ should be divided among information about various things? Could we then have the information about everything add up to $h(\O\tr R)$, analogous to how conservation of mass and energy in physics results in reactions or interactions s.t. the energies and masses of the outputs add up to the input energy and mass?&lt;/p&gt;
&lt;p&gt;Information does not quite work like this. For instance, the sum of the vector $\vtup{i(\t_\Dt(a\up{t}), b) \mid b\in\mf{B}}$ need not be $h(R)$. This is evident when you consider that it&amp;rsquo;s possible for the system with information $\O\tr R$ to have the information quantity $h(R)$ about $b\in\mf{B}$ and about $b&#39;\in\mf{B}$, if both $b$ and $b&#39;$ are contained within $R$. Then at least two entries in the vector are each $h(R)$. Furthermore, entries can be arbitrarily negative, and even $-\infty$ as we saw above.&lt;/p&gt;
&lt;p&gt;I have found two ways to achieve something like a sum-conservation law for information quantity. One way uses pointwise quantities, and the other way uses expected quantities, i.e. entropy and mutual information.&lt;/p&gt;
&lt;h2 id=&#34;pointwise&#34;&gt;Pointwise&lt;/h2&gt;
&lt;p&gt;Suppose $i(b, R) &amp;lt; h(\O\tr R)$. If we &amp;ldquo;shrank&amp;rdquo; $b$ down by intersecting with some other set $c$ such that $b \cap c \in R$, then we&amp;rsquo;d have  $i(b\cap c, R) = h(\O\tr R)$. Writing $i(b\cap c, R)$ as a sum involving $i(b, R)$ gives us a sum-conservation law:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
i(b \cap c, R) &amp;amp;= i(b, R) + i(c, R) - i(b, c, R) \\&lt;br&gt;
&amp;amp;= i(b, R) + i(c, R) - i(b, c) + i(b, c \mid R) \\&lt;br&gt;
&amp;amp;= h(\O\tr R)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is easy to check using the definition of PMI and $i(b, c, R)\df i(b, c) - i(b, c \mid R)$, and&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
i(b, c \mid R) \df \lg\par{\frac{\mu(b\cap c\cap R)\mu(R)}{\mu(b\cap R)\mu(c \cap R)}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If $c$ is chosen such that $b$ and $c$ are orthogonal, i.e. $i(b,c)=0$, and plugging in $h(\O\tr R)$ for $i(b \cap c, R)$, then we have the simpler form&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
h(\O\tr R) = i(b, R) + i(c, R) + i(b, c \mid R)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which does not involve the information quantity $i(b,c)$ due to redundancy between the choice of $b$ and $c$.&lt;/p&gt;
&lt;p&gt;Note that the &amp;ldquo;$i$&amp;rdquo; quantities may be negative, but $h(\O\tr R)$ is always positive. Thus at least one of these terms is positive.&lt;/p&gt;
&lt;p&gt;It is possible for the information $\O\tr R$ to not contain information (or contain negative information) about $b$ or $c$. Then all of the positive information quantity goes into the term $i(b, c \mid R)$, which can be thought of as the quantity of information $\O\tr b$ has about whether $c$ is the case, given the smaller state space $R$.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514153229.png&#34; alt=&#34;&#34;&gt;
&lt;/p&gt;
&lt;h3 id=&#34;information-about-more-than-two-sets&#34;&gt;Information about more than two sets&lt;/h3&gt;
&lt;p&gt;Suppose $i(b\cap c, R) &amp;lt; h(\O\tr R)$. We can repeat the process above again by choosing a third set $a$ s.t. $i(a\cap b\cap c, R) = h(\O\tr R)$. In general, there is an $n$-way relationship.&lt;/p&gt;
&lt;p&gt;Let $x_1,\dots,x_n,R \subseteq \O$ be arbitrary sets. Then,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
i(x_1\cap\dots\cap x_n,R) = i(x_1\cap\dots\cap x_{n-1},R) + i(x_n, R \mid x_1\cap\dots\cap x_{n-1})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Using the identity&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; i(y_{n-k+1}, \dots, y_n, R \mid y_1\cap\dots\cap y_{n-k}) \\&lt;br&gt;
&amp;amp;\quad\,\, = \,\,i(y_{n-k+1}, \dots, y_n, R \mid y_1\cap\dots\cap y_{n-k-1}) \\&lt;br&gt;
&amp;amp;\qquad\quad- i(y_{n-k}, \dots, y_n, R \mid y_1\cap\dots\cap y_{n-k-1})&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;we can recursively expanding out these terms to produce an expression for $i(x_1\cap\dots\cap x_n,R)$ entirely composed of non-conditional multi-way PMI terms of the form $i(y_1, \dots, y_n)$.&lt;/p&gt;
&lt;p&gt;So for example, expanding out $i(x_n, R \mid x_1\cap\dots\cap x_{n-1})$ with $k=1$, we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; i(x_n, R \mid x_1\cap\dots\cap x_{n-1}) \\&lt;br&gt;
&amp;amp;\quad=\,\, i(x_n, R \mid x_1\cap\dots\cap x_{n-2}) \\&lt;br&gt;
&amp;amp;\qquad\,\,- i(x_{n-1}, x_n, R \mid x_1\cap\dots\cap x_{n-2})\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then further expanding out the resulting terms, using $y_1, \dots, y_{n&#39;} = x_1, \dots, x_{n-2}, x_n$ and $k=1$, we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; i(x_n, R \mid x_1\cap\dots\cap x_{n-2})\\&lt;br&gt;
&amp;amp;\quad=\,\, i(x_n, R \mid x_1\cap\dots\cap x_{n-3}) \\&lt;br&gt;
&amp;amp;\qquad\,\,- i(x_{n-2}, x_n, R \mid x_1\cap\dots\cap x_{n-3})\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and using $y_1, \dots, y_n = x_1, \dots, x_n$ and $k=2$, we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; i(x_{n-1}, x_n, R \mid x_1\cap\dots\cap x_{n-2}) \\&lt;br&gt;
&amp;amp;\quad=\,\, i(x_{n-1}, x_n, R \mid x_1\cap\dots\cap x_{n-3}) \\&lt;br&gt;
&amp;amp;\qquad\,\,- i(x_{n-2}, x_{n-1}, x_n, R \mid x_1\cap\dots\cap x_{n-3})\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Repeat this process of expanding out these terms until all conditional arguments are gone.&lt;/p&gt;
&lt;p&gt;Multi-way PMI terms can then be converted into 2-way PMI terms using the definition $i(y_1, \dots, y_n) \df i(y_1, \dots, y_{n-1}) - i(y_1, \dots, y_{n-1}\mid y_n)$.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For case of three sets $a,b,c$, we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; i(a\cap b\cap c,R) \\&lt;br&gt;
&amp;amp;\quad\,=\, i(a, R) + i(b, R) + i(c, R) \\&lt;br&gt;
&amp;amp;\qquad\,\,- i(a, b, R) - i(a, c, R) - i(b, c, R) \\&lt;br&gt;
&amp;amp;\qquad\,\,+ i(a,b,c,R)&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
i(a,b,c,R) &amp;amp;= i(a,b,c) - i(a,b,c\mid R)\\&lt;br&gt;
&amp;amp;= i(a,b) - i(a,b \mid c) - i(a,b \mid R) + i(a,b \mid c\cap R)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If $a,b,c$ are all mutually orthogonal, i.e. $i(a,b) = i(a,c) = i(b,c) = i(a,b,c) = 0$, then&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; i(a\cap b\cap c,R) \\&lt;br&gt;
&amp;amp;\quad\,=\, i(a, R) + i(b, R) + i(c, R) \\&lt;br&gt;
&amp;amp;\qquad\,\,+ i(a,b \mid R) + i(b,c \mid R) + i(a,b \mid c\cap R)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;h2 id=&#34;expectation&#34;&gt;Expectation&lt;/h2&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210518175356.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
It would be nice if there was a single quantity describing $\mf{B}\tr\mf{B}\dom{R}$, the narrowing down of partition $\mf{B}$ to domain $R$, analogous to $h(\O\tr R)$ for individual sets. I will motivate such a quantity from a few special cases.&lt;/p&gt;
&lt;p&gt;If $P\subseteq 2^\O$ is some set of sets, then let $\mu(P) = \mu(\bigcup P)$. Then for partition $\mf{B}$ of $\O$, we have $\mu(\mf{B}) = \mu(\bigcup\mf{B}) = \mu(\O)$. Let&amp;rsquo;s define the quantity $h(\mf{B}\tr \mf{B}\dom{R})$ on a few special cases.&lt;/p&gt;
&lt;p&gt;Consider $\mf{B}\tr \set{b}$ for some $b\in\mf{B}$.&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514154831.png&#34; alt=&#34;&#34;&gt;
&lt;/p&gt;
&lt;p&gt;Let&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h(\mf{B}\tr \set{b}) &amp;amp;= \lg\par{\frac{\mu(\mf{B})}{\mu\set{b}}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\mu(\O)}{\mu(b)}} \\&lt;br&gt;
&amp;amp;= h(\O\tr b)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then the quantity of information due to narrowing down the partition $\mf{B}$ to one of its elements $b$ is equal to the quantity of information due to narrowing down the possibility space $\O$ to $b$.&lt;/p&gt;
&lt;p&gt;In general, let $\mf{B}&#39; \subseteq \mf{B}$. Then&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h(\mf{B}\tr \mf{B}&#39;) &amp;amp;= \lg\par{\frac{\mu(\mf{B})}{\mu(\mf{B}&#39;)}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\mu(\O)}{\mu(\bigcup \mf{B}&#39;)}} \\&lt;br&gt;
&amp;amp;= h\par{\O\tr \bigcup \mf{B}&#39;}\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514154938.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
One more special case: suppose $\mf{B}$ and $\mf{B}\dom{R}$ are &lt;strong&gt;uniform&lt;/strong&gt;, meaning that $\mu(b) = \mu(b&#39;)$ or $0$ for all $b,b&#39;\in\mf{B}$, and likewise for $\mf{B}\dom{R}$ (which may contain the empty set).&lt;/p&gt;
&lt;p&gt;Then&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h(\mf{B}\tr \mf{B}\dom{R}) &amp;amp;= h(\O\tr b) - h(R \tr b\cap R)\\&lt;br&gt;
&amp;amp;= h(\O\tr R) - h(b \tr b\cap R)\\&lt;br&gt;
&amp;amp;= i(b, \bigcup \mf{B}\dom{R}) \\&lt;br&gt;
&amp;amp;= i(b, R)&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for some $b\in\mf{B}$ s.t. $b\cap R \neq \es$. If $R$ reduces each $b\in\mf{B}$ by the same amount, then $i(b, R) = 0$ for all $b\in\mf{B}$ (i.e. $R$ and $b$ are orthogonal), and so $h(\mf{B}\tr \mf{B}\dom{R}) = 0$, indicating that $\O\tr R$ contains no information about the partition $\mf{B}$.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514155029.png&#34; alt=&#34;&#34;&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514155002.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
Let $\hat{\mf{B}}\dom{R} = \set{b\in\mf{B} \mid b\cap R \neq \es}$ be the subset of $\mf{B}$ containing elements that have non-zero intersection with $R$. We&amp;rsquo;d like $h(\mf{B}\tr \mf{B}\dom{R}) = h(\mf{B}\tr \hat{\mf{B}}\dom{R}) = \lg\par{\frac{\mu(\mf{B})}{\mu(\hat{\mf{B}}\dom{R})}}$. Notice that $\mu(b)/\mu(\bigcup\hat{\mf{B}}\dom{R}) = \mu(b\cap R)/\mu(R)$, which gives us&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(\bigcup\hat{\mf{B}}\dom{R}) = \mu(b)\frac{\mu(R)}{\mu(b\cap R)}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Plugging in, we get $\lg\par{\frac{\mu(\mf{B})}{\mu(\hat{\mf{B}}\dom{R})}} = \lg\par{\frac{\mu(\O)}{\mu(b)\frac{\mu(R)}{\mu(b\cap R)}}} = i(b, R)$.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In the more general case $\mf{B}$ and $\mf{B}\dom{R}$ are not uniform partitions. We will need some kind of averaging operation over $i(b,R)$ for $b\in\mf{B}$ that reduces to our special cases above, and zeros out all the $i(b,R) = -\infty$ terms where $b\cap R = \es$. Taking the expectation w.r.t. $\mu(\cdot \mid R)$ fulfills both requirements.&lt;/p&gt;
&lt;p&gt;Let&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\expt{x\in\mf{X}}{f(x)} = \frac{1}{\mu(\bigcup\mf{X})} \sum_{x\in\mf{X}} \mu(x)\,f(b)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for discrete $\mf{X}$ and&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\expt{x\in\mf{X}}{f(x)} = \frac{1}{\mu(\bigcup\mf{X})}\int_{x\in\mf{X}} \rho(x)\,f(b)\,\mathrm{d}x&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for continuous $\mf{X}$, where $\rho$ is the density function for measure $\mu$. These quantities are normalized by $\mu(\bigcup\mf{X})$ where $\bigcup\mf{X}$ is the domain of the partition $\mf{X}$.&lt;/p&gt;
&lt;p&gt;Define the mutual information between partitions $\mf{A}$ and $\mf{B}$ as&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\I(\mf{A}, \mf{B}) \df \expt{a\in\mf{A},b\in\mf{B}}{i(a,b)}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;as well as the conditional mutual information&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\I(\mf{A}, \mf{B} \mid R) \df \expt{a\dom{R}\in\mf{A}\dom{R},b\dom{R}\in\mf{B}\dom{R}}{i(a,b\mid R)}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Define the mutual information between partition $\mf{B}$ and set $R$ as&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\I(\mf{B}, R) \df \expt{b\dom{R}\in\mf{B}\dom{R}}{i(b,R)}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;In the discrete case,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\I(\mf{B}, R) = \sum_{b\in\mf{B}} \frac{\mu(b\cap R)}{\mu(R)}i(b,R)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;In general, define the quantity of information that $\O\tr R$ contains about which element of partition $\mf{B}$ is the state of system B:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
h(\mf{B}\tr \mf{B}\dom{R}) \df \I(\mf{B}, R)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Let $\mf{C}$ be the state space of some other system C. Taking the expectation of the pointwise sum-conservation law from above, we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\I(\mf{B}\otimes\mf{C}, R) &amp;amp;= \I(\mf{B}, R) + \I(\mf{C}, R) - \I(\mf{B}, \mf{C}, R) \\&lt;br&gt;
&amp;amp;= \I(\mf{B}, R) + \I(\mf{C}, R) - \I(\mf{B}, \mf{C}) + \I(\mf{B}, \mf{C} \mid R)\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mf{B}\otimes\mf{C}\df \set{b\cap c \mid b\in\mf{B} \and c\in\mf{C}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is the &lt;strong&gt;partition product&lt;/strong&gt; of $\mf{B}$ and $\mf{C}$, i.e. the intersection of all pairs of elements of $\mf{B}$ and $\mf{C}$.&lt;/p&gt;
&lt;p&gt;While $\I(\mf{B}, R)$ and $\I(\mf{C}, R)$ are always non-negative (2-way mutual information is always non-negative), the 3-way mutual information $\I(\mf{B}, \mf{C}, R)$ can be negative. If $\mf{B}$ and $\mf{C}$ are orthogonal, i.e. $\I(\mf{B}, \mf{C}) = 0$, then&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\I(\mf{B}\otimes\mf{C}, R) = \I(\mf{B}, R) + \I(\mf{C}, R) + \I(\mf{B}, \mf{C} \mid R)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is a decomposition of $\I(\mf{B}\otimes\mf{C}, R)$ into non-negative terms. This looks more like the sum-conservation laws we have for mass and energy (sum of non-negative terms is conserved).&lt;/p&gt;
&lt;h3 id=&#34;environments&#34;&gt;Environments&lt;/h3&gt;
&lt;p&gt;Let $\mf{B}$ be some state space. A &lt;strong&gt;partition complement&lt;/strong&gt; of $\mf{B}$, denoted $\c{\mf{B}}$, satisfies the following properties:&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;$\c{\mf{B}}$ is a partition of $\O$.&lt;/li&gt;
&lt;li&gt;$\I(\mf{B}, \c{\mf{B}}) = 0$,&lt;/li&gt;
&lt;li&gt;$\mf{B}\otimes\c{\mf{B}} = \mf{O} = \set{\set{\o}\mid\o\in\O}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In general, $\mf{B}$ does not have a unique complement, and may not have any complement. For example, if $\O = \set{\o_1, \o_2, \o_3}$ and $\mf{B} = \set{\set{\o_1, \o_2}, \set{\o_3}}$, then it is easy to check that $\mf{B}$ has no complement.&lt;/p&gt;
&lt;p&gt;If each element of $\mf{B}$ is infinite, then $\mf{B}$ always has a complement. If each element of $\mf{B}$ is finite, then it has a complement if each element has the same cardinality.&lt;/p&gt;
&lt;p&gt;We can regard a complement $\c{\mf{B}}$ as the &lt;strong&gt;environment&lt;/strong&gt; of system B, which is everything in the universe outside of system B.&lt;/p&gt;
&lt;p&gt;Assuming $\mf{B}$ has a complement $\c{\mf{B}}$, then the sum-conservation law from above is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\I(\mf{B}\otimes\c{\mf{B}}, R) = \I(\mf{B}, R) + \I(\c{\mf{B}}, R) + \I(\mf{B}, \c{\mf{B}} \mid R)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;A useful identity here is that $\I(\mf{O}, R) = h(\O\tr R)$. Since $\mf{B}\otimes\c{\mf{B}} = \mf{O}$, then we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
h(\O\tr R) = \I(\mf{B}, R) + \I(\c{\mf{B}}, R) + \I(\mf{B}, \c{\mf{B}} \mid R)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/em&gt; that $\I(\mf{O}, R) = h(\O\tr R)$.&lt;/p&gt;
&lt;p&gt;Let $\mf{X}$ be a partition of $\O$ s.t. for each $x\in\mf{X}$, either $x\cap R = \es$ or $x$. That is to say, there is some subset $Y \subseteq \mf{X}$ s.t. $R = \bigcup Y$. Then $Y = \mf{X}\dom{R}$.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\I(\mf{X}, R) &amp;amp;= \sum_{x\in\mf{X}} \mu(x \mid R) \lg\par{\frac{\mu(x\cap R)\mu(\O)}{\mu(x)\mu(R)}} \\&lt;br&gt;
&amp;amp;= \sum_{x\in \mf{X}\dom{R}} \mu(x \mid R) \lg\par{\frac{\cancel{\mu(x)}\mu(\O)}{\cancel{\mu(x)}\mu(R)}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\mu(\O)}{\mu(R)}} \\&lt;br&gt;
&amp;amp;= h(\O\tr R)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Since $\mf{O}$ is the singleton partition, it satisfies the condition above for all $R\subseteq \O$. Thus $\I(\mf{O}, R) = h(\O\tr R)$. $\qed$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Combining conservation of information quantity,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
h(\O\tr R) = h(\O\tr \t_\Dt(R))\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;with the sum-conservation law,  we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; \I(\mf{B}, R) + \I(\c{\mf{B}}, R) + \I(\mf{B}, \c{\mf{B}} \mid R) \\&lt;br&gt;
=\,\, &amp;amp; \I(\mf{B}, \t_\Dt(R)) + \I(\c{\mf{B}}, \t_\Dt(R)) + \I(\mf{B}, \c{\mf{B}} \mid \t_\Dt(R))\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Consider system A with state space $\A$, and its environment with state space $\cA$. At time $t$, system A is in state $a\up{t}\in\mf{A}$ and has the information $\O\tr a\up{t}$ about time $t$. Since $\I(\A, a\up{t}) = h(\O\tr a\up{t})$, then $\I(\cA, a\up{t}) + \I(\A, \cA \mid a\up{t}) = 0$.&lt;/p&gt;
&lt;p&gt;Then our conservation law reduces to&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\I(\A, a\up{t}) &amp;amp;= \I(\A, \t_\Dt(a\up{t})) + \I(\cA, \t_\Dt(a\up{t})) + \I(\A, \cA \mid \t_\Dt(a\up{t})) \\&lt;br&gt;
&amp;amp;= h(\O\tr a\up{t}) \,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If $\I(\A, \t_\Dt(a\up{t})) = h(\O\tr \t_\Dt(a\up{t})) = h(\O\tr a\up{t})$, then $\I(\cA, \t_\Dt(a\up{t})) + \I(\A, \cA \mid \t_\Dt(a\up{t})) = 0$ and system A (at time $t$) has no information about the environment at time $t+\Dt$. Since all the terms are positive, for system A to have information about the future environment, system A must have less than complete information about its own future state.&lt;/p&gt;
&lt;p&gt;A tricky case to be aware of is when $\I(\A, \t_\Dt(a\up{t})) = 0$ and $\I(\cA, \t_\Dt(a\up{t})) = 0$. This would seem to be saying that system A has no information about its own future state and the environment&amp;rsquo;s future state. It would then seem that the information quantity $h(\O\tr a\up{t})$ simply disappeared and was not conserved. That quantity went into the third term, $\I(\A, \cA \mid \t_\Dt(a\up{t})) = h(\O\tr a\up{t})$, which indicates that system A becomes highly correlated with its environment in the future.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514160217.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;$R=a\in\A$. Then $\I(\A,R) = h(\O\tr R)$ and $\I(\cA,R) = 0$, $\I(\A,\cA \mid R) = 0$.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;br&gt;


  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514160232.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;$R=a^\dg\in\cA$. Then $\I(\cA,R) = h(\O\tr R)$ and $\I(\A,R) = 0$, $\I(\A,\cA \mid R) = 0$.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;br&gt;


  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514160247.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;$\I(\A,R)$ and $\I(\cA,R)$ are both non-zero, and $\I(\A,\cA \mid R) = 0$, meaning $\A$ and $\cA$ are still orthogonal when restricted to the domain $R$.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;br&gt;


  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210514160256.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;$\I(\A,R) = 0$ and $\I(\cA,R) = 0$, since restricting either partition to the domain $R$ still tells you nothing about the other partition. However, $\I(\A,\cA \mid R) = h(\O\tr R)$, meaning $\A$ and $\cA$ restricted to the domain $R$ are maximally redundant, i.e. given $R$ and some $a\in\mf{A}$, you can uniquely determine $a^\dg\in\mf{A}$, and vice versa.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;shannon-quantities&#34;&gt;Shannon Quantities&lt;/h2&gt;
&lt;p&gt;It is helpful to connect this all back to the standard language of information theory. Let $\rv{A}, \rv{B}, \rv{C}$ be random variables with joint distribution $p(\rv{A}, \rv{B}, \rv{C})$.&lt;/p&gt;
&lt;p&gt;Mutual information is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
I(\rv{A}, \rv{B}) = \expt{a,b\sim p(\rv{A},\rv{B})}{\lgfr{p(a, b)}{p(a)p(b)}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;What are less obvious are the Shannon analogs to $\I(\mf{A}, R)$ and $\I(\mf{A}\otimes\mf{B}, R)$.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll define the non-standard Shannon quantity:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
I(\rv{A}, \rv{B} = b) \df \expt{a\sim p(\rv{A} \mid \rv{B}=b)}{\lgfr{p(a, b)}{p(a)p(b)}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is not quite a conditional quantity like $H(\rv{A} \mid \rv{B} = b)$, because the contents of the expectation are not a pointwise conditional quantity (like $h(a \mid b)$). Then $I(\rv{A}, \rv{B} = b)$ is really its own thing. It is the expectation of non-conditional pointwise mutual information $i(a,b)$, but over a conditional probability distribution. Then (for $b\in\mf{B}$ on the lhs) we have the analog&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\I(\mf{A}, b) \iff I(\rv{A}, \rv{B} = b)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Next we have the partition product, $\mf{A}\otimes\mf{B}$. As a random variable, this is just the random tuple $\rv{T_{A,B}} = (\rv{A}, \rv{B})$. The random variable $\rv{T_{A,B}}$ is just the combined outcome of both random variables $\rv{A}$ and $\rv{B}$. We must distinguish between&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
I((\rv{A}, \rv{B}), \rv{C}) &amp;amp;= \expt{a,b,c\sim p(\rv{A},\rv{B},\rv{C})}{\lgfr{p(a, b, c)}{p(a,b)p(c)}} \\&lt;br&gt;
&amp;amp;= \expt{t,c\sim p(\rv{T_{A,B}},\rv{C})}{\lgfr{p(t, c)}{p(t)p(c)}} \\&lt;br&gt;
&amp;amp;= I(\rv{T_{A,B}}, \rv{C})&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and the 3-way mutual information&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
I(\rv{A}, \rv{B}, \rv{C}) = I(\rv{A}, \rv{B}) - I(\rv{A}, \rv{B} \mid \rv{C})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then we have the analogs (for $c\in\mf{C}$)&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\I(\mf{A}\otimes\mf{B}, c) \iff I((\rv{A}, \rv{B}), \rv{C}=c)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\I(\mf{A}, \mf{B}, c) \iff I(\rv{A}, \rv{B}, \rv{C}=c)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We also have the same decompositions on these Shannon quantities:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
I((\rv{A}, \rv{B}), \rv{C}) = I(\rv{A}, \rv{C}) + I(\rv{B}, \rv{C}) - I(\rv{A}, \rv{B}, \rv{C})&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
I((\rv{A}, \rv{B}), \rv{C}=c) = I(\rv{A}, \rv{C}=c) + I(\rv{B}, \rv{C}=c) - I(\rv{A}, \rv{B}, \rv{C}=c)\,.&lt;br&gt;
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information Algebra</title>
      <link>https://danabo.github.io/blog/posts/information-algebra/</link>
      <pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/information-algebra/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\es}{\emptyset}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\N}{\mb{N}}&lt;br&gt;
\newcommand{\I}{\mb{I}}&lt;br&gt;
\newcommand{\H}{\mb{H}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[1]{_{|#1}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\Dt}{{\Delta t}}&lt;br&gt;
\newcommand{\Dh}{{\Delta h}}&lt;br&gt;
\newcommand{\tr}{\rightarrowtail}&lt;br&gt;
\newcommand{\tra}[2]{\,^{#1\!\!}\searrow _{#2\,}}&lt;br&gt;
\newcommand{\mi}[4]{\,^{#1\!\!}\searrow _{#2\,}\rightrightarrows ^{#3}\searrow _{#4\,}}&lt;br&gt;
\newcommand{\absp}[1]{\abs{#1}^+}&lt;br&gt;
\newcommand{\Bar}{\overline}&lt;br&gt;
\newcommand{\dmid}{\,\|\,}&lt;br&gt;
\newcommand{\V}[1]{\begin{pmatrix}#1\end{pmatrix}}&lt;br&gt;
\require{cancel}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is a review of the ideas I introduced in &lt;a href=&#34;https://danabo.github.io/blog/posts/bayesian-information-theory/&#34;&gt;Bayesian information theory&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We have a set of possibilities $\O$, and there is a &lt;em&gt;true&lt;/em&gt; but unknown possibility $\o^*\in\O$. I define information as a tuple of the form $(\O,R)$ where $R\subseteq \O$, which asserts that $\o^*\in R$. I notate these information tuples with arrows:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\O\tr R \df (\O, R)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which makes it clear that information is the &lt;em&gt;narrowing-down&lt;/em&gt; of a possibility space.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210505092619.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210505092619.png&#34; width=&#34;600&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
I also use the following notation for &lt;strong&gt;domain restriction&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
A\dom{B} \df A \cap B\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is the set $A$ restricted to the domain of $B$. This notation is more compact than intersection notation, and I can use it to emphasize the semantic distinction between which set is being restricted and which set is the domain.&lt;/p&gt;
&lt;p&gt;To quantify information, we need a measure $\mu$ on $\O$. When $\O$ is finite, I always use the counting measure $\mu(A) = \abs{A}$. The measure $\mu$ need not be normalized on $\O$, i.e. $\mu(\O)$ need not be $1$. I interpret $\mu$ as just a measure of the size of regions of the possibility space, rather than a measure of probability or randomness.&lt;/p&gt;
&lt;p&gt;The information $\O \tr R$ is quantified by&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
h_\O(R) \df \lg\par{\frac{\mu(\O)}{\mu(R)}} = \lg\par{\frac{1}{\mu(R\mid \O)}}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which has the unit &lt;em&gt;bits&lt;/em&gt; and measures the number of halvings it takes to go from $\O$ to $R$.&lt;/p&gt;
&lt;p&gt;I put $\O$ in the subscript of $h$ to make it clear what the domain is. Let $\mu(A \mid B) \df \mu(A\dom{B} \mid B) = \mu(A\cap B\mid B)$ be the measure $\mu$ restricted to $B$ and normalized so that $\mu(B \mid B) = 1$. When $\mu(\O) = 1$, then $h_\O(R) = -\lg\mu(R)$ which is called &lt;em&gt;self-information&lt;/em&gt; in Shannon&amp;rsquo;s information theory, and $h(A \mid B) = -\lg\mu(A \mid B)$ is called &lt;em&gt;conditional self-information&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If $\O$ has already been narrowed down to $R$, and is then further narrowed down to $R&#39;\subseteq R$, the incremental quantity of information, i.e. the quantity of $R\tr R&#39;$, is given by $h(R&#39; \mid R)$. In general, for any sets $A,B \subseteq \O$, the information $B \tr A\dom{B}$ is quantified by&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h(A \mid B) &amp;amp;\df h_\O(A\dom{B}) - h_\O(B) \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\mu(B)}{\mu(A\dom{B})}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{1}{\mu(A\mid B)}}\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;I leave off the $\O$ subscript because it has no bearing on this quantity. Note that $h(A \mid B) = h_B(A\dom{B}) = h_B(A\cap B)$ is just another way to specify the domain $B$. This is convenient notationally when $A\setminus B\neq \es$.&lt;/p&gt;
&lt;p&gt;To make working with these quantities easier, I adopt the following shorthand:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h(B \tr A\dom{B}) &amp;amp;\df h(A\dom{B} \mid B) = h_B(A\dom{B}) \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\mu(B)}{\mu(A\dom{B})}}\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The lhs of the arrow goes in the numerator, and the rhs of the arrow goes in the denominator. This is convenient for thinking algebraically about more complex manipulations:&lt;/p&gt;
&lt;p&gt;For $A \subseteq B$ and $B\subseteq C$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h(C \tr B) + h(B \tr A) &amp;amp;= \lg\par{\frac{\mu(C)}{\cancel{\mu(B)}}} + \lg\par{\frac{\cancel{\mu(B)}}{\mu(A)}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\mu(C)}{\mu(A)}} \\&lt;br&gt;
&amp;amp;= h(C\tr A)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Likewise we obtain the following identities:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h(C\tr A) - h(B \tr A) &amp;amp;= h(C \tr B) \\&lt;br&gt;
h(C \tr A) - h(C \tr B) &amp;amp;= h(B \tr A)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This successive narrowing down can be represented visually:&lt;/p&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210505092828.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210505092828.png&#34; width=&#34;600&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;
&lt;h1 id=&#34;mutual-information&#34;&gt;Mutual Information&lt;/h1&gt;
&lt;p&gt;Let $A,R\subseteq \O$. Suppose we have the information $\O\tr R$, corresponding to the knowledge that $\o^*\in R$. To be more succinct, I will say that we know that $R$ is &lt;em&gt;true&lt;/em&gt;. Also suppose we don&amp;rsquo;t know whether $A$ is true, i.e. we don&amp;rsquo;t have the information $\O\tr A$. For information that is not known, I will use a &lt;em&gt;small diagonal arrow&lt;/em&gt;, $\tra{\O}{A}$. We can think of this as aspirational information, i.e. information we do not have but would like to have.&lt;/p&gt;
&lt;p&gt;Since $\tra{\O}{A}$ and $\O\tr A$ are mathematically equivalent, $h(\tra{\O}{A}) = h(\O\tr A) = \lg\frac{\mu(\O)}{\mu(A)}$.&lt;/p&gt;
&lt;p&gt;Does $\O\tr R$ move us closer to the goal of $\tra{\O}{A}$? That is to say, given we have the information $\O\tr R$, what &lt;strong&gt;information about&lt;/strong&gt; $A$ do we have? As we shall see, pointwise mutual information quantifies &amp;ldquo;information about&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;There are three ways $R$ and $A$ can interact:&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210505094432.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210505094432.png&#34; width=&#34;600&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210505094504.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210505094504.png&#34; width=&#34;600&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210505094519.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210505094519.png&#34; width=&#34;600&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
Arrows here indicate information we actually have, i.e. $\O\tr R$, and lack of arrows indicates information we don&amp;rsquo;t have, i.e. $\tra{\O}{A}$ and $\tra{R}{A\cap R}$.&lt;/p&gt;
&lt;p&gt;In the first case, $A \subseteq R$, and $\tra{\O}{A}$ is transformed into $\tra{R}{A}$. Clearly $h(\tra{\O}{A}) - h(\tra{R}{A}) = h(\O\tr R)$ which is the quantity of information we have gained towards $\tra{\O}{A}$, and $h(\tra{R}{A})$ is the quantity of information still needed to know that $A$ is true.&lt;/p&gt;
&lt;p&gt;In the other two cases, parts of $A$ are ruled out, which shrinks $A$ or reduces it to the empty set. The aspirational information $\tra{\O}{A}$ is transformed into $\tra{R}{A\dom{R}}$. The change in quantity is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h(\tra{\O}{A}) - h(\tra{R}{A\dom{R}}) &amp;amp;= \lg\par{\frac{\mu(\O)}{\mu(A)}} - \lg\par{\frac{\mu(R)}{\mu(A\dom{R})}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\mu(A\dom{R})\mu(\O)}{\mu(A)\mu(R)}}\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and nothing cancels out if $A \neq A\dom{R}$. This is an irreducible quantity of interest, called &lt;strong&gt;pointwise mutual information&lt;/strong&gt; (PMI), formally defined as&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
i_\O(A, R) &amp;amp;\df \lg\par{\frac{\mu(A\dom{R})\mu(\O)}{\mu(A)\mu(R)}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\mu(A \mid \O)}{\mu(A \mid R)}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\mu(R \mid \O)}{\mu(R \mid A)}} \,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;When $\mu(\O) = 1$ we get the more familiar expression, $i_\O(A, R) = \lg\par{\frac{\mu(A \cap R)}{\mu(A)\mu(R)}}$.&lt;/p&gt;
&lt;p&gt;We can see that $i_\O(A, R)$ can also be written&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
i_\O(A, R) &amp;amp;= \lg\par{\frac{\mu(\O)}{\mu(R)}} - \lg\par{\frac{\mu(A)}{\mu(A\dom{R})}} \\&lt;br&gt;
&amp;amp;= h(\O\tr R) - h(\tra{A}{A\dom{R}}) \\&lt;br&gt;
&amp;amp;= i_\O(R, A)\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where the $A$ and $R$ corners are swapped.&lt;/p&gt;
&lt;p&gt;Since $h(\tra{A}{A\dom{R}})$ is always positive (because $\mu(A\dom{R})\leq\mu(A)$), we see that $i_\O(A, R)$ is upper bounded by $h(\O\tr R)$, and $i_\O(A, R) = h(\O\tr R)$ when $A \subseteq R$ (since $\mu(A\dom{R}) = \mu(A)$), which we previously derived.&lt;/p&gt;
&lt;p&gt;What about when $A\dom{R} \neq A$? It turns out that $i_\O(A, R)$ is not lower bounded, and can be arbitrarily negative. To interpret these negative values, let&amp;rsquo;s think about what is going on visually. The information $\O\tr R$ transforms $(\O,A)$ to $(R,A\dom{R})$:&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210505104353.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
In terms of quantity, what has changed is the ratio: $\frac{\mu(\O)}{\mu(A)}$ to $\frac{\mu(R)}{\mu(A\dom{R})}$. Specifically, if $\frac{\mu(\O)}{\mu(A)} \to \frac{\mu(R)}{\mu(A\dom{R})}$ is one halving, i.e. $\frac{\mu(R)}{\mu(A\dom{R})} = \frac{1}{2}\frac{\mu(\O)}{\mu(A)}$, then $i_\O(A, R) = \lg\par{\frac{\mu(\O)}{\mu(A)}\Big{/}\frac{\mu(R)}{\mu(A\dom{R})}} = \lg\par{\frac{\mu(\O)}{\mu(A)}\Big{/}\frac{1}{2}\frac{\mu(\O)}{\mu(A)}} = \lg(2) = 1$ bit.&lt;/p&gt;
&lt;p&gt;A different way to think about it is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
i_\O(A,R) &amp;amp;= \lg\par{\frac{\mu(\O)}{\mu(R)\frac{\mu(A)}{\mu(A\dom{R})}}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\mu(\O)}{\nu(R)}}\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\nu(Q)=\mu(Q)\frac{\mu(A)}{\mu(A\dom{R})}$ rescales the size of any set $Q$ so that $\nu(A\dom{R}) = \mu(A\dom{R})\frac{\mu(A)}{\mu(A\dom{R})} = \mu(A)$. In this form, $i_\O(A,R)$ looks like the quantity of information for $\O\tr R$, but where the numerator and denominator use different measures. This quantity of information can be negative, unlike $h(\O\tr R)$.&lt;/p&gt;
&lt;p&gt;This rescaling can be visualized by drawing to scale the relative proportions of $\mu(\O)$ and $\mu(A)$, and show below that the same relative proportions of $\nu(R)$ and $\nu(A\dom{R})$, so that $\nu(A\dom{R})$ is visually the same size as $\mu(A)$:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210506164550.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210506164550.png&#34; width=&#34;400&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
Here lengths denote size. This image shows that 1 bit is gained about whether $A$ is true because the domain is halved, i.e. we are 1 bit closer to knowing that $A$ is true. However, the bottom rectangle is rescaled so that $A$ and $A\dom{R}$ are visually the same size. $h(\O\tr R)$ may not be 1.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210506164602.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210506164602.png&#34; width=&#34;740&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
Here is another example where the narrowing down $\tra{A}{A\dom{R}}$ outpaces the narrowing down $\O\tr R$, i.e. more of $A$ is ruled out than the domain of $A$ is reduced. We see that this scaled domain appears to be doubled, which is the loss of 1 bit, i.e. $i_\O(A,R) = -1$. We are 1 bit further away from knowing that $A$ is true, and we now need an additional bit of information to know $\tra{\O}{A}$ compared with before $\O\tr R$ was known (compared with total ignorance).&lt;/p&gt;
&lt;p&gt;Now we see why PMI is upper bounded but not lower bounded. At most, $i_\O(A, R) = h(\O\tr A)$ if $R = A$, which is equivalent to gaining the information that $A$ is true. This can be achieved in a finite number of halvings. On the other hand, the scaled domain of $A\dom{R}$ can grow arbitrarily large as $R$ rules out more and more of $A$, i.e. $\mu(A \setminus R) \to \mu(A)$ implies $\mu(A\dom{R}) \to 0$. If $A \cap R = \es$, then $i_\O(A,R) = -\infty$, which we can interpret to mean that $\O\tr R$ proves that $A$ is &lt;em&gt;false&lt;/em&gt;, i.e. the knowledge that $\o^* \notin A$. Thus no amount of information can make $A$ true (an infinite quantity of information here indicates a contradiction).&lt;/p&gt;
&lt;h2 id=&#34;pmi-vs-conditional-information&#34;&gt;PMI vs conditional information&lt;/h2&gt;
&lt;p&gt;$i_\O(A, R)$ and $h(A \mid R)$ are each quantifying a kind of transformation on $\tra{\O}{A}$. Assuming that $\O\tr R$ is already known,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$i_\O(A, R)$ quantifies a change in the lhs (domain) and a rescaling of the rhs: $\tra{\O}{A} \to \tra{R}{A\dom{R}}$, whereas&lt;/li&gt;
&lt;li&gt;$h(A \mid R)$ quantifies a change in the rhs (target): $(\O\tr R) \to (\O \tr A)$, i.e. the amount of additional bits gained by this transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;A well known identity from information theory is $i_\O(A, R) + h(A \mid R) = h_\O(A)$, or written another way:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
i_\O(A, R) + h(R \tr A\dom{R}) = h(\O\tr A)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Why is this sum not equal to $h(\O\tr A\dom{R})$? Note that $h(\O\tr R) + h(R \tr A\dom{R}) = h(\O\tr A\dom{R})$. As we saw, $i_\O(A, R)$ is closely related to $h(\O\tr R)$ but not always the same.&lt;/p&gt;
&lt;p&gt;The difference between $i_\O(A, R) + h(R \tr A\dom{R})$ and $h(\O\tr R) + h(R \tr A\dom{R})$ can be illustrated visually.&lt;/p&gt;
&lt;p&gt;Double domain reduction $h(\O\tr R) + h(R \tr A\dom{R}) = h(\O\tr A\dom{R})$:&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210505132315.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210505132315.png&#34; width=&#34;600&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The PMI $i_\O(A, R)$ involves a rescaling of $A\dom{R}$ to $A$, shown visually. The transformation $\tra{\O}{A} \to \tra{R}{A\dom{R}}$, when rescaled covers the &lt;em&gt;distance&lt;/em&gt; $i_\O(A, R)$ in the diagram. $h(R \tr A\dom{R})$ covers the remaining &lt;em&gt;distance&lt;/em&gt;, which is equivalent to the total distance $h(\O\tr A)$.&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210505134431.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210505134431.png&#34; width=&#34;400&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;h1 id=&#34;appendix-pmi-algebra&#34;&gt;Appendix: PMI Algebra&lt;/h1&gt;
&lt;p&gt;I&amp;rsquo;ve played around with an algebraically convenient notation for PMI, and this is what I arrived at:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
h(\mi{\O}{A}{R}{A\dom{R}}) \df h(\tra{\O}{A}) - h(\tra{R}{A\dom{R}}) = i_\O(A, R)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;It visualizes the joint narrowing down involved in mutual information:&lt;/p&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210505100706.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210505100706.png&#34; width=&#34;400&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;
&lt;p&gt;This notation has the downside of not being compact. I&amp;rsquo;m not sure if it helps with reasoning about relations between quantities. You can evaluate that for yourself:&lt;/p&gt;
&lt;p&gt;$h(\mi{\O}{A}{R}{A\dom{R}}) = h(\mi{\O}{R}{A}{A\dom{R}})$&lt;br&gt;
$h(\mi{\O}{A}{R}{A\dom{R}}) = h(\O\tr R) - h(A \tr A\dom{R})$&lt;/p&gt;
&lt;p&gt;$h(\O\tr R) - h(\mi{\O}{A}{R}{A\dom{R}}) = h(A \tr A\dom{R})$&lt;br&gt;
$h(\tra{\O}{A}) - h(\mi{\O}{A}{R}{A\dom{R}}) = h(\tra{R}{A\dom{R}})$&lt;br&gt;
$h(\mi{\O}{A}{R}{A\dom{R}}) + h(\tra{R}{A\dom{R}}) = h(\tra{\O}{A})$&lt;br&gt;
$h(\mi{\O}{A}{R}{A\dom{R}}) + h(A \tr A\dom{R}) = h(\O\tr R)$&lt;/p&gt;
&lt;p&gt;$h(\mi{\O}{A}{R}{A\dom{R}}) + h(R\tr A\dom{R}) + h(A \tr A\dom{R}) = h(\O\tr A\dom{R})$&lt;br&gt;
$h(\mi{\O}{A}{R}{A\dom{R}}) + h(\O\tr R) + h(R \tr A\dom{R}) = h(\O\tr A\dom{R})$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Causality For Physics</title>
      <link>https://danabo.github.io/blog/posts/causality-for-physics/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/causality-for-physics/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\s}{\sigma}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\d}{\delta}&lt;br&gt;
\newcommand{\dd}{\mathrm{d}}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[2]{#1_{\mid #2}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\Do}{\mathrm{do}}&lt;br&gt;
\newcommand{\do}[2]{\underset{#1\leadsto #2}{\mathrm{do}}}&lt;br&gt;
\newcommand{\restr}[1]{_{\mid{#1}}}&lt;br&gt;
\newcommand{\dt}{{\D t}}&lt;br&gt;
\newcommand{\Dt}{{\D t}}&lt;br&gt;
\newcommand{\ddT}{{\delta T}}&lt;br&gt;
\newcommand{\Mid}{\,\middle|\,}&lt;br&gt;
\newcommand{\qed}{\ \ \blacksquare}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The definition of causality within physics is not a settled matter, perhaps surprisingly. My understanding is that this question is studied more by philosophers than physicists, as the field of physics tends to avoid interpretational problems. That is to say, theories like relativity or quantum mechanics are mathematically well defined and make predictions, so that&amp;rsquo;s all there is to it, right? I&amp;rsquo;m not a physicist, so I will proceed to ask such questions.&lt;/p&gt;
&lt;p&gt;I suspect that causality and information are intimately related. To initiate my pursuit to understand physical information, I am starting by trying to understand the role causality plays in physics. The &lt;a href=&#34;https://plato.stanford.edu/entries/causation-physics&#34;target=&#34;_blank&#34;&gt;SEP&lt;/a&gt; outlines some of the conversation and ideas around causality and physics. I haven&amp;rsquo;t read these ideas yet, but I want to take my own tabula rasa stab at the problem before reading about what other people have tried. I am familiar with Judea Pearl&amp;rsquo;s notion of causality in machine learning and statistics, which I will attempt to apply to physics below.&lt;/p&gt;
&lt;h1 id=&#34;causal-models&#34;&gt;Causal Models&lt;/h1&gt;
&lt;p&gt;First, I&amp;rsquo;ll outline Pearl&amp;rsquo;s framework for causality. I used &lt;a href=&#34;http://bayes.cs.ucla.edu/BOOK-2K/&#34;target=&#34;_blank&#34;&gt;Causality&lt;/a&gt; (Pearl) and &lt;a href=&#34;https://mitpress.mit.edu/books/elements-causal-inference&#34;target=&#34;_blank&#34;&gt;Elements of Causal Inference&lt;/a&gt; (Peters, Janzing, SchÃ¶lkopf) to learn about this topic.&lt;/p&gt;
&lt;p&gt;Pearl assumes the world (or some part of it) can be represented by graph, where nodes represent potential observations, and their directed edges represent causal links. For example (from Pearl):&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210412175938.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
The core idea in Pearl&amp;rsquo;s causality is the &lt;strong&gt;intervention&lt;/strong&gt;, which is a modification to the graph where a node is disconnected from all incoming arrows and held fixed at some value.&lt;/p&gt;
&lt;p&gt;An example of an intervention:&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210412175953.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
An intervention in this graph is a &lt;strong&gt;graph surgery&lt;/strong&gt; (as Pearl calls it). Graph interventions correspond to real-world interventions. The intervention depicted above corresponds to someone forcing the sprinkler system to turn on (e.g. by switching the sprinkler system&amp;rsquo;s setting from auto to manual). The sprinkler state is now causally independent of everything else in the graph, because we, the experimenters, have directly determined its state (we would need to be careful to ensure our own actions are not causally linked to the system we are studying). By observing the down stream effects of this change to the graph, the &lt;strong&gt;causal effect&lt;/strong&gt; of the particular node $X_3$ can be measured. That is the effect of $X_3$, independent of other nodes like $X_1$.&lt;/p&gt;
&lt;p&gt;Generally Pearl places a probability distribution on graph node states, given by $P(X_1=x_1, X_2=x_2, X_3=x_3, \dots)$, or using shorthand, $P(x_1, x_2, x_3, \dots)$. I&amp;rsquo;ll use capital letters, $X_i$, to denote graph nodes themselves (or random variables on graph nodes), and lowercase letters, $x_i$, to denote a specific value that the correspond node takes on. So for example, node $X_3$, the sprinkler state, could take on the values $\mathrm{ON}$ or $\mathrm{OFF}$. In the abstract, $X_3$ takes on some value $x_3$. Sometimes I&amp;rsquo;ll introduce a &amp;ldquo;prime&amp;rdquo; tick, $x&#39;_3$, to denote some other value that may be distinct from $x_3$.&lt;/p&gt;
&lt;p&gt;There is an alternative &lt;strong&gt;functional&lt;/strong&gt; perspective, where each node&amp;rsquo;s value is a deterministic function of incoming values traveling along inward arrows, and an auxiliary noise input not depicted in the graph. Those noise inputs can themselves be determined (i.e. held fixed), but be pulled from an algorithmically random stream. I will stick to the deterministic perspective when I discuss physics, while recognizing that random physical processes can be viewed as deterministic but algorithmically random.&lt;/p&gt;
&lt;p&gt;Quoting &lt;a href=&#34;http://bayes.cs.ucla.edu/BOOK-2K/&#34;target=&#34;_blank&#34;&gt;Causality&lt;/a&gt;, section 1.4.1, &lt;em&gt;Structural Equations&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In its general form, a functional causal model consists of a set of equations of the form&lt;br&gt;
$$x_i = f_i(pa_i,u_i),\quad i=1,\dots,n\,,$$&lt;br&gt;
where $pa_i$ (connoting parents) stands for the set of variables that directly determine the value of $X_i$ and where the $U_i$ represent errors (or âdisturbancesâ) due to omitted factors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That is to say, the parents $PA_i$ of node $X_i$ is the set of nodes with arrows pointing into $X_i$. So in the example, $PA_3 = \set{X_1}$ because $X_1$ is the only node pointing into $X_3$, and $PA_4 = \set{X_2, X_3}$ because both $X_2$ and $X_3$ point into $X_4$. Node $X_1$ is not a direct parent of $X_4$.&lt;/p&gt;
&lt;p&gt;$U_i$ is an auxiliary input node to each $X_i$ which is not depicted in the graph, which makes the output value $x_i$ random. In my view, each value $u_i$ is pulled from an algorithmically random stream. Given the set of values $pa_i$ and value $u_i$, the output of the function $f_i$ is then able to be random.&lt;/p&gt;
&lt;p&gt;A note about notation: It would not be correct to write $f_i(PA_i,U_i)$ which passes the nodes themselves into the function $f_i$. On the other hand, $f_i(pa_i,u_i)$ is passing the values $pa_i$ of the parent nodes $PA_i$ and $u_i$ of the noise input node $U_i$ into the function.&lt;/p&gt;
&lt;h2 id=&#34;do-operator&#34;&gt;Do-Operator&lt;/h2&gt;
&lt;p&gt;If $P$ is the probability measure on the initial graph (e.g. figure 1.2 above), then what is the probability measure on the modified graph after taking an intervention (e.g. figure 1.4)? Pearl uses &amp;ldquo;do&amp;rdquo;-notation, which for the example above looks like this:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
P(x_1, x_2, x_3, x_4, x_5 \mid \Do(X_3 = \mathrm{ON}))\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is the probability of the vector of node values $(x_1, x_2, x_3, x_4, x_5)$ given that the intervention setting node $X_3$ to constant value $\mathrm{ON}$ was taken. Note the notational similarity to conditional probability: $P(x_1, x_2, x_3, x_4, x_5 \mid X_3 = \mathrm{ON})$. Conditionalization is a different operation on the measure $P$ than the &amp;ldquo;do&amp;rdquo;-operator, but they are mathematically related and their similar notation is justified.&lt;/p&gt;
&lt;p&gt;For an arbitrary graph with nodes $X_1,\dots,X_n$, and probability measure $P$ on node values, the conditional probability of value vector $(x_1, \dots, x_n)$ given $X_i = x&#39;_i$ is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
P(x_1, \dots, x_n \mid x&#39;_i) = \begin{cases}&lt;br&gt;
\frac{P(x_1, \dots, x_n)}{P(x&#39;_i)} &amp;amp; x_i=x&#39;_i \\&lt;br&gt;
0 &amp;amp; x_i \neq x&#39;_i&lt;br&gt;
\end{cases}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;whereas the probability of $(x_1, \dots, x_n)$ given that intervention $\Do(X_i = x&#39;_i)$ was taken is (&lt;a href=&#34;http://bayes.cs.ucla.edu/BOOK-2K/&#34;target=&#34;_blank&#34;&gt;Causality&lt;/a&gt;, eq 3.11)&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
P(x_1, \dots, x_n \mid \mathrm{do}(x&#39;_i)) = \begin{cases}&lt;br&gt;
\frac{P(x_1, \dots, x_n)}{P(x&#39;_i \mid pa_i)} &amp;amp; x_i=x&#39;_i \\&lt;br&gt;
0 &amp;amp; x_i \neq x&#39;_i&lt;br&gt;
\end{cases}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Both operations are performing a &lt;strong&gt;domain restriction&lt;/strong&gt; on $P$, in the sense that the resulting measure assigns 0 probability to all vectors $(x_1, \dots, x_n)$ where $x_i \neq x&#39;_i$, for some constant $x&#39;_i$. The difference between them is that conditionalization, $P(x_1, \dots, x_n \mid x&#39;_i)$, simply rescales the resulting measure by $1/P(x&#39;_i)$ after domain restriction, whereas intervention, $P(x_1, \dots, x_n \mid \mathrm{do}(x&#39;_i))$, re-weights every single probability independently by $1/P(x&#39;_i \mid pa_i)$, where $pa_i$ is the set of values in $(x_1, \dots, x_n)$ for the parent nodes $PA_i$ of node $X_i$.&lt;/p&gt;
&lt;p&gt;Rewriting $P(x_1, \dots, x_n)$, we can see why multiplying by $1/P(x&#39;_i \mid pa_i)$ corresponds to an intervention:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
P(x_1, \dots, x_n) = \prod_{j=1}^n P(x_j \mid pa_j)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;by the chain rule of probability, because the graph also encodes which nodes are statistically independent, i.e. if $x_k \notin pa_j$, then $P(x_j \mid x_k) = P(x_j)$.&lt;/p&gt;
&lt;p&gt;The operation of removing the connections going into $X_i$ from the parents $PA_i$ is a matter of removing the term $P(x_i \mid pa_i)$ by dividing it out.&lt;/p&gt;
&lt;p&gt;This formulation of an intervention can be generalized further. Instead of setting $X_i$ to a constant value $x&#39;_i$, in general, we can replace the node distribution $P(X_i \mid PA_i)$ with the new distribution $Q(X_i \mid PA&#39;_i)$ where $PA&#39;_i$ is some new set of parents, which may or may not be the empty set, or equivalent to or overlap with the old parents $PA_i$. If $PA&#39;_i$ is empty, that is equivalent to making $Q$ statistically independent where $Q(X_i \mid PA_i) = Q(X_i)$. We can get our constant-value intervention by choosing a delta distribution (one-hot for discrete $X_i$, and Dirac delta for continuous $X_i$) $Q(X_i) = \d_{x&#39;_i}$ which is non-zero only if $X_i = x&#39;_i$. Now this general-case intervention is replacing the term $P(X_i \mid PA_i)$ with $Q(X_i \mid PA&#39;_i)$, which looks like this:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; P\left(x_1, \dots, x_n \Mid\, \Do\left\{P(x_i\mid pa_i) \to Q(x_i\mid pa&#39;_i)\right\}\ \right) \\ \\&lt;br&gt;
&amp;amp;=&lt;br&gt;
P(x_1, \dots, x_n)\frac{Q(x_i \mid pa&#39;_i)}{P(x_i \mid pa_i)}\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;When $Q(x_i \mid pa&#39;_i) = Q(x_i) = \d_{x&#39;_i}$ this expression reduces to the constant-value intervention defined above.&lt;/p&gt;
&lt;p&gt;In the functional perspective, an intervention replaces $f_i(pa_i, u_i)$ with some other function $f&#39;_i(pa&#39;_i, u_i)$.&lt;/p&gt;
&lt;h2 id=&#34;causal-effect&#34;&gt;Causal Effect&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;http://bayes.cs.ucla.edu/BOOK-2K/&#34;target=&#34;_blank&#34;&gt;Causality&lt;/a&gt;, definition 3.2.1, Pearl defines causal effect as follows:&lt;/p&gt;
&lt;p&gt;Let $X$ and $Y$ be two disjoint sets of graph nodes. The &lt;strong&gt;causal effect&lt;/strong&gt; of $X$ on $Y$ is the &lt;em&gt;function&lt;/em&gt; $\mc{E}$ from the space of node values for $X$ to the space of probability measures on $Y$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mc{E}(x) = P(Y \mid \Do(X=x))\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $x$ is some chosen vector of values for the nodes $X$.&lt;/p&gt;
&lt;p&gt;That is to say, the causal effect of nodes $X$ on nodes $Y$ is characterized by the set of all interventions obtained setting $X$ to every possible value $x$, where each intervention is characterized by a change in probability distribution on $Y$. That is to say, the causal effect of $X$ on $Y$ is characterized by how $P(Y \mid \Do(X=x))$ varies for different $x$, and compared to no intervention $P(Y)$.&lt;/p&gt;
&lt;h2 id=&#34;when-interventions-and-conditionalization-are-equivalent&#34;&gt;When Interventions And Conditionalization Are Equivalent&lt;/h2&gt;
&lt;p&gt;It should be obvious that when node $X_i$ has no parents then $P(x_{1:n} \mid \Do(x&#39;_i)) = P(x_{1:n} \mid x&#39;_i)$ for all node values $x&#39;_i$, because $PA_i = \emptyset$ and so $P(x&#39;_i \mid pa_i) = P(x&#39;_i)$.&lt;/p&gt;
&lt;p&gt;Another case is when we are only considering the marginal distribution on a subset of variables. Then the conditional distribution and intervention distribution on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_blanket&#34;target=&#34;_blank&#34;&gt;Markov blanket&lt;/a&gt; of that subset are equivalent.&lt;/p&gt;
&lt;p&gt;To see what I mean, let&amp;rsquo;s consider the Markov chain $X_1, \dots, X_n$ where $P(x_1, \dots, x_n) = P(x_n \mid x_{n-1})P(x_{n-1} \mid x_{n-2})\dots P(x_2 \mid x_1)P(x_1)$ and $PA_i = \set{X_{i-1}}$ for all $i &amp;gt; 1$. Then we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; P(x_n, \dots, x_{i+1} \mid \Do(x&#39;_i)) \\&lt;br&gt;
&amp;amp;\quad= \sum_{x_{i-1},\dots,x_1} P(x_n, \dots, x_1 \mid \Do(x&#39;_i)) \\&lt;br&gt;
&amp;amp;\quad= \begin{cases}&lt;br&gt;
\sum_{x_{i-1},\dots,x_1}\frac{P(x_n, \dots, x_{i+1}\mid x_i)P(x_i \mid x_{i-1})P(x_{i-1},\dots,x_1)}{P(x&#39;_i \mid x_{i-1})} &amp;amp; x_i=x&#39;_i \\&lt;br&gt;
0 &amp;amp; x_i \neq x&#39;_i&lt;br&gt;
\end{cases} \\&lt;br&gt;
&amp;amp;\quad= \sum_{x_{i-1},\dots,x_1}P(x_n, \dots, x_{i+1} \mid x&#39;_i)P(x_{i-1},\dots,x_1) \\&lt;br&gt;
&amp;amp;\quad= P(x_n, \dots, x_{i+1} \mid x&#39;_i)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210519122301.png&#34; alt=&#34;&#34;&gt;
&lt;/p&gt;
&lt;h1 id=&#34;causality-for-physics&#34;&gt;Causality For Physics&lt;/h1&gt;
&lt;p&gt;Pearl&amp;rsquo;s causality is based on the idea of the intervention, which is a kind of graph surgery.&lt;/p&gt;
&lt;p&gt;To apply Pearl&amp;rsquo;s causality to physics, we&amp;rsquo;d need to define what an intervention does to physical processes. There are two immediate problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pearl defines interventions for causal graphs, where node values are sampled i.i.d., and the nodes represent are stateless and otherwise  isolated processes (aside from their arrows). Physics, on the other hand, allows for arbitrary interactions between systems, to the point where the boundaries between systems may be blurred or destroyed so that it does not even make sense to think about there being any independent components at all (think about a liquid or gas). Physical processes are not i.i.d. (the future depends on the past), and they have internal state which determines their future time evolution.&lt;/li&gt;
&lt;li&gt;Classical physics is non-probabilistic (non-statistical Newtonian mechanics and relativity). If our notation of causality is to be suitable to all of physics, we need to apply to Newtonian mechanics, which means causality must precede probability. Therefore we need to define interventions on deterministic systems.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Pearl generally considers a graph intervention to represent an intervention that can conceivably be taken, and ideally taken recently so that the causal effect of various interventions can be empirically estimated with histograms (empirically estimate $P(Y \mid \Do(X))$ and $P(Y)$).&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t think physical plausible interventions can generalize to arbitrary physical systems. I will instead consider what I call a &lt;strong&gt;counterfactual intervention&lt;/strong&gt;, which is merely a modification to a mathematical model (i.e. representation) of physics. A counterfactual intervention is hypothetical, and produces a different time-line than the &amp;ldquo;factual&amp;rdquo; time-evolution of the system. A counterfactual intervention is the answer to the question, &amp;ldquo;what would have happened if the system were in state $x&#39;$ rather than state $x$ at time $t$?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;If intuition serves right and the logical structure of causality lies within all theories of physics, the purpose of the counterfactual intervention is to probe those theories to make their implicit causal structures mathematically explicit.&lt;/p&gt;
&lt;p&gt;My objective here is to define an abstract definition for theories of physics in general, define what it means to take a counterfactual intervention on a physical system (both probabilistic or non-probabilistic), and then to show the equivalence of this type of intervention to Pearl&amp;rsquo;s graph intervention above.&lt;/p&gt;
&lt;h2 id=&#34;abstract-physics&#34;&gt;Abstract Physics&lt;/h2&gt;
&lt;p&gt;In any theory of physics there is a state space $\O$. In Newtonian mechanics, state is a vector of various components of the system, such as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Canonical_coordinates#Definition_in_classical_mechanics&#34;target=&#34;_blank&#34;&gt;vector of positions and momenta&lt;/a&gt; given by $\o = (\vec{q}, \vec{p}) \in \O$. In general state can include other kinds of &lt;a href=&#34;https://en.wikipedia.org/wiki/Degrees_of_freedom_%28mechanics%29&#34;target=&#34;_blank&#34;&gt;degrees of freedom&lt;/a&gt; such as the orientation of solid bodies in 3D space. In quantum mechanics there is &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantum_state&#34;target=&#34;_blank&#34;&gt;quantum state&lt;/a&gt;, and state spaces are &lt;a href=&#34;https://en.wikipedia.org/wiki/Hilbert_space#Quantum_mechanics&#34;target=&#34;_blank&#34;&gt;Hilbert spaces&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;theory of physics&lt;/strong&gt; specifies both the state space $\O$ and how to solve for the time-evolution of the system given a particular state $\o_t$ at time $t$. The result is a complete description of a system&amp;rsquo;s time evolution through state space given as a state-function of time, $\s : \R \to \O : t \mapsto \s(t)$, which I&amp;rsquo;ll call a &lt;strong&gt;trajectory&lt;/strong&gt;. To be clear, a single trajectory $\s$ is a single possible time-evolution, e.g. where $\s(t) = \o_t$.&lt;/p&gt;
&lt;p&gt;The mathematical machinery that converts known information, e.g. the state of the system at time $t$, varies between theories of physics and often makes use of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Analytical_mechanics#Lagrangian_mechanics&#34;target=&#34;_blank&#34;&gt;Lagrangian&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Analytical_mechanics#Hamiltonian_mechanics&#34;target=&#34;_blank&#34;&gt;Hamiltonian&lt;/a&gt;. These details can be abstracted away.  In principle, for any theory of physics  there is a family of &lt;a href=&#34;https://en.wikipedia.org/wiki/Time_evolution#Time_evolution_operators&#34;target=&#34;_blank&#34;&gt;time-evolution functions&lt;/a&gt; $\t_{\Dt} : \O \to \O$, for every time interval $\Dt\in\R$ (both positive and negative) which maps any state $\o\in\O$ at time $t$ to the state at time $t+\Dt$. Typically physics is &lt;a href=&#34;https://en.wikipedia.org/wiki/T-symmetry&#34;target=&#34;_blank&#34;&gt;time-symmetric&lt;/a&gt;, which means that $\t_{\Dt}$ is a bijection and thus invertible. Note also that $\t_{\Dt}$ does not depend on the absolute time $t$, and so we are implicitly assuming the given theory of physics is &lt;a href=&#34;https://en.wikipedia.org/wiki/Time_translation_symmetry&#34;target=&#34;_blank&#34;&gt;time-translationally invariant&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The set of all trajectories is $\R \to \O$, &lt;a href=&#34;https://en.wikipedia.org/wiki/Function_space&#34;target=&#34;_blank&#34;&gt;denoting the set of all functions&lt;/a&gt; from $\R$ to $\O$. For a given time-evolution family $\t$, there is a subset of trajectories which are &lt;strong&gt;valid for $\t$&lt;/strong&gt; (or &lt;strong&gt;$\t$-valid&lt;/strong&gt;),&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\Sigma = \set{\s : \R\to\O \mid \fa t,\Dt\in\R : \s(t+\dt) = \t_\dt(\s(t))}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;h3 id=&#34;incorporating-probability&#34;&gt;Incorporating Probability&lt;/h3&gt;
&lt;p&gt;Suppose we want to work with some kind of statistical physics. Perhaps we are uncertain about which state the system is in, or the state is randomly chosen. We can just as easily put a probability measure on the set of trajectories.&lt;/p&gt;
&lt;p&gt;Let $M$ be a probability measure on the set of all trajectories $\R \to \O$. Moreover, we want to require $M$ to obey the physics of $\t$ and assign zero probability to physically impossible trajectories, i.e. $\t$-invalid trajectories. Specifically, $M$ should assign 0 probability to any set comprised &lt;em&gt;only&lt;/em&gt; of $\t$-invalid trajectories, or equivalently, $M(\Sigma) = 1$ (if $M$ is a normalized measure).&lt;/p&gt;
&lt;p&gt;This is not typically how statistical physics is conceived of. Normally, there is a probability measure on states at time $t$, and time evolution time-evolves that measure. Instead, I&amp;rsquo;ve put a static global measure $M$ on entire trajectories. However, these two views are equivalent.&lt;/p&gt;
&lt;p&gt;Let $\mu_t$ be the marginal probability measure on state space $\O$ of the &amp;ldquo;system&amp;rdquo; at time $t$. Specifically, $\mu_t$ is the unique marginal distribution of $M$ on time $t$ only, given by&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu_t(\mc{O}) = M\set{\s:\R\to\O \mid \s(t) \in \mc{O}}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for (measurable) state subsets $\mc{O}\subseteq \O$. Then $\mu_{t+\Dt}$ is then the time-evolution of measure $\mu_t$, given by&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu_{t+\Dt}(\mc{O}) = \mu_t(\t^{-1}_\Dt(\mc{O}))\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/em&gt; that $\mu_t(\t^{-1}_\Dt(\mc{O})) = M\set{\s:\R\to\O \mid \s(t+\Dt) \in \mc{O}}$:&lt;br&gt;
$M\set{\s:\R\to\O \mid \s(t+\Dt) \in \mc{O}}$&lt;br&gt;
$= M\set{\s\in\Sigma \mid \s(t+\Dt) \in \mc{O}} + M\set{\s\in\overline{\Sigma} \mid \s(t+\Dt) \in \mc{O}}$&lt;br&gt;
$= M\set{\s\in\Sigma \mid \s(t+\Dt) \in \mc{O}} + 0$.&lt;br&gt;
$\set{\s\in\Sigma \mid \s(t+\Dt) \in \mc{O}} = \set{\s\in\Sigma \mid \s(t) \in \t^{-1}_\Dt(\mc{O})}$ by the definition of $\Sigma$.&lt;br&gt;
$M\set{\s\in\Sigma \mid \s(t) \in \t^{-1}_\Dt(\mc{O})}$&lt;br&gt;
$= M\set{\s:\R\to\O \mid \s(t) \in \t^{-1}_\Dt(\mc{O})}$&lt;br&gt;
$= \mu_t(\t^{-1}_\Dt(\mc{O}))$ by the definition of $\mu_t$. $\qed$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/em&gt; that $M$ is uniquely determined by $\mu_t$, so long as $\t_\Dt$ is a bijection and $M(\Sigma)=1$.&lt;/p&gt;
&lt;p&gt;At time $t$, for each $\o\in\O$, there is a unique $\t$-valid trajectory $\s$ that passes through $\o$, given by the mapping $t&#39;\mapsto\t_{t&#39;-t}(\o)$. Therefore, there is a family of bijections between the $\t$-valid trajectories $\Sigma$ and state space $\O$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\Gamma_t : \O \to \Sigma : \o \mapsto(t&#39;\mapsto\t_{t&#39;-t}(\o))\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $t\in\R$. So $\Gamma_t(\o) = \s$ where $\s(t) = \o$ and $\s$ is $\t$-valid. The inverse is then $\Gamma^{-1}_t(\s)=\s(t)$. I haven&amp;rsquo;t defined proper measure spaces on trajectories and states, so I will just assume $\Gamma_t$ is a measurable function.&lt;/p&gt;
&lt;p&gt;We can derive the following relation:&lt;br&gt;
$\mu_t(\mc{O}) = M\set{\s:\R\to\O \mid \s(t) \in \mc{O}}$&lt;br&gt;
$=M\set{\s\in\Sigma \mid \s(t) \in \mc{O}}$&lt;br&gt;
$=M\Gamma_t\mc{O}$.&lt;/p&gt;
&lt;p&gt;Thus for any (measurable) subset of trajectories $S \subseteq (\R\to\O)$, there is a corresponding (measurable) subset of states $\mc{O} = \Gamma^{-1}_t(S\cap\Sigma)$ so that $M(S) = M(S\cap\Sigma) = M(\Gamma_t\Gamma^{-1}_t(S\cap\Sigma)) = M(\Gamma_t\mc{O}) = \mu_t(\mc{O})$. $\qed$&lt;/p&gt;
&lt;h2 id=&#34;interventions-for-physics&#34;&gt;Interventions For Physics&lt;/h2&gt;
&lt;p&gt;Let $I : \O\to\O$ be a &lt;strong&gt;state-replacement function&lt;/strong&gt;. Usually we want $I$ to be some kind of state projection function where $I(\O) \subset \O$ is a strict subset.&lt;/p&gt;
&lt;p&gt;I will define a &amp;ldquo;do&amp;rdquo;-operator on individual trajectories which performs a surgery and outputs a modified trajectory. Specifically, given state-replacement function $I$ and time $T$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\do{I}{T}[\s](t) \df \begin{cases}\s(t) &amp;amp; t &amp;lt; T \\ \t_{t-T}(I(\s(T))) &amp;amp; t\geq T\end{cases}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The resulting trajectory is identical to $\s$ prior to time $T$, and discontinuously jumps at time $T$ to the alternative $\t$-valid trajectory starting from state $I(\s(T))$. In this way, $I$ determines which trajectory &amp;ldquo;tail&amp;rdquo; to attach to the given trajectory &amp;ldquo;head&amp;rdquo; $\s$. The resulting &amp;ldquo;Frankenstein&amp;rdquo;-trajectory is usually not globally $\t$-valid, but its head and tail are guaranteed to be $\t$-valid, and thus is locally $\t$-valid everywhere &lt;em&gt;except&lt;/em&gt; across time $T$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s overload this &amp;ldquo;do&amp;rdquo;-operator to apply element-wise to sets, so&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\Sigma&#39; = \do{I}{T} \Sigma = \set{\do{I}{T}[\s] \Mid \s\in\Sigma}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Starting with the measure $M$ from above, applying $\do{I}{T}$ to the set of all trajectories $\R\to\O$ induces a transformed measure $M&#39;$, where for subsets $S&#39; \subseteq (\R\to\O)$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
M&#39;(S&#39;) = M\left(\do{I}{T}^{-1}S&#39;\right) = M\set{\s\in\Sigma \Mid \do{I}{T}[\s] \in S&#39;}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This conception of intervention is different from Pearl&amp;rsquo;s, which is a modification of the functions generating the behavior of the process in question. In my formulation, I am modifying the state of the process at some point in time, but keeping the behavior-generating-functions, i.e. $\t$, unchanged. That is to say, I am modifying systems, but not the physics, whereas Pearl is modifying the physics, so to speak.&lt;/p&gt;
&lt;h3 id=&#34;proof-of-pearl-equivalence&#34;&gt;Proof of Pearl-Equivalence&lt;/h3&gt;
&lt;p&gt;The question is whether my definition of an intervention is equivalent with Pearl&amp;rsquo;s. To prove this, I need to put my intervention in terms of Pearl&amp;rsquo;s setup.&lt;/p&gt;
&lt;p&gt;The measure $M$ on trajectories corresponds to the measure $P$ on graph states, and the transformed measure $M&#39;$ corresponds to the transformed measure $P(\ldots \mid \Do(\ldots))$ on the modified graph.&lt;/p&gt;
&lt;p&gt;Recall that $M&#39;$ is defined by&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
M&#39;(S&#39;) = M\set{\s\in\Sigma \Mid \do{I}{T}[\s] \in S&#39;}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for subsets $S&#39;\subseteq (\R\to\O)$ of trajectories. I need to show that $M&#39;$ has the same form as Pearl&amp;rsquo;s general intervention.&lt;/p&gt;
&lt;p&gt;It will help to define the following notation on trajectories:&lt;br&gt;
$\s\restr{(a,b)}$ is the domain restriction of trajectory $\s$ to time interval $(a,b)$.&lt;/p&gt;
&lt;p&gt;I will use the following short-hands:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\s_{&amp;gt;T} = \s\restr{(T,\infty)}$&lt;/li&gt;
&lt;li&gt;$\s_{&amp;lt;T} = \s\restr{(-\infty,T)}$&lt;/li&gt;
&lt;li&gt;$\s_{\ddT} = \s\restr{(T-\dt,T)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $\dt &amp;gt; 0$ be an arbitrarily small positive real number.&lt;/p&gt;
&lt;p&gt;The goal is to prove that&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
M&#39;(S&#39;) = \int_{S&#39;} \frac{Q(\s(T) \mid \s_{\ddT})}{M(\s(T) \mid \s_{\ddT})}  \dd M(\s)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for some probability measure $Q$. This is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Lebesgue_integration&#34;target=&#34;_blank&#34;&gt;Lebesgue integral&lt;/a&gt; w.r.t. $\s$ using measure $M$, which for our purposes is just the expectation of the integrand w.r.t. measure $M$. As a Riemann integral: $\int_{S&#39;} M(\s)\frac{Q(\s(T) \mid \s_{\ddT})}{M(\s(T) \mid \s_{\ddT})}  \dd \s$.&lt;/p&gt;
&lt;p&gt;It turns out that&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
Q(\s(T) \mid \s_{\ddT}) = M(I^{-1}(\s(T)) \mid \s_{\ddT})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We have,&lt;br&gt;
$M(\s) = M(\s_{&amp;gt;T} \mid \s(T)) \cdot M(\s(T) \mid \s_{\ddT}) \cdot M(\s_{&amp;lt;T})$,&lt;br&gt;
where $M(\s(T) \mid \s_{\ddT}) \cdot M(\s_{&amp;lt;T}) = M(\s(T) \mid \s_{&amp;lt;T})$ because $M$ is Markov w.r.t. time, which is guaranteed by the construction of $\Sigma$ from $\t_{\D t}$.&lt;/p&gt;
&lt;p&gt;Expanding out the integrand, we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; M(\s)\frac{Q(\s(T) \mid \s_{\ddT})}{M(\s(T) \mid \s_{\ddT})} \\ \\&lt;br&gt;
=\ &amp;amp; M(\s_{&amp;gt;T} \mid \s(T)) \cdot M(\s(T) \mid \s_{\ddT}) \cdot M(\s_{&amp;lt;T})\frac{Q(\s(T) \mid \s_{\ddT})}{M(\s(T) \mid \s_{\ddT})} \\ \\&lt;br&gt;
=\ &amp;amp; M(\s_{&amp;gt;T} \mid \s(T)) \cdot Q(\s(T) \mid \s_{\ddT}) \cdot M(\s_{&amp;lt;T}) \\ \\&lt;br&gt;
=\ &amp;amp; M(\s_{&amp;gt;T} \mid \s(T)) \cdot M(I^{-1}(\s(T)) \mid \s_{\ddT}) \cdot M(\s_{&amp;lt;T})\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The remainder of this proof consists of showing the following equivalences, which I&amp;rsquo;ll prove below as lemmas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$M(\s_{&amp;gt;T} \mid \s(T)) = M&#39;(\s_{&amp;gt;T} \mid \s(T))$&lt;/li&gt;
&lt;li&gt;$M(I^{-1}(\s(T)) \mid \s_{\ddT}) = M&#39;(\s(T) \mid \s_{\ddT})$&lt;/li&gt;
&lt;li&gt;$M(\s_{&amp;lt;T}) = M&#39;(\s_{&amp;lt;T})$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That allows us to rewrite:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; M(\s_{&amp;gt;T} \mid \s(T)) \cdot M(I^{-1}(\s(T)) \mid \s_{\ddT}) \cdot M(\s_{&amp;lt;T}) \\ \\&lt;br&gt;
=\ &amp;amp; M&#39;(\s_{&amp;gt;T} \mid \s(T)) \cdot M&#39;(\s(T) \mid \s_{\ddT}) \cdot M&#39;(\s_{&amp;lt;T}) \\ \\&lt;br&gt;
=\ &amp;amp; M&#39;(\s)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Therefore&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; \int_{S&#39;} \frac{Q(\s(T) \mid \s_{\ddT})}{M(\s(T) \mid \s_{\ddT})}  \dd M(\s) \\ \\&lt;br&gt;
=\ &amp;amp; \int_{S&#39;} \dd M&#39;(\s) \\ \\&lt;br&gt;
=\ &amp;amp; M&#39;(S&#39;)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\qed$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/em&gt; of remaining lemmas:&lt;/p&gt;
&lt;p&gt;The cases $M(\s_{&amp;gt;T} \mid \s(T)) = M&#39;(\s_{&amp;gt;T} \mid \s(T))$ and $M(\s_{&amp;lt;T}) = M&#39;(\s_{&amp;lt;T})$ are easy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Because the trajectory before $T$ is unchanged,&lt;br&gt;
$M(\s_{&amp;lt;T}) = M&#39;(\s_{&amp;lt;T})$,&lt;/li&gt;
&lt;li&gt;Because time evolution from $T$ onward is deterministic and obeys $\t_\dt$, there is exactly one trajectory $\s^*$ that is valid under $\t_\dt$ s.t. $\s^*(T) = \s(T)$. Thus&lt;br&gt;
$M(\s_{&amp;gt;T} \mid \s(T)) = M&#39;(\s_{&amp;gt;T} \mid \s(T)) = \begin{cases}1 &amp;amp; \s_{\geq T} = \s^*_{\geq T} \\ 0 &amp;amp; \mathrm{otherwise}\end{cases}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now to prove $M(I^{-1}(\s(T)) \mid \s_{\ddT}) = M&#39;(\s(T) \mid \s_{\ddT})$. Expanding out $M(I^{-1}(\s(T)) \mid \s_{\ddT})$, we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp;M(I^{-1}(\s(T)) \mid \s_{\ddT}) \\&lt;br&gt;
=\ &amp;amp; M\set{\z \in \Sigma \mid \z(T) \in I^{-1}(\s(T)) \wedge \z\restr{(T-\dt,T)} = \s_{\ddT}}\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Since&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\do{I}{T}[\z](t) = \begin{cases}\z(t) &amp;amp; t &amp;lt; T \\ \t_{t-T}(I(\z(T))) &amp;amp; t\geq T\end{cases}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;then $\z(T) \in I^{-1}(\s(T)) \iff I(\z(T)) = \s(T) \iff \do{I}{T}[\z](T) = \s(T)$,&lt;br&gt;
because $\t_0$ is the identity function.&lt;/p&gt;
&lt;p&gt;Furthermore, $\do{I}{T}[\z]\restr{(-\infty,T)} = \z\restr{(-\infty,T)}$,&lt;br&gt;
so $\z\restr{(T-\dt,T)} = \s_{\ddT} \iff \do{I}{T}[\z]\restr{(T-\dt,T)} = \s_{\ddT}$.&lt;/p&gt;
&lt;p&gt;Thus we can further expand out $M(I^{-1}(\s(T)) \mid \s_{\ddT})$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; M(I^{-1}(\s(T)) \mid \s_{\ddT}) \\&lt;br&gt;
=\ &amp;amp; M\set{\z \in \Sigma \mid \z(T) \in I^{-1}(\s(T)) \wedge \z\restr{(T-\dt,T)} = \s_{\ddT}} \\&lt;br&gt;
=\ &amp;amp; M\set{\z \in \Sigma \,\middle|\, \do{I}{T}[\z](T) = \s(T) \wedge \do{I}{T}[\z]\restr{(T-\dt,T)} = \s_{\ddT}} \\&lt;br&gt;
=\ &amp;amp; M&#39;(\s(T) \mid \s_{\ddT})\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\qed$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;There is one problematic aspect to this equivalence. Taking another look at the first step in the proof,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; M(\s)\frac{Q(\s(T) \mid \s_{\ddT})}{M(\s(T) \mid \s_{\ddT})} \\ \\&lt;br&gt;
=\ &amp;amp; M(\s_{&amp;gt;T} \mid \s(T)) \cdot M(\s(T) \mid \s_{\ddT}) \cdot M(\s_{&amp;lt;T})\frac{Q(\s(T) \mid \s_{\ddT})}{M(\s(T) \mid \s_{\ddT})} \\ \\&lt;br&gt;
=\ &amp;amp; M(\s_{&amp;gt;T} \mid \s(T)) \cdot Q(\s(T) \mid \s_{\ddT}) \cdot M(\s_{&amp;lt;T})\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;we make the assumption that $M(\s(T) \mid \s_{\ddT}) \neq 0$. Given the trajectory slice $\s_{\ddT} = \s\restr{(T-\dt,T)}$, there is only one $\t$-valid trajectory which shares the same slice, and so there is only one valid state $\o^*_T$ at time $T$ to follow from $\s\restr{(T-\dt,T)}$. Since $M$ obeys $\t$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
M(\s(T) \mid \s_{\ddT}) = \d_{\o^*_T}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is non-zero only if $\s(T) = \o^*_T$. If $\s\restr{(T-\dt,T)}$ is itself not $\t$-valid, we can define $M(\s(T) \mid \s_{\ddT})$ to be an improper probability measure that is always $0$.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d argue that interventions on deterministic trajectories is a limiting case of interventions on probabilistic trajectories where the transition probabilities converge to delta distributions. Then $M(\s(T) \mid \s_{\ddT})/M(\s(T) \mid \s_{\ddT})\to 1$ no matter what and the cancellation works.&lt;/p&gt;
&lt;h2 id=&#34;compatibility-with-modern-physics&#34;&gt;Compatibility with modern physics&lt;/h2&gt;
&lt;p&gt;The generalized formulation of physics, using state space $\O$ and time-evolution function $\t_{\D t}$, is compatible with classical physics and special relativity (for arbitrary choice of Lorentz frame). Is it compatible with quantum mechanics, general relativity, and beyond?&lt;/p&gt;
&lt;p&gt;It is compatible with QM if we are time-evolving quantum state and disregarding measurement. If we wanted to model stochastic measurement outcomes, or stochastic interactions in general, then we could do that using a non-deterministic time-evolution function, i.e. $\t_{\D t}$ is not a proper function and assigns more than one output to a given input. Alternatively, the state $\O$ could contain algorithmically random data which serves as a source of random inputs for $\t_{\D t}$.&lt;/p&gt;
&lt;p&gt;For special relativity, simultaneity is relative, but consistently holding to an arbitrary choice of Lorentz frame will work. Then, there is a $\t_{\D t}$ for every Lorentz frame, and one can transform between these time-evolution functions via Lorentz boosts.&lt;/p&gt;
&lt;p&gt;For general relativity, I am not personally clear on whether there exist global reference frames where there is a single simultaneous state of the universe, even if what is regarded as simultaneous is arbitrarily chosen. In that case, my formulation may break down. However, there should still be a causal DAG. Is it possible to topologically sort that DAG and then organize it into something like time slices? Each such slice would then correspond to a state in $\O$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modular Neural Networks</title>
      <link>https://danabo.github.io/blog/posts/modular-neural-networks/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/modular-neural-networks/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\s}{\sigma}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\H}{\mb{H}}&lt;br&gt;
\newcommand{\I}{\mb{I}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[2]{#1_{\mid #2}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;I wrote up these notes in preparation for my guest lecture in Tom Dean&amp;rsquo;s Stanford course, &lt;a href=&#34;https://web.stanford.edu/class/cs379c/&#34;target=&#34;_blank&#34;&gt;CS379C: Computational Models of the Neocortex&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Selected papers&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.04227&#34;target=&#34;_blank&#34;&gt;Towards Modular Algorithm Induction&lt;/a&gt; (Abolafia et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Modular Networks: Learning to Decompose Neural Computation&lt;/a&gt; (Kirsch et al.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;what&#34;&gt;What?&lt;/h1&gt;
&lt;p&gt;What do the phrases &amp;ldquo;modular architectures&amp;rdquo; and &amp;ldquo;learning modular structures&amp;rdquo; mean?&lt;/p&gt;
&lt;p&gt;In programming, a module is a reusable function. Modularity is a design principle, where code is composed of smaller functions which have well defined behavior in isolation, so that the system can be understood by looking at its parts (i.e. &lt;a href=&#34;https://en.wikipedia.org/wiki/Reductionism#In_science&#34;target=&#34;_blank&#34;&gt;reduction&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Modular code satisfies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Isolation&lt;/strong&gt;: The internal behavior of one module doesn&amp;rsquo;t affect other modules.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reusability&lt;/strong&gt;: The same module applied in different circumstances, potentially given different kinds of data that follow the same pattern (think &lt;a href=&#34;https://en.wikipedia.org/wiki/Generic_programming&#34;target=&#34;_blank&#34;&gt;generics&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Abstract_type&#34;target=&#34;_blank&#34;&gt;abstract types&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the context of machine learning, a modular neural architecture is a type of neural network composed of smaller neural modules. If data can be said to contain modular structure (e.g. see &lt;a href=&#34;https://arxiv.org/abs/1902.07181&#34;target=&#34;_blank&#34;&gt;Andreas 2019&lt;/a&gt;), then one goal of modular neural networks is to recover that latent structure.&lt;/p&gt;
&lt;p&gt;Pictorial examples of modular neural networks:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210413141842.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;[Kirsch et al.](https://arxiv.org/abs/1811.05249)&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210413141514.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;[Chang et al.](https://arxiv.org/abs/1807.04640)&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210413141932.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;[Abolafia et al.](https://arxiv.org/abs/2003.04227)&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;why&#34;&gt;Why?&lt;/h1&gt;
&lt;p&gt;Why have modular neural architectures? Is it beneficial for program synthesis? Is it beneficial for machine learning in general?&lt;/p&gt;
&lt;h2 id=&#34;data-invariances&#34;&gt;Data invariances&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.06765&#34;target=&#34;_blank&#34;&gt;Modularity Matters: Learning Invariant Relational Reasoning Tasks&lt;/a&gt;  (Jo et al.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When a dataset has a large number of invariances, a machine learning model must learn to associate a large number of seemingly unrelated patterns with one another, which may exacerbate the interference problem &amp;hellip; One natural way to combat the interference problem is to allow for specialized sub-modules in our architecture. Once we modularize, we reduce the amount of interference that can occur between features in our model. These specialized modules can now learn highly discriminative yet invariant representations while not interfering with each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;strong-generalization&#34;&gt;Strong generalization&lt;/h2&gt;
&lt;p&gt;Strong generalization means getting the right answer for an input/task that is very different from the training regime. Sometimes this is called zero-shot learning. Humans seem to be able to this. How does it work?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.07088&#34;target=&#34;_blank&#34;&gt;Learning Modular Neural Network Policies for Multi-Task and Multi-Robot Transfer&lt;/a&gt; (Devin et al.)&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210413144242.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;The robot and task networks are trained end-to-end on different robot-task combinations, with some held out. For example, during training this system does not encounter robot 2 combined with task 2, but does encounter robot 2 and task 2 separately in different situations. At test time, the system has to perform well when robot 2 is combined with task 2.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.01796&#34;target=&#34;_blank&#34;&gt;Modular Multitask Reinforcement Learning with Policy Sketches&lt;/a&gt; (Andreas 2016)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The modular structure of our approach, which associates every high-level action symbol with a discrete subpolicy,naturally induces a library of interpretable policy fragments that are easily recombined. This makes it possible to evaluate our approach under a variety of different data conditions: (1) learning the full collection of tasks jointly via reinforcement, (2) in a zero-shot setting where a policy sketch is available for a held-out task, and (3) in a adaptation setting, where sketches are hidden and the agent must learn to adapt a pretrained policy to reuse high-level actions in a new task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;hellip; we have shown that it is possible to build agents that share behavior across tasks in order to achieve success in tasks with sparse and delayed rewards. This process induces an inventory of reusable and interpretable subpolicies which can be employed for zero-shot generalization when further sketches are available, and hierarchical reinforcement learning when they are not.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;parameter-sharing&#34;&gt;Parameter sharing&lt;/h2&gt;
&lt;p&gt;Convolutional and recurrent layers can be viewed as modular, in that they &amp;ldquo;stamp&amp;rdquo; a small neural network (with the same parameters) repeatedly in some pattern - repeated over space for CNNs, and repeated over time for RNNs.&lt;/p&gt;
&lt;h2 id=&#34;causal-learning&#34;&gt;Causal learning&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.10893&#34;target=&#34;_blank&#34;&gt;Recurrent Independent Mechanisms&lt;/a&gt; (Goyal et al.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Physical processes in the world often have a modular structure which human cognition appears toexploit, with complexity emerging through combinations of simpler subsystems. Machine learningseeks to uncover and use regularities in the physical world. Although these regularities manifestthemselves as statistical dependencies, they are ultimately due to dynamic processes governed bycausal physical phenomena.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The notion of independent or autonomous mechanisms has been influential in the field of causal inference. A complex generative model, temporal or not, can be thought of as the composition of independent mechanisms or âcausalâ modules. In the causality community, this is often considered a prerequisite for being able to perform localized interventions upon variables determined by such models (Pearl, 2009). It has been argued that the individual modules tend to remain robust or invariant even as other modules change, e.g., in the case of distribution shift (SchÃ¶lkopf et al., 2012; Peterset al., 2017). This independence is not between the random variables being processed but between the description or parametrization of the mechanisms: learning about one should not tell us anything about another, and adapting one should not require also adapting another. One may hypothesize that if a brain is able to solve multiple problems beyond a single i.i.d. (independent and identically distributed) task, they may exploit the existence of this kind of structure by learning independent mechanisms that can flexibly be reused, composed and re-purposed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An excerpt from Judea Pearl&amp;rsquo;s book &lt;a href=&#34;http://bayes.cs.ucla.edu/BOOK-2K/&#34;target=&#34;_blank&#34;&gt;Causality, 2nd ed.&lt;/a&gt;. Pearl refers to &amp;ldquo;mechanisms&amp;rdquo; as the nodes in a Bayesian network (e.g. depicted in figure 1.2), which are assumed to be modular: i.e. they are internally isolated, apart from causation traveling along their arrows, and they are reusable in the sense that the graph can be modified, which Pearl calls an intervention.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210413224425.png&#34; alt=&#34;&#34;&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The example reveals a stronger sense in which causal relationships are more stable than the corresponding probabilistic relationships, a sense that goes beyond their basic ontologicalâepistemological difference. The relationship, âTurning the sprinkler on would not affect the rain,â will remain invariant to changes in the mechanism that regulates how seasons affect sprinklers. In fact, it remains invariant to changes in all mechanisms shown in this causal graph. We thus see that causal relationships exhibit greater robustness to ontological changes as well; they are sensitive to a smaller set of mechanisms. More specifically, and in marked contrast to probabilistic relationships, causal relationships remain invariant to changes in the mechanism that governs the causal variables ($X_3$ in our example).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Regardless of what use is eventually made  of our âunderstandingâ of things, we surely would prefer an understanding in terms of durable relationships, transportable across situations, over those based on transitory relationships. The sense of âcomprehensibilityâ that accompanies an adequate explanation is a natural by-product of the transportability of (and hence of our familiarity with) the causal relationships used in the explanation. It is for reasons of stability that we regard the falling barometer as predicting but not explaining the rain; those predictions are not transportable to situations where the pressure surrounding the barometer is controlled by artificial means. True understanding enables predictions in such novel situations, where some mechanisms change and others are added. It thus seems reasonable to suggest that, in the final analysis, the explanatory account of causation is merely a variant of the manipulative account, albeit one where interventions are dormant. Accordingly, we may as well view our unsatiated quest for understanding âhow data is generatedâ or âhow things workâ as a quest for acquiring the ability to make predictions under a wider range of circumstances, including circumstances in which things are taken apart, reconfigured, or undergo spontaneous change.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;program-induction-and-synthesis&#34;&gt;Program induction and synthesis&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1804.00218&#34;target=&#34;_blank&#34;&gt;HOUDINI: Lifelong Learning as Program Synthesis&lt;/a&gt; (Valkov et al.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In contrast to standard methods for transfer learning in deep networks, which re-use the first few layers of the network, neural libraries have the potential to enable reuse of higher, more abstract levels of the network, in what could be called high-level transfer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;hellip; our results indicate that the modular representation used in HOUDINI allows it to transfer high-level concepts and avoid negative transfer. We demonstrate that HOUDINI offers greater transfer than progressive neural networks and traditional âlow-levelâ transfer, in which early network layers are inherited from previous tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.13404&#34;target=&#34;_blank&#34;&gt;Towards modular and programmable architecture search&lt;/a&gt; (Negrinho et al.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The building blocks of our search spaces are modules and hyper-parameters. Search spaces are created through the composition of modules and their interactions.Implementing a new module only requires dealing with aspects local to the module. Modules and hyperparameters can be reused across search spaces, and new search spaces can be written by combining existing search spaces.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;how&#34;&gt;How?&lt;/h1&gt;
&lt;p&gt;How can modularity be achieved? Two things need to be simultaneously learned:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The modules themselves.&lt;/li&gt;
&lt;li&gt;How the modules are to be connected together.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Key ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Routing&lt;/strong&gt;: How the modules are connected together.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt; vs &lt;strong&gt;static&lt;/strong&gt; routing: Static routing is fixed for all inputs, while dynamic routing is conditioned on a given input or context. A router is a function that outputs module routing given context.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Soft&lt;/strong&gt; vs &lt;strong&gt;hard&lt;/strong&gt; routing: Soft routing is a weighted sum across all choices, while hard routing is a single discrete choice. Soft routing is differentiable while hard routing is not.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples of soft routing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1410.5401&#34;target=&#34;_blank&#34;&gt;Neural Turing Machines&lt;/a&gt; (Graves et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/nature20101&#34;target=&#34;_blank&#34;&gt;Differentiable neural computers&lt;/a&gt; (Graves et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.08228&#34;target=&#34;_blank&#34;&gt;Neural GPUs Learn Algorithms&lt;/a&gt; (Kaiser et al.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples of hard routing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1807.04640&#34;target=&#34;_blank&#34;&gt;Automatically Composing Representation Transformations as a Means for Generalization&lt;/a&gt; (Chang et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.06392&#34;target=&#34;_blank&#34;&gt;Neural Random-Access Machines&lt;/a&gt; (Kurach et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.07275&#34;target=&#34;_blank&#34;&gt;Learning Simple Algorithms from Examples&lt;/a&gt; (Zaremba et al.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is there something in between soft and hard routing?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1701.06538&#34;target=&#34;_blank&#34;&gt;Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer&lt;/a&gt; (Shazeer et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Modular Networks: Learning to Decompose Neural Computation&lt;/a&gt; (Kirsch et al.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;module-routing-in-detail&#34;&gt;Module routing in detail&lt;/h2&gt;
&lt;p&gt;Following the setup in &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;In the case of soft routing, think of module weights as probabilities. Instead of summing module outputs, sum probabilities of each possible output. This leads to a natural correspondence between soft and hard routing, where hard routing is sampled from the probability distribution.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt;, the choices are organized into layers, where for each layer $l$ a subset of $M$ modules are selected and their outputs are summed. In &lt;a href=&#34;https://arxiv.org/abs/2003.04227&#34;target=&#34;_blank&#34;&gt;Abolafia et al.&lt;/a&gt;, modules are connected into arbitrary computation graphs.&lt;/p&gt;
&lt;p&gt;To keep things general, let $\mc{A}$ be a set of possible module routing choices, so that an element $a\in\mc{A}$ consists of all  choices needed for the architecture to produce an output (e.g. which modules to use in the computation graph and how they are connected).&lt;/p&gt;
&lt;p&gt;Given a routing choice $a\in\mc{A}$, the architecture output probability given input $x$ and parameters $\t$ is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y\mid x,a,\t)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Under the hood this output probability may be arrived at by combining the following components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A collection of differentiable functions (modules) $f_{\t\up{i}}\up{i} : \mc{Z}\up{i} \to \mc{Z}\up{j}$ where $\mc{Z}\up{i}$ and $\mc{Z}\up{j}$ are latent spaces (e.g. vectors in $\R^n$), and $\t\up{i}$ are the function parameters. Typically $\t = (\t\up{1},\t\up{2},\dots)$ and each module has independent and isolated parameters. The input $x$ may be initially encoded into a latent space, or some modules may have $\mc{Z} = \mc{X}$ and the data can be fed in directly.&lt;/li&gt;
&lt;li&gt;A function $g$ that takes routing specification $a$, modules $\set{f_i}$, and input $x$, and outputs hidden representation $h$ (also a real vector).&lt;/li&gt;
&lt;li&gt;A distribution &amp;ldquo;head&amp;rdquo; on output space $\mc{Y}$, s.t. $h$ represents the parameters of the distribution. For example, a Gaussian $\mc{N}(y \mid h)$ where $h$ encodes a vector of means and a covariance matrix.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Putting these three things together produces the probability distribution $p(y\mid x,a,\t)$. Given training target $y$, this probability is fully differentiable w.r.t. $\t$ (typically given by the architecture as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logit&#34;target=&#34;_blank&#34;&gt;logit&lt;/a&gt;, or log-probability). If routing choice $a$ were held fixed, we can train this architecture with standard supervised techniques, e.g. with SGD. Furthermore, if $a\up{k}$ can be provided by some external hard-coded system given training example $(x\up{k}, y\up{k})$, then we can use SGD to maximize the dataset log-probability&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\sum_{k=1}^N \log p(y\up{k}\mid x\up{k},a\up{k},\t)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is essentially the used in &lt;a href=&#34;https://arxiv.org/abs/1807.04640&#34;target=&#34;_blank&#34;&gt;Chang et al.&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1511.02799&#34;target=&#34;_blank&#34;&gt;Andreas et al.&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ideally, we&amp;rsquo;d like to learn the module routing $a\up{k}$ for each training example $(x\up{k}, y\up{k})$. That means learning a routing function, which produces a routing $a\up{k}$ given input $x\up{k}$. Assuming $\mc{A}$ is a discrete space (routing choices are discrete, e.g. which modules to pick and how to connect them), we cannot differentiate such a function. RL could be used, as in &lt;a href=&#34;https://arxiv.org/abs/2003.04227&#34;target=&#34;_blank&#34;&gt;Abolafia et al.&lt;/a&gt;, but &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt; provides a more general perspective that allows for differentiation in principle, though intractable in practice.&lt;/p&gt;
&lt;p&gt;Let the routing function output a probability distribution over $a\in\mc{A}$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(a \mid x, \p)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This router is comprised of a function $r_\p$, parameterized by parameters $\p$, that takes in $x$ and outputs a hidden representation $h&#39;$ that, like before, holds the parameters for some probability distribution on $\mc{A}$ (e.g. Gaussian). If $a$ decomposes into a set of independent routing choices, e.g. $a=(a_1, a_2, \dots)$, then $p(a \mid x, \p) = \prod_t p(a_t \mid z_t, \p)$ where $\set{z_t}$ are potentially intermediate outputs from various modules, and at least one $z_t$ equals $x$.&lt;/p&gt;
&lt;p&gt;This gives us a joint distribution on $y$ and $a$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y, a \mid x,\t,\p) = p(y\mid x,a,\t)p(a\mid x, \p)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;To find the probability of $y$ given $x$, independent of routing choice $a$, we marginalize out $a$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y \mid x, \t, \p) = \sum_{a\in\mc{A}} p(y\mid x,a,\t)p(a\mid x, \p)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;What this means in terms of training, is that if we are using naive SGD to maximize $\log p(y \mid x, \t, \p)$ w.r.t $\t$ and $\p$ jointly, then we simply sum over all possible routing choices in $\mc{A}$. In practice, this summation is intractable, since the number of routing decisions explodes as the number of modules is increased (as well as with other complexities like multi-input and multi-output modules).&lt;/p&gt;
&lt;p&gt;We can view the same optimization through the lense of the REINFORCE algorithm, which naturally leads to RL optimization, where $p(a\mid x, \p)$ is the policy, $a$ are actions, and $x$ are environment observations. The reward function is then $R(a\mid x) = p(y\mid x,a,\t)$ (or the log-probability of $y$ like in &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt;). Let the loss function $\mc{L} = p(y \mid x, \t, \p)$. Then the gradient $\nabla_\p \mc{L}$ is given using the &lt;a href=&#34;https://dallascard.github.io/the-reinforce-trick.html&#34;target=&#34;_blank&#34;&gt;&amp;ldquo;log-trick&amp;rdquo;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\nabla_\p \mc{L} &amp;amp;= \nabla_\p\E_{p(a\mid x, \p)}[p(y\mid x,a,\t)] \\&lt;br&gt;
&amp;amp;= \nabla_\p\E_{p(a\mid x, \p)}[R(a \mid x)] \\&lt;br&gt;
&amp;amp;= \E_{p(a\mid x, \p)}[\nabla_\p\log p(a\mid x, \p) R(a \mid x)]\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is the gradient as given in REINFORCE.&lt;/p&gt;
&lt;p&gt;For the experiments in &lt;a href=&#34;https://arxiv.org/abs/2003.04227&#34;target=&#34;_blank&#34;&gt;Abolafia et al.&lt;/a&gt;, I found that scaling up episode collection as much as possible using &lt;a href=&#34;https://arxiv.org/abs/1802.01561&#34;target=&#34;_blank&#34;&gt;IMPALA&lt;/a&gt; partially overcame the usual issues associated with RL: sparse reward, high variance gradients, and lack of stability. Stability is especially an issue when the modules are being trained at the same time, so that the reward being optimized is a moving target. We were not able to jointly learn module and routing, so in &lt;a href=&#34;https://arxiv.org/abs/2003.04227&#34;target=&#34;_blank&#34;&gt;Abolafia et al.&lt;/a&gt; we settled for hard coded modules and focused on getting the router to work.&lt;/p&gt;
&lt;h2 id=&#34;other-kinds-of-training&#34;&gt;Other kinds of training&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt; introduces a modified &lt;a href=&#34;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&#34;target=&#34;_blank&#34;&gt;EM algorithm&lt;/a&gt; for simultaneously training $\t$ and $\p$, where routing choice $a$ is the latent variable. As with EM, this is a two step iterative process, where joint probability of $a$ and $x$ is maximized with fixed $a$, and then $a$ is locally improved according to the current probability distribution. Rather than finding the argmax $a$, which is intractable, &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt; samples an i.i.d. batch from $p(a \mid x, \p)$ and updates $a$ only if a higher probability $a$ is found.&lt;/p&gt;
&lt;p&gt;This training algorithm can be viewed as performing policy gradient training (RL) with a &amp;ldquo;top-$K$&amp;rdquo; buffer, as described in &lt;a href=&#34;https://arxiv.org/abs/1801.03526&#34;target=&#34;_blank&#34;&gt;Neural Program Synthesis with Priority Queue Training&lt;/a&gt; (Abolafia 2018), where $K=1$.&lt;/p&gt;
&lt;p&gt;Here is the general case of this training procedure:&lt;/p&gt;
&lt;p&gt;Let $\tilde{A}_n$ be a length-$S$ i.i.d. sample from $p(a_n \mid x_n,\p)$.&lt;br&gt;
Let $A_n^*$ be a top-$K$ buffer for the $n$-th training example $(x\up{n},y\up{n})$. That means $A_n^*$ holds the best $a_n$ observed over the course of training, scored by $p(a_n \mid x_n,\p)$. Note that this is a moving target, since $\p$ is simultaneously changing, so the joint score of $A_n^*$ can decrease.&lt;br&gt;
Let $A_n = \tilde{A}_n \cup A_n^*$.&lt;/p&gt;
&lt;p&gt;Let $B \subseteq D$ be a minibatch. The Monte Carlo gradient approximation (ala REINFORCE) is:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\nabla_\p\mc{L} \approx \frac{1}{\abs{B}}\sum_{n\in B}\frac{1}{\abs{A_n}}\sum_{a_n \in A_n} \nabla_\p\log p(a\mid x, \p) R(a \mid x)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $R(a\mid x) = p(y\mid x,a,\t)$ or $\log p(y\mid x,a,\t)$.&lt;/p&gt;
&lt;p&gt;If $A_n^* = \emptyset$ this setup reduces to usual policy gradient training, and if $\tilde{A}_n = \emptyset$ and $\abs{A_n^*}=1$ (i.e. $K=1$) this reduces to the EM algorithm (Algorithm 1) in &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.03526&#34;target=&#34;_blank&#34;&gt;Abolafia 2018&lt;/a&gt; is similar (where $K\geq 1$), except that the reward function is assumed to be fixed through out training, so the top-$K$ buffer $A_n^*$ cannot diminish in total score over time.&lt;/p&gt;
&lt;h3 id=&#34;information-theory&#34;&gt;Information theory&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt; plots the quantities $H_a$ and $H_b$ as diagnostic tools and measures of &amp;ldquo;module collapse&amp;rdquo; and training convergence. I used the same quantities in my own experiments and found them to be similarly helpful.&lt;/p&gt;
&lt;p&gt;There is theoretical justification for these quantities, and they may even be used as regularizers.&lt;/p&gt;
&lt;p&gt;Let $A$ be the routing random variable and let $X$ be the input random variable, distributed jointly by $p(a \mid x, \p)p(x)$ where $p(x)$ is the true and unknown input distribution.&lt;/p&gt;
&lt;p&gt;The mutual information between $A$ and $X$ decomposes:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\I(A, X) = \H(A) - \H(A \mid X)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We cannot explicitly compute these entropy terms because we do not have access to $p(x)$ or $p(a \mid \p)$.&lt;/p&gt;
&lt;p&gt;Assume we can only explicitly compute $p(a \mid x, \p)$. Let&amp;rsquo;s also suppose that we can explcitily compute $\H(A \mid X=x) = -\sum_{a\in\mc{A}} p(a \mid x, \p) \log p(a \mid x, \p)$. Though summing over $\mc{A}$ is still intractable, if we suppose that the routing distribution factors into independent choices which are themselves tractable to enumerate (like the choice for each layer in &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt;): $\mc{A} = \mc{A}_1\times\mc{A}_2\times\dots$ and $a = (a_1, a_2, \dots) \in \mc{A}$ s.t.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(a \mid x, \p) = \prod_{l=1}^L p(a_l \mid x, \p_l)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;then we can tractably compute the conditional entropy as the sum of entropies of each choice:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\H(A \mid X=x) &amp;amp;= \sum_{l=1}^L \H(A_l \mid X=x) \\&lt;br&gt;
&amp;amp;= \sum_{l=1}^L \sum_{a_l\in\mc{A}_l} p(a_l \mid x, \p_l) \log p(a_l \mid x, \p_l)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Now we can do a Monte Carlo approximation of the entropy terms of interest using the existing dataset. The statistical properties of this MC estimation is discussed in &lt;a href=&#34;https://arxiv.org/abs/1905.06922&#34;target=&#34;_blank&#34;&gt;On Variational Bounds of Mutual Information&lt;/a&gt;. Let $B \subseteq D$ be a minibatch uniformly sampled from dataset $D$.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\H(A \mid X) &amp;amp;= -\E_{p(x)}[\H(Y\mid X=x)] \\&lt;br&gt;
&amp;amp;\approx \frac{1}{\abs{B}}\sum_{n \in B} \H(Y\mid X=x_n)&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Letting $q(a \mid \p)$ be a Monte Carlo approximation of the marginal distribution $p(a\mid \p)$, derived from&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(a \mid \p) &amp;amp;= \E_{p(x)}p(a \mid x, \p) \\ &amp;amp;\approx \frac{1}{\abs{B}}\sum_{n \in B}p(a \mid x, \p) \\&lt;br&gt;
&amp;amp;= q(a\mid \p)\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;we can approximate the marginal entropy,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\H(A) &amp;amp;= -\E_{p(a \mid \p)}[\log p(a \mid \p)] \\&lt;br&gt;
&amp;amp;\approx -\E_{q(a \mid \p)}[\log q(a\mid \p)]\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We can think of $\I$ as measuring the bijectivness of the mapping from $x$ to $a$, where $\H(A)$ measures surjectivity and $\H(A\mid X)$ measures anti-injectivity. See &lt;a href=&#34;http://zhat.io/articles/primer-shannon-information#expected-mutual-information&#34;&gt;http://zhat.io/articles/primer-shannon-information#expected-mutual-information&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;$\H(A)$ measures module use. If its maximized, all the modules are getting used equally often.&lt;br&gt;
$\H(A\mid X)$ measures convergence. If its maximized, then the router is completely confident that there is exactly one appropriate routing choice for any given input.&lt;/p&gt;
&lt;p&gt;The ideal is that all modules get used often and the router thinks there is one routing choice per input.&lt;/p&gt;
&lt;p&gt;$\H(A)$ is a typical RL regularizer (called entropy regularization, see &lt;a href=&#34;https://arxiv.org/abs/1602.01783&#34;target=&#34;_blank&#34;&gt;A3C&lt;/a&gt;). However maximizing $\I(A,X)$ (where $x$ are env states) is uncommon. It may be unnecessary since $p(a \mid x)$ naturally becomes peaky over the course of training.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian information theory</title>
      <link>https://danabo.github.io/blog/posts/bayesian-information-theory/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/bayesian-information-theory/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[2]{#1_{\mid #2}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\tr}{\rightarrowtail}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\newcommand{\H}{\Omega}$&lt;/p&gt;
&lt;p&gt;Shannon&amp;rsquo;s information theory defines quantity of information (e.g. &lt;a href=&#34;https://en.wikipedia.org/wiki/Information_content#Definition&#34;target=&#34;_blank&#34;&gt;self-information&lt;/a&gt; $-\lg p(x)$) in terms of probabilities. In the context of data compression, these probabilities are given a frequentist interpretation (Shannon makes this interpretation explicit in his &lt;a href=&#34;http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf&#34;target=&#34;_blank&#34;&gt;1948 paper&lt;/a&gt;). In &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/&#34;&gt;Deconstructing Bayesian Inference&lt;/a&gt;, I introduced the idea of a subjective data distribution. If quantities of information are calculated using a subjective data distribution, what is their meaning? Below I will answer this question by building from the ground up a different notion of Bayesian inference.&lt;/p&gt;
&lt;p&gt;My thesis is that subjective (Bayesian) probabilities quantify non-determinism, rather than randomness (where non-determinism means that something takes on more than one value, i.e. is a set rather than a single value). Below I motivate the idea that quantity of information based on non-determinism can be interpreted as measuring the reduction in size (&amp;ldquo;narrowing down&amp;rdquo;) of a possibility space.&lt;/p&gt;
&lt;h1 id=&#34;information-and-finite-possibilities&#34;&gt;Information and finite possibilities&lt;/h1&gt;
&lt;p&gt;Following &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/&#34;&gt;Deconstructing Bayesian Inference&lt;/a&gt;, let&amp;rsquo;s suppose an agent is predicting the continuation of a data string, and the agent&amp;rsquo;s prediction is not uniquely determined, i.e. non-deterministic. I will represent the agent&amp;rsquo;s prediction as a set of possible predictions, called the agent&amp;rsquo;s &lt;strong&gt;hypothesis set&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Formally, the agent receives an endless stream of data $\o$ drawn from the set $\X^\infty$, where $\X$ is some finite character set. In the examples below, let&amp;rsquo;s assume $\X = \set{0,1}$. Given some finite sequence $x\in\X^*$, a prediction is a continuation (not necessarily the correct continuation), i.e. an infinite sequence starting with prefix $x$.&lt;/p&gt;
&lt;p&gt;Let $\H \subseteq \X^\infty$ be the agent&amp;rsquo;s hypothesis set. When finite data $x$ is observed, we narrow down $\H$ to the subset of all sequences starting with $x$. This is called &lt;strong&gt;conditionalizing&lt;/strong&gt; (or &lt;strong&gt;restriction&lt;/strong&gt;). Denote $\dom{\H}{x} = \set{\o\in\H \mid x\sqsubset\o}$ as the subset of $\H$ consisting of sequences starting with the prefix $x$. The set $\dom{\H}{x}$ is $\H$ conditioned on $x$.&lt;/p&gt;
&lt;p&gt;For example, a rigid agent that only ever predicts $0$s no matter what has the following hypothesis set:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\H = \set{0000000000\dots}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Alternatively, consider:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\H = \set{0000000000\dots, 1111111111\dots}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Before observing anything, the agent&amp;rsquo;s prediction for the first timestep is not determined - it could be 0 or it could be 1. When the first bit is observed, be it a 0 or a 1, the agent&amp;rsquo;s predictions after that become fully determined: $\dom{\H}{0} = \set{0000000000\dots}$ and $\dom{\H}{1} = \set{1111111111\dots}$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider a more complex hypothesis set:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\H = \{&amp;amp;0000000000\dots,&lt;br&gt;
\\&amp;amp;0100000000\dots,&lt;br&gt;
\\&amp;amp;1000000000\dots,&lt;br&gt;
\\&amp;amp;1001111111\dots,&lt;br&gt;
\\&amp;amp;1010101010\dots,&lt;br&gt;
\\&amp;amp;1101100110\dots,&lt;br&gt;
\\&amp;amp;1110111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Here are the conditionalized sets on the shortest prefixes:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{0} = \{&amp;amp;0000000000\dots,&lt;br&gt;
\\&amp;amp;0100000000\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{1}  = \{&amp;amp;1000000000\dots,&lt;br&gt;
\\&amp;amp;1001111111\dots,&lt;br&gt;
\\&amp;amp;1010101010\dots,&lt;br&gt;
\\&amp;amp;1101100110\dots,&lt;br&gt;
\\&amp;amp;1110111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{00} &amp;amp;= \set{0000000000\dots}\\&lt;br&gt;
\dom{\H}{01} &amp;amp;= \set{0100000000\dots}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{10}  = \{&amp;amp;1000000000\dots,&lt;br&gt;
\\&amp;amp;1001111111\dots,&lt;br&gt;
\\&amp;amp;1010101010\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{11}  = \{&amp;amp;1101100110\dots,&lt;br&gt;
\\&amp;amp;1110111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{100}  = \{&amp;amp;1000000000\dots,&lt;br&gt;
\\&amp;amp;1001111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\dom{\H}{101}  = \{1010101010\dots\}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\dom{\H}{110}  = \{1101100110\dots\}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{111}  = \{&amp;amp;1110111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;There are more sequences in $\H$ starting with $1$ than with $0$, and so $\dom{H}{0}$ is a relatively smaller portion of $\H$ than $\dom{H}{1}$ is - specifically $\dom{H}{0}$ is 1/4 of $\H$ and $\dom{H}{1}$ is 3/4 of $\H$.&lt;/p&gt;
&lt;p&gt;If the agent&amp;rsquo;s goal is to make a prediction, it has incentive to narrow down as much of $\H$ as possible to reduce prediction uncertainty, prior to making the prediction. Thus, for this particular $\H$, the agent prefers to observe $0$ than $1$ as the first bit. In other words, smaller subsets $\H$ are better because they also mean $\H$ was narrowed down by a greater amount.&lt;/p&gt;
&lt;p&gt;It can be easier to think in terms of maximizing the amount of &amp;ldquo;narrowing-down&amp;rdquo;. We can formally quantify it by counting the number of &amp;ldquo;halvings&amp;rdquo; of $\H$ a given subset is worth.&lt;/p&gt;
&lt;p&gt;For subset $A \subseteq \H$ (still assuming finite $\H$), define the &lt;strong&gt;information gain&lt;/strong&gt; of $A$ (or &lt;strong&gt;surprise&lt;/strong&gt;) as:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
h_{\H}(A) \df -\lg\par{\frac{\abs{A}}{\abs{\H}}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\lg$ is log base 2. In general $h(A)$ is a real number, so if $n \leq h(A) &amp;lt; n+1$, then $\abs{A}$ is smaller than $1/2^{-n}$ the size of $\H$, and no smaller than  $1/2^{-n-1}$ of $\H$. In the context of narrowing down $\H$ with data $x$, the quantity $h(\dom{\H}{x})$ tells us how many halvings of $\H$ the data $x$ gave us. When the goal is to be as certain as possible about predictions of the future (where prediction uncertainty is represented by the set $\dom{\H}{x}$), the larger $h(\dom{\H}{x})$ is the better.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s calculate possible information gains for the example above:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h_\H(\dom{\H}{0}) &amp;amp;= -\lg\par{\frac{2}{8}} = 2 \\&lt;br&gt;
h_\H(\dom{\H}{1}) &amp;amp;= -\lg\par{\frac{6}{8}} \approx 0.415 \\&lt;br&gt;
h_\H(\dom{\H}{00}) &amp;amp;= -\lg\par{\frac{1}{8}} = 3 \\&lt;br&gt;
h_\H(\dom{\H}{10}) &amp;amp;= -\lg\par{\frac{3}{8}} \approx 1.415 \\&lt;br&gt;
&amp;amp;\vdots&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Suppose we previously observed $1$, for an information gain of about $0.415$, and reduced the remaining hypothesis set to $\dom{\H}{1}$. Then if we observe $0$, the hypothesis set is further reduced to $\dom{\H}{10}$. Going from $\H$ to $\dom{\H}{10}$ is worth a total information gain of about $1.415$, but what about the relative information gain going from $\dom{\H}{1}$ to $\dom{\H}{10}$? This quantity is called &lt;strong&gt;conditional information gain&lt;/strong&gt;, defined as&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h_\H(A \mid B) &amp;amp;\df h_\H(A\cap B) - h_\H(B) \\&lt;br&gt;
&amp;amp;= -\lg\par{\frac{\abs{A \cap B}}{\abs{B}}} \\&lt;br&gt;
&amp;amp;= h_B(A\cap B)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;So $h_\H(\dom{\H}{10} \mid \dom{\H}{1}) = h_\H(\dom{\H}{10}) - h_\H(\dom{\H}{1}) \approx 1.415 - 0.415 = 1$. Conditional information gain is just the information gain starting from a different hypothesis set, so $h_\H(\dom{\H}{10} \mid \dom{\H}{1}) = h_{\dom{\H}{1}}(\dom{\H}{10}) = -\lg(3/6) = 1$, which is the number of halvings it takes to get from $\dom{\H}{1}$ to $\dom{\H}{10}$.&lt;/p&gt;
&lt;p&gt;The agent would like $h(\dom{\H}{x})$ to be maximized given a pre-defined $\H$, but the agent may not have any control over this quantity, unless the agent can take actions that affect what data $x$ it observes. However, if the agent gets to choose $\H$, then would the agent choose a very small set to begin with so that it does not need to be narrowed down very much?&lt;/p&gt;
&lt;p&gt;There is a problem. Take again as an example the rigid agent: $\H = \set{0000000000\dots}$. If only $0$s are ever observed, then this is a great hypothesis set, because the agent will be maximally certain about its prediction of future $0$s, and the agent will be right. But suppose the agent is wrong, e.g. the agent observes data $x = 001$. Then $\dom{\H}{001} = \set{}$ is the empty set. The agent can no longer make any prediction! If we quantify this narrowing-down, we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
h_\H(\dom{\H}{001}) = -\lg\par{\frac{0}{1}} = \infty\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ve maximized the amount of narrowing-down - it&amp;rsquo;s infinite. But at the same time this defeats the actual goal of being maximally certain about predictions. Having an empty hypothesis set is a degenerate state. Clearly, too much information gain is bad. Is there an ideal trade-off?&lt;/p&gt;
&lt;h2 id=&#34;compound-hypotheses&#34;&gt;Compound hypotheses&lt;/h2&gt;
&lt;p&gt;Consider the following hypothesis set:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\H = \{&amp;amp;0000000000\dots,&lt;br&gt;
\\&amp;amp;0100000000\dots,&lt;br&gt;
\\&amp;amp;0010000000\dots,&lt;br&gt;
\\&amp;amp;0110000000\dots,&lt;br&gt;
\\&amp;amp;1001111111\dots,&lt;br&gt;
\\&amp;amp;1101111111\dots,&lt;br&gt;
\\&amp;amp;1011111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The first symbol in these sequences fully determines the long-run behavior of the sequences, i.e. sequences starting with 0 end with 0s, and sequences starting with 1 end with 1s. However, the 2nd and 3rd symbols are not determined by the 1st. Perhaps it would make sense to not care about predicting them. In that case, we are not so interested in narrowing down $\H$ to one sequence, as we are in narrowing down $\H$ into a particular long-run pattern.&lt;/p&gt;
&lt;p&gt;$\newcommand{\h}{\mc{H}}$We can formally represent what we care about and don&amp;rsquo;t care about predicting, by partitioning $\H$. In this example, suppose we make the following partition:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mf{H} = \set{\h_1, \h_2} = \{&lt;br&gt;
\{&amp;amp;0000000000\dots,&lt;br&gt;
\\&amp;amp;0100000000\dots,&lt;br&gt;
\\&amp;amp;0010000000\dots,&lt;br&gt;
\\&amp;amp;0110000000\dots\},&lt;br&gt;
\\\{&amp;amp;1001111111\dots,&lt;br&gt;
\\&amp;amp;1101111111\dots,&lt;br&gt;
\\&amp;amp;1011111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\h_1$ contains only sequences ending in 0s, and $\h_2$ contains only sequences ending in 1s.&lt;/p&gt;
&lt;p&gt;In general, for a partition $\mf{H}$ of $\H$, call each $\h\in\mf{H}$ a &lt;strong&gt;compound hypothesis&lt;/strong&gt;, indicating that its a set of &lt;strong&gt;primitive hypotheses&lt;/strong&gt; (data continuations). As we shall see, compound hypotheses correspond closely to the hypotheses-as-data-distributions formulation which we saw in &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/&#34;&gt;Deconstructing Bayesian Inference&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For some partition $\mf{H}$ of $\H$, let $\dom{\mf{H}}{x} = \set{\dom{\h}{x} \mid \h \in \mf{H}}$ be the partition of $\dom{\H}{x}$ consisting of the parts in $\mf{H}$ which have each been conditionalized on $x$ (narrowed down to the sequences starting with $x$).&lt;/p&gt;
&lt;p&gt;In our example, $\dom{\mf{H}}{0} = \set{\h_1, \set{}}$ and $\dom{\mf{H}}{1} = \set{\set{}, \h_2}$. Let&amp;rsquo;s consider empty compound hypotheses to be eliminated. Then in either scenario, observing just the 1st symbol narrows down $\mf{H}$ to exactly one compound hypothesis, analogous to our original goal of narrowing down $\H$ to one hypothesis. The remaining compound hypothesis (either $\h_1$ or $\h_2$) is uncertain about what the 2nd and 3rd symbols will be, but certain about all symbols after that.&lt;/p&gt;
&lt;h2 id=&#34;defining-information&#34;&gt;Defining Information&lt;/h2&gt;
&lt;p&gt;Let $\H \subseteq \X^\infty$ be a hypothesis set and $\o\in\X^\infty$ be the &lt;em&gt;true&lt;/em&gt; data sequence, i.e. the data sequence that will be observed. Let $\mc{O}\subseteq\H$ be some subset of $\H$ containing $\o$. I define &lt;strong&gt;information&lt;/strong&gt; as a tuple of the form $(\H, \mc{O})$, which specifies a set of possibilities and a narrowed down subset of remaining possibilities. The information $(\H, \mc{O})$ represents the &lt;em&gt;knowledge&lt;/em&gt; that $\o\in\mc{O}$ and $\o\notin\H\setminus\mc{O}$. This definition allows us to separate the issue of quantifying information with specifying information. Quantity of information may depend on an arbitrary choice of measure, whereas the information itself is what we often care about.&lt;/p&gt;
&lt;p&gt;Below I will use the notation&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\H \tr \mc{O} \df (\H, \mc{O})\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which can be read as &amp;ldquo;$\H$ is narrowed down to $\mc{O}$.&amp;rdquo; This arrow-notation represents information gain, which is equivalent to having the knowledge that $\o\in\mc{O}$ and $\o\notin\H\setminus\mc{O}$. The information gain $\H \tr \mc{O}$ is quantified by $h_\H(\mc{O})$.&lt;/p&gt;
&lt;p&gt;Some special cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$(\O, \set{\o})$ is called &lt;strong&gt;total information&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;$(\O, \mc{O})$ where $\o\in\mc{O}$ and $\mc{O}\setminus\set{\o}$ is non-empty is called &lt;strong&gt;partial information&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;$(\O, \emptyset)$ is called &lt;strong&gt;contradictory information&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$(\O, \O)$ is called &lt;strong&gt;trivial information&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;information-gain&#34;&gt;Information Gain&lt;/h2&gt;
&lt;p&gt;Earlier, agent&amp;rsquo;s goal was to gain maximum prediction certainty by narrowing down $\H$ as much as possible. Now with partition $\mf{H}$, the agent&amp;rsquo;s goal is to narrow down $\mf{H}$ as much as possible, ideally reducing all of the parts but one to empty sets. However, if no compound hypothesis $\dom{\h}{x}\in\dom{\mf{H}}{x}$ is empty, is there still a sense in which the agent narrowed its compound hypotheses down? This question can be answered by considering information gain quantities.&lt;/p&gt;
&lt;p&gt;$\H \tr \dom{\H}{x}$ is the total information gained, and is quantified by $h_\H(\dom{\H}{x})$. This is the number of halvings the hypothesis set $\H$ is reduced by due to observing $x$. This quantity does not depend on the choice of partition $\mf{H}$.&lt;/p&gt;
&lt;p&gt;$\h \tr \dom{\h}{x}$ is the information gained within compound hypothesis $\h \in \mf{H}$, and is quantified by $h_\H(\dom{\h}{x} \mid \h) = h_\h(\dom{\h}{x})$. This is the information gained where $\h$ is treated as its own hypothesis set. This quantity only depends on the given $\h$, and not the other parts in $\mf{H}$. For each $\h\in\mf{H}$ there is an information gain $\h \tr \dom{\h}{x}$. Since we are considering $\h$ to be a set of sequences that the agent doesn&amp;rsquo;t care about distinguishing, reductions in $\h$ are essentially wasted information gain. If we regard variation of sequences within $\h$ to be noise, then this quantity measures information gained about that noise.&lt;/p&gt;
&lt;p&gt;Let $\o\in\H$ be the full data sequence, of which only a finite prefix $x \sqsubset \o$ has been observed. Call $\o$ the &lt;strong&gt;true hypothesis&lt;/strong&gt;. Likewise, for partition $\mf{H}$ there is exactly one compound hypothesis $\h\in\mf{H}$ containing $\o$. Call this the &lt;strong&gt;true compound hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Does observing $x$ tell the agent anything about which compound hypothesis $\h\in\mf{H}$ is true (contains $\o$)? Consider first the amount of information gain needed to reach certainty: $\H\tr\h$, i.e. reduce all but one compound hypotheses to the empty set (again, we don&amp;rsquo;t care about the size of the remaining compound hypothesis). This is quantified by $h_\H(\h)$. When $x$ is observed, $\H$ becomes $\dom{\H}{x}$ and $\h$ becomes $\dom{\h}{x}$. At that point, the information gain needed to achieve certainty that the same compound hypothesis true is $\dom{\H}{x}\tr\dom{\h}{x}$, quantified by $h_{\dom{\H}{x}}(\dom{\h}{x})$.&lt;/p&gt;
&lt;p&gt;We haven&amp;rsquo;t achieved compound hypothesis certainty, but the quantity of information gain needed to do so has changed:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\D_x\up{\h} = h_\H(\h) - h_{\dom{\H}{x}}(\dom{\h}{x})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s confirm that the sign is correct. If $h_{\dom{\H}{x}}(\dom{\h}{x}) &amp;gt; h_\H(\h)$ then we need more bits to achieve $\dom{\H}{x}\tr\dom{\h}{x}$ than to achieve $\H\tr\h$, i.e. the task of narrowing down to this compound hypothesis has gotten harder. In that case, $\D_x\up{\h}$ is negative. If, on the other hand, $h_\H(\h) &amp;gt; h_{\dom{\H}{x}}(\dom{\h}{x})$ then this compound hypothesis has become a larger portion of the remaining $\dom{\H}{x}$ than it was before observing $x$, so the task of narrowing down to this compound hypothesis has gotten easier. In that case, $\D_x\up{\h}$ is positive.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210409163721.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
Note that $\D_x\up{\h}$ is upper bounded. If the agent has succeeded in ruling out all other compound hypotheses so that $\dom{\h}{x} = \dom{\H}{x}$, then $h_{\dom{\H}{x}}(\dom{\h}{x}) = 0$ and $\D_x\up{\h} = h_\H(\h)$, which is the maximum amount of information that can be gained about whether $\h$ is true.&lt;/p&gt;
&lt;p&gt;If $\h$ is known to be false, i.e. $\dom{\h}{x} = \set{}$, then $h_{\dom{\H}{x}}(\dom{\h}{x}) = \infty$ and so $\D_x\up{\h} = -\infty$. Thus $\D_x\up{\h}$ is not lower bounded, i.e. there is a maximum amount of information to be gained about whether a compound hypothesis is true, but an infinite amount of information to lose. For example, if $\h$ is true but $x$ is very misleading, then $\D_x\up{\h}$ will be very negative. In the long run, if the agent observes enough data, $\o_{1:n}$ for large $n$, then $\D_{\o_{1:n}}\up{\h}$ will go up and eventually converge to $h_\H(\h)$. The misleading initial data caused the agent to lose information, in the sense that even more information needs to be gained to achieve the same certainty that $\h$ is true.&lt;/p&gt;
&lt;p&gt;Also note that $\D_x\up{\h}$ is a total change given the entire data sequence $x$. We can also quantify the change due to a single timestep $x_n$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\D_{x_n}\up{\dom{\h}{x_{&amp;lt;n}}} = h_{\dom{\H}{x_{&amp;lt;n}}}(\dom{\h}{x_{&amp;lt;n}}) - h_{\dom{\H}{x_{1:n}}}(\dom{\h}{x_{1:n}})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then the total change is the sum of changes for each timestep:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\D_x\up{\h} = \sum_{i=1}^\abs{x} \D_{x_i}\up{\dom{\h}{x_{&amp;lt;i}}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;It may even be of some interest to plot incremental information gains over time for each compound hypothesis $\h$. What we will observe is that in the long run is that $\D_x\up{\h}$ asymptotically converges to $h_\H(\h)$ for the true compound hypothesis $\h$, and the incremental change $\D_{x_n}\up{\dom{\h}{x_{&amp;lt;n}}}$ gets smaller and smaller over time for that same $\h$. Meanwhile, for all other compound hypotheses $\h&#39;$ (the false ones), $\D_x\up{\h&#39;}$ diverges to $-\infty$ and the incremental change $\D_{x_n}\up{\dom{\h&#39;}{x_{&amp;lt;n}}}$ goes negative and grows larger and larger in magnitude. For &amp;ldquo;misleading&amp;rdquo; data, the short-run behavior of these plots may be oscillatory before long-run behavior takes over.&lt;/p&gt;
&lt;h3 id=&#34;useful-identities&#34;&gt;Useful Identities&lt;/h3&gt;
&lt;p&gt;Doing some algebraic manipulation, we get:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\D_x\up{\h} &amp;amp;= h_\H(\h) - h_{\dom{\H}{x}}(\dom{\h}{x}) \\&lt;br&gt;
&amp;amp;= -\lg\par{\frac{\abs{\h}}{\abs{\H}}} + \lg\par{\frac{\abs{\dom{\h}{x}}}{\abs{\dom{\H}{x}}}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\abs{\dom{\h}{x}}/\abs{\dom{\H}{x}}}{\abs{\h}/\abs{\H}}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{f_x\up{\h}}{f\up{\h}}}\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $f\up{\h} = \abs{\h}/\abs{\H}$ is the fraction of predictions that $\h$ takes up, and $f_x\up{\h} = \abs{\dom{\h}{x}}/\abs{\dom{\H}{x}}$ is that fraction after $x$ is observed. This gives us an interpretation for the quantity $\D_x\up{\h}$: the number of doublings it takes to go from $f\up{\h}$ to $f_x\up{\h}$. E.g. if $\h$ is a quarter the size of $\H$ and $\dom{\h}{x}$ is half the size of $\dom{\H}{x}$, then $\D_x\up{\h} = \lg\frac{1/2}{1/4} = \lg 2 = 1$. That means the agent has one less bit to gain about whether $\dom{\h}{x}$ is true, and has in that sense gained one bit of information.&lt;/p&gt;
&lt;p&gt;Doing even more algebra, we get another useful identity:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\D_x\up{\h} &amp;amp;= \lg\par{\frac{\abs{\dom{\h}{x}}/\abs{\dom{\H}{x}}}{\abs{\h}/\abs{\H}}} \\&lt;br&gt;
&amp;amp;= -\lg\par{\frac{\abs{\dom{\H}{x}}}{\abs{\H}}} + \lg\par{\frac{\abs{\dom{\h}{x}}}{\abs{\h}}} \\&lt;br&gt;
&amp;amp;= h_\H(\dom{\H}{x}) - h_\h(\dom{\h}{x})&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Thus, $h_\H(\dom{\H}{x}) = h_\h(\dom{\h}{x}) + \D_x\up{\h}$, for all $\h\in\mf{H}$. That is to say, the total information gain $h_\H(\dom{\H}{x})$ can be decomposed as the sum of information gained within a given compound hypothesis $\h$ (information gained about noise, i.e. what we don&amp;rsquo;t care about predicting), plus the information gained about whether $\h$ is true. Total info gain $h_\H(\dom{\H}{x})$ decomposes similarly for every $\h\in\mf{H}$.&lt;/p&gt;
&lt;h2 id=&#34;other-information-quantities&#34;&gt;Other Information Quantities&lt;/h2&gt;
&lt;p&gt;See my &lt;a href=&#34;http://zhat.io/articles/primer-shannon-information&#34;target=&#34;_blank&#34;&gt;primer to Shannon&amp;rsquo;s information theory&lt;/a&gt; for more intuition about the interpretation of these quantities, and specifically the sections on&lt;br&gt;
&lt;a href=&#34;http://zhat.io/articles/primer-shannon-information#mutual-information&#34;target=&#34;_blank&#34;&gt;mutual information&lt;/a&gt; and &lt;a href=&#34;http://zhat.io/articles/primer-shannon-information#entropy&#34;target=&#34;_blank&#34;&gt;entropy&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For sets $A,B\subseteq\H$, the quantity $i_\H(A, B)$ is called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Pointwise_mutual_information&#34;target=&#34;_blank&#34;&gt;&lt;strong&gt;pointwise mutual information&lt;/strong&gt;&lt;/a&gt; (PMI) between $A$ and $B$, defined by&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
i_\H(A,B) &amp;amp;\df h_\H(A) - h_\O(A \mid B) \\&lt;br&gt;
&amp;amp;= -\lg\par{\frac{\abs{A}}{\abs{\H}}} + \lg\par{\frac{\abs{A\cap B}}{\abs{B}}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\abs{A\cap B}}{\abs{A}\abs{B}}\abs{\H}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\abs{A\cap B}}{\abs{A}\abs{B}}} - \lg\par{\frac{1}{\abs{\H}}}\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Note that PMI is symmetric, so $i_\H(A, B) = i_\H(B, A)$.&lt;/p&gt;
&lt;p&gt;Notice that $\D_x\up{\h} = i_\H(\h, \dom{\H}{x})$. Using our intuition about narrowing down compound hypotheses from before, we can interpret the meaning of $i_\H(A, B)$ in general.&lt;/p&gt;
&lt;p&gt;Let&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\frac{\H \tr \H&#39;}{U \tr U&#39;}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;denote the statement &amp;ldquo;$\H$ is narrowed down to $\H&#39;$ while $U$ is narrowed down to $U&#39;$&amp;rdquo;, with the assumption that $U \subseteq \H$ and $U&#39;\subseteq\H&#39;$. So the idea of gaining information about whether compound hypothesis $\h$ is true can be written succinctly as $\frac{\H \tr \dom{\H}{x}}{\h \tr \dom{\h}{x}}$.&lt;/p&gt;
&lt;p&gt;In general, $i_\H(A, B)$ quantifies $\frac{\H \tr B}{A \tr (A \cap B)}$, and is the number of doublings achieved from fraction $f = \abs{A}/\abs{\H}$ to fraction $f&#39;=\abs{A\cap B}/\abs{B}$, i.e. $\lg(f&#39;/f)$.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210409163741.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210409163757.png&#34; alt=&#34;&#34;&gt;
&lt;/p&gt;
&lt;h3 id=&#34;entropy&#34;&gt;Entropy&lt;/h3&gt;
&lt;p&gt;It is useful to have a single quantity representing the state of $\dom{\mf{H}}{x}$.&lt;/p&gt;
&lt;p&gt;Let $\mf{A}$ be some partition of $\H$. Define &lt;strong&gt;entropy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mb{H}(\mf{A}) \df \sum_{A\in\mf{A}} \frac{\abs{A}}{\abs{\H}} h_\H(A)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This quantity doesn&amp;rsquo;t require $\O$ to be explicitly specified because it determined by the argument, i.e. $\O = \bigcup \mf{A}$.&lt;/p&gt;
&lt;p&gt;Let $\mf{H}$ be a compound hypothesis set. Then $\mb{H}(\mf{H})$ quantifies roughly how much of a difference there is in information that can be gained about whether each compound hypothesis $\h\in\mf{H}$ is true. Specifically, it is the expected information gain across $\mf{H}$, though expectations don&amp;rsquo;t have the same meaning here because these &amp;ldquo;probabilities&amp;rdquo; don&amp;rsquo;t denote randomness. $\mb{H}(\mf{H})$ is maximized if $h_\H(\h) = h_\H(\h&#39;)$ for all $\h,\h&#39;\in\mf{H}$, and $\mb{H}(\mf{H})$ is 0 if one $\h\in\mf{H}$ is non-empty while all other compound hypotheses are empty. If we observe $x$, then high $\mb{H}(\dom{\mf{H}}{x})$ indicates high uncertainty about which compound hypothesis is true, and small $\mb{H}(\dom{\mf{H}}{x})$ indicates high certainty about which compound hypothesis is true.&lt;/p&gt;
&lt;h3 id=&#34;mutual-information&#34;&gt;Mutual Information&lt;/h3&gt;
&lt;p&gt;Let $\mf{A}$ and $\mf{B}$ be partitions of $\H$. Define &lt;strong&gt;mutual information&lt;/strong&gt; (MI)&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mb{I}(\mf{A}, \mf{B}) \df \sum_{A\in\mf{A}}\sum_{B\in\mf{B}} \frac{\abs{A\cap B}}{\abs{\H}} i_\H(A, B)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is the expected pointwise mutual information between the two partitions, which quantifies roughly how much redundancy there is between them.&lt;/p&gt;
&lt;p&gt;MI plays the following role in Bayesian information gain: Let $\mf{X}_n = \set{\dom{\H}{x} \mid x\in\X^n}$ be the partition of $\H$ consisting of all possible conditionalizations $\dom{\H}{x}$ for each possible length-$n$ data sequence $x\in\X^n$. Then $\mb{I}(\mf{H}, \mf{X}_n)$ quantifies how much information could be gained (in expectation) about which compound hypothesis is true given a (not yet observed) length-$n$ data sequence. If MI is minimized, $\mb{I}(\mf{H}, \mf{X}_n)=0$, then we expect that $\D_x\up{\h} = i_\H(\h, \dom{\H}{x})=0$ for $\h\in\mf{H}$ and $\dom{\H}{x}\in\mf{X}_n$. This would indicate that the partitions $\mf{H}$ and $\mf{X}_n$ are orthogonal, in a sense. On the other hand, MI is maximized when $\mb{I}(\mf{H}, \mf{X}_n)=\min\set{\mb{H}(\mf{H}), \mb{H}(\mf{X}_n)}$, and indicates that each $x\in\X^n$ will narrow down some $\h\in\mf{H}$ to empty sets (with either one remaining compound hypothesis which is narrowed down, or multiple remaining compound hypotheses which are not narrowed down at all), i.e. the partitions are parallel, in a sense.&lt;/p&gt;
&lt;h1 id=&#34;infinite-possibilities&#34;&gt;Infinite possibilities&lt;/h1&gt;
&lt;p&gt;In practice agents would want to have a large enough hypothesis set to be able to make predictions in all circumstances. That is to say, given any finite observation $x\in\X^*$, it is desirable for at least one sequence $\o\in\H$ to begin with $x$, denoted $x \sqsubset \o$. Then, $\dom{\H}{x}$ is non-empty for all $x$ and so the agent always has at least one prediction to make.&lt;/p&gt;
&lt;p&gt;Clearly such an $\H$ cannot be finite because $\X^*$ is infinite, i.e. there is at least one $\o\in\H$ for every $x\in\X^*$ (Simple proof: Suppose $\H$ were finite. Construct finite $x$ s.t. $x\not\sqsubset\o$ for all $\o\in\H$).  (Note that such an $\H$ need not be equal to $\X^\infty$. For example, $\H = \set{x`00000\dots \mid x\in\X^*}$ where $x`00000\dots$ is $x$ appended with infinite $0$s. This $\H$ does not include sequences with other limiting behavior, e.g. the binary digits of Pi.)&lt;/p&gt;
&lt;p&gt;However, $\H$ can be too big. Suppose $\H=\X^\infty$. Then $\dom{\H}{x} = \X^\infty$ for all $x\in\X^*$. This hypothesis set can never be narrowed down to anything, i.e. there is never information gain. An agent with this hypothesis set remains maximally uncertain always, and so it is not useful.&lt;/p&gt;
&lt;p&gt;Even if $\H$ is a strict subset of $\X^\infty$ but contains every finite data sequence (so that $\dom{\H}{x}$ is non-empty for all $x$), it&amp;rsquo;s usefulness is still dubious. In general $\abs{\dom{\H}{x}}=\infty$ for all $x$ (the cardinality of $\dom{\H}{x}$ is always infinite; disregarding different sizes of infinity), so we cannot compare to what extent we&amp;rsquo;ve narrowed down $\H$ further by observing $x$ rather than $y$. That is to say, $\abs{\dom{\H}{x}}/\abs{\H} = \infty/\infty$ is indeterminate. Furthermore, $\dom{\H}{x}$ will contain every finite data sequence starting with $x$, so there is no tangible sense in which the agent&amp;rsquo;s predictions at finite time are narrowed down.&lt;/p&gt;
&lt;p&gt;The problem of measuring narrowing-down of infinite possibility sets is resolved by choosing a measure $\mu$ on $\H$. However a new problem arises: how to choose $\mu$. My purpose in presenting Bayesian information theory as narrowing down possibility spaces is to give meaning to probability values. Now, we&amp;rsquo;ve reintroduced an arbitrary measure $\mu$.&lt;/p&gt;
&lt;p&gt;There is a sort of middle ground that also serves as a bridge between the usual probabilistic conception of Bayesian inference I introduced in &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/&#34;&gt;Deconstructing Bayesian Inference&lt;/a&gt; and the hypothesis set conception, via algorithmic information theory.&lt;/p&gt;
&lt;h2 id=&#34;algorithmic-randomness&#34;&gt;Algorithmic Randomness&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/&#34;&gt;Deconstructing Bayesian Inference&lt;/a&gt;, I defined Bayesian inference as a manipulation of probability measures on $\X^\infty$. Given a set $\M$ of measures $\mu$ on $\X^\infty$, and a prior $p$ on $\M$, what is the corresponding &amp;ldquo;possibility space narrowing-down&amp;rdquo; perspective?&lt;/p&gt;
&lt;p&gt;I supposed that hypothesis probabilities $\mu(\o_{1:n})$ for $\mu\in\M$ represented irreducible uncertainty about the world, whereas subjective probabilities $p(\o_{1:n})$ represented a mixture of reducible and irreducible uncertainty, where knowing which hypothesis $\mu$ is true maximally reduces your uncertainty.&lt;/p&gt;
&lt;p&gt;I also introduced a distinction between randomness and non-determinism. Randomness can be defined as incompressibility, and non-determinism refers to the output of a mathematical construct (such as a function) being not uniquely determined by givens (such as an input). Probabilities can measure both, which can lead to confusion.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s suppose here that hypothesis probabilities always measure randomness. Assuming $\mu\in\M$ is computable, we can precisely define what it means for an infinite sequence $\o\in\X^\infty$ to &amp;ldquo;look like&amp;rdquo; a typical sequence randomly drawn from $\mu$. Call such &amp;ldquo;typical looking&amp;rdquo; sequences &lt;strong&gt;$\mu$-typical&lt;/strong&gt; (or &lt;strong&gt;$\mu$-random&lt;/strong&gt;). Formally, $\o\in\X^\infty$ is $\mu$-typical iff the optimal compression rate of $\o$ (via monotone algorithmic complexity $Km$) is achieved with arithmetic coding using $\mu$. (See &lt;a href=&#34;https://www.springer.com/gp/book/9781489984456&#34;target=&#34;_blank&#34;&gt;Li &amp;amp; VitÃ¡nyi&lt;/a&gt;, 3rd edition, theorem 4.5.3 on page 318, which states, $\sup_n\left\{\lg(1/\mu(\o_{1:n})) - Km(\o_{1:n})\right\} &amp;lt; \infty$.)&lt;/p&gt;
&lt;p&gt;Let $\h\up{\mu}\subseteq\X^\infty$ be the set of all infinite sequences which are $\mu$-typical, called the &lt;strong&gt;$\mu$-typical set&lt;/strong&gt;. The $\mu$-probability of drawing a $\mu$-typical sequence is 1, i.e. $\mu(\h\up{\mu})=1$, and the $\mu$-probability of drawing a $\mu$-atypical sequence is 0. You can think of $\h\up{\mu}$ as $\X^\infty$ with a $\mu$-measure 0 subset subtracted from it (specifically $\h\up{\mu}$ is the smallest constructable $\mu$-measure 1 subset of $\X^\infty$).&lt;/p&gt;
&lt;p&gt;The prior $p$ on $\M$ induces a subjective data distribution: $p(\o_{1:n}) = \sum_{\mu\in\M} p(\mu)\mu(\o_{1:n})$. So long as $p$ is computable, there is a $p$-typical set $\h\up{p}$.&lt;/p&gt;
&lt;p&gt;Let $\H = \h\up{p}$ and $\mf{H} = \set{\h\up{\mu} \mid \mu\in\M}$, where $\H$ is an agent&amp;rsquo;s hypothesis set (set of sequence predictions) and $\mf{H}$ is a set of compound hypotheses, where $\bigcup \mf{H} = \H$. I&amp;rsquo;m dropping the requirement that $\mf{H}$ be a proper partition of $\O$ (i.e. all $\h\in\mf{H}$ are mutually disjoint), in which case we call $\mf{H}$ a cover of $\O$.&lt;/p&gt;
&lt;p&gt;Now, Bayesian inference with hypothesis set (of measures) $\M$ and prior $p$, and Shannon quantities using these measures, corresponds to Bayesian inference with $\H$ and $\mf{H}$ as I outlined above. The restriction $\dom{\H}{x}$ is the typical set for the conditional measure $p(\cdot \mid x)$, and likewise for $\h\up{\mu}\in\mf{H}$, the restriction $\dom{\h\up{\mu}}{x}$ is the typical set for the conditional measure $\mu(\cdot \mid x)$.&lt;/p&gt;
&lt;p&gt;The prior $p(\mu)$ is simply the relative size of $\h\up{\mu}$ within $\H$, given by $p(\h\up{\mu})$. The prior encodes how much information we would gain if all other hypotheses were ruled out: $\H\tr\h\up{\mu}$, quantified by $h_p(\h\up{\mu}) = -\lg p(\h\up{\mu})$.&lt;/p&gt;
&lt;p&gt;The posterior $p(\mu\mid x)$ is simply the relative size of $\dom{\h\up{\mu}}{x}$ within $\dom{\H}{x}$, given by $p(\dom{\h\up{\mu}}{x}) = p(\h\up{\mu} \cap \dom{\H}{x})$. The posterior encodes how much information we would gain if all other hypotheses were ruled out (after observing $x$): $\dom{\H}{x}\tr\dom{\h\up{\mu}}{x}$, quantified by $h_p(\h\up{\mu} \mid \dom{\H}{x}) = -\lg p(\dom{\h\up{\mu}}{x})/p(\dom{\H}{x})$.&lt;/p&gt;
&lt;p&gt;Furthermore, $\H\tr\dom{\H}{x}$ is quantified by $h_p(\dom{\H}{x}) = -\lg p(\dom{\H}{x}) = -\lg p(x)$, and $\h\up{\mu}\tr\dom{\h\up{\mu}}{x}$ is quantified by $h_\mu(\dom{\h\up{\mu}}{x}) = -\lg \mu(\dom{\h\up{\mu}}{x}) = -\lg \mu(x)$.&lt;/p&gt;
&lt;p&gt;The mysterious &amp;ldquo;information gained about whether hypothesis $\mu$ is true&amp;rdquo; becomes $\frac{\H \tr \dom{\H}{x}}{\h\up{\mu} \tr \dom{\h\up{\mu}}{x}}$, quantified by $i_p(\h\up{\mu}, \dom{\H}{x}) = \lg\frac{p(\dom{\h\up{\mu}}{x})}{p(\h\up{\mu})p(\dom{\H}{x})} = \lg\frac{p(\mu,x)}{p(\mu)p(x)}$ which is the pointwise mutual information between hypothesis $\mu$ and data $x$.&lt;/p&gt;
&lt;p&gt;Finally, the information gained about which hypothesis is true is summarized by the quantity&lt;/p&gt;
&lt;p&gt;$$\mb{I}_p(\mf{H}, \mf{X}_\abs{x}) = \sum_{\h\up{\mu}\in\mf{H}}\sum_{\dom{\H}{x}} \mu(\dom{\h}{x}) i_p(\h\up{\mu}, \dom{\H}{x}) = \mb{E}_p\left[i(H,X_{1:\abs{x}})\right] = I(H, X_{1:\abs{x}})\,,$$&lt;/p&gt;
&lt;p&gt;where $H$ is the random variable corresponding to choice of hypothesis $\mu$ sampled from $p(\mu)$ and $X_{1:\abs{x}}$ is the random variable corresponding to choice of length-$\abs{x}$ data sampled from $p(x)$. This quantity is sometimes called &lt;strong&gt;Bayesian surprise&lt;/strong&gt; (see &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2860069/&#34;target=&#34;_blank&#34;&gt;ref 1&lt;/a&gt; and &lt;a href=&#34;https://www.fil.ion.ucl.ac.uk/~karl/The%20free-energy%20principle%20A%20unified%20brain%20theory.pdf&#34;target=&#34;_blank&#34;&gt;ref 2&lt;/a&gt;), though more commonly &amp;ldquo;surprise&amp;rdquo; refers to the total info gain $h_\mu(\dom{\H}{x})$ (&lt;a href=&#34;https://en.wikipedia.org/wiki/Information_content&#34;target=&#34;_blank&#34;&gt;ref 1&lt;/a&gt;, &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;ref 2&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;posterior-consistency&#34;&gt;Posterior consistency&lt;/h3&gt;
&lt;p&gt;When $\mf{H}$ is a strict partition, this is a special (and desirable) case in Bayesian inference, called &lt;strong&gt;posterior consistency&lt;/strong&gt;. Informally, posterior consistency is when posterior probabilities converge to one-hot (or Dirac delta) distributions (almost surely). Posterior consistency is a property of a particular set of hypotheses $\M$, and is invariant to prior probabilities (assuming suppose on $\M$).&lt;/p&gt;
&lt;p&gt;If $\M$ has the posterior consistency property, then the corresponding $\mf{H}$ will be &lt;em&gt;nearly&lt;/em&gt; a partition, where $\mu(\h\up{\mu}\cap\h\up{\nu}))=\nu(\h\up{\mu}\cap\h\up{\nu}))=0$, for $\mu,\nu\in\M$.&lt;/p&gt;
&lt;p&gt;Posterior inconsistency in $\M$ corresponds to $\mf{H}$ which has overlapping compound hypotheses (of non-zero measure).&lt;/p&gt;
&lt;p&gt;Why do we want $\mf{H}$ to be a partition? So then information gained about one hypothesis $\mu$, corresponding to compound hypothesis $\h\up{\mu}$, necessarily implies information loss about all other hypotheses. If compound hypotheses overlap, then you can become more certain about multiple hypotheses at the same time, and in the infinite data limit, many hypotheses may remain (and we have not achieved our goal of prediction certainty by narrowing down to one hypothesis).&lt;/p&gt;
&lt;h3 id=&#34;posterior-convergence&#34;&gt;Posterior convergence&lt;/h3&gt;
&lt;p&gt;For any typical set $\h\up{\mu}$, there are actually infinitely many computable measures with the same typical set. Why is that? For any $\mu$, a new measure $\mu&#39;$ can be constructed that assigns different probabilities than $\mu$ to finite sequences, e.g. $\mu&#39;(\o_{1:n}) \neq \mu(\o_{1:n})$, while preserving the limiting compression rates:&lt;/p&gt;
&lt;p&gt;$$\lim_{n\to\infty} \frac{1}{n}\lg\par{\frac{1}{\mu(\o_{1:n})}} = \lim_{n\to\infty} \frac{1}{n}\lg\par{\frac{1}{\mu&#39;(\o_{1:n})}}$$&lt;/p&gt;
&lt;p&gt;A difference in probabilities of finite sequence $\o_{1:n}$ corresponds to a constant offset in compression length (according to Shannon):&lt;/p&gt;
&lt;p&gt;$$\abs{\lg\par{\frac{1}{\mu(\o_{1:n})}} - \lg\par{\frac{1}{\mu&#39;(\o_{1:n})}}} = C &amp;lt; \infty$$&lt;/p&gt;
&lt;p&gt;Any finite difference becomes negligible as $n\to\infty$. $\mu$ is said to have &lt;strong&gt;posterior convergence&lt;/strong&gt; to $\mu&#39;$.&lt;/p&gt;
&lt;p&gt;This is a strange predicament, because it implies that there are infinitely many probability measures that essentially encode the same randomness. That is to say, the measurement of randomness of finite sequences is not uniquely determined. This actually falls in line with the results of algorithmic information theory, where optimal compression length (Kolmogorov complexity) depends on choice of programming language (universal Turing machine), and is also arbitrary for that reason.&lt;/p&gt;
&lt;p&gt;It is the infinite sequences which have a unique quantity of randomness, in a sense. Non-computable infinite sequences have a compressed length that is also infinite, but often a finite compression rate: $\limsup_{n\to\infty}\frac{Km(\o_{1:n})}{n}$. Moreover, $\o$ has a unique limiting data posterior, i.e.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lim_{n\to\infty} \mu(\o_n \mid \o_{&amp;lt;n}) - \mu&#39;(\o_n \mid \o_{&amp;lt;n}) = 0\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;(almost surely), for any two measures $\mu$ and $\mu&#39;$ which have the same typical set $\h\up{\mu}$. Solomonoff&amp;rsquo;s universal data distribution $\xi$ (the mixture of all semicomputable semimeasures, see &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/#solomonoff-induction&#34;&gt;Deconstructing Bayesian Inference#solomonoff-induction&lt;/a&gt;) has posterior convergence to all such $\mu$. In fact, Solomonoff&amp;rsquo;s mixture can be used as a test for whether $\o$ is $\mu$-typical by observing if $\lim_{n\to\infty} \xi(\o_n \mid \o_{&amp;lt;n}) - \mu(\o_n \mid \o_{&amp;lt;n}) = 0$ (almost surely).&lt;/p&gt;
&lt;p&gt;The takeaway here is that if compound hypothesis $\h$ is the typical set for some measure $\mu$, then if we don&amp;rsquo;t specify any particular measure, the limiting information gain $h_\H(\dom{\h}{x&amp;rsquo;y} \mid \dom{\h}{x})$ as $\abs{x}\to\infty$ converges to something unique. So if we have infinite hypothesis sets and don&amp;rsquo;t want to arbitrarily choose a measure, not all hope is lost.&lt;/p&gt;
&lt;h1 id=&#34;shannon-equivalence&#34;&gt;Shannon Equivalence&lt;/h1&gt;
&lt;p&gt;I defined the quantities of information above using the set cardinality function $\abs{\cdot}$ to measure the sizes of sets (called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Counting_measure&#34;target=&#34;_blank&#34;&gt;counting measure&lt;/a&gt;). In general, the size of a set can be defined with a &lt;strong&gt;measure&lt;/strong&gt;, which is a function from subsets to non-negative real numbers. So a measure $\mu$ on $\H$ has the type signature $\mu : 2^\H \to \mb{R}_{\geq 0}$ (though technically we need to restrict ourselves to &lt;em&gt;measurable&lt;/em&gt; subsets of $\H$, see my &lt;a href=&#34;http://zhat.io/articles/primer-probability-theory#primer-to-measure-theory&#34;target=&#34;_blank&#34;&gt;primer to measure theory&lt;/a&gt; for details). Furthermore, if we choose measure $\mu$ s.t. $\mu(\H) = 1$, then $\mu$ is called a &lt;strong&gt;probability measure&lt;/strong&gt; (or a &lt;strong&gt;normalized measure&lt;/strong&gt;). See my &lt;a href=&#34;http://zhat.io/articles/primer-probability-theory#definitions&#34;target=&#34;_blank&#34;&gt;primer to probability theory&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;If we replace $\abs{\cdot}$ with probability measure $\mu(\cdot)$ everywhere in the quantities of information defined above, then we get the usual Shannon definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$h_\mu(A) = -\lg \mu(A)$&lt;/li&gt;
&lt;li&gt;$h_\mu(A \mid B) = -\lg \par{\frac{\mu(A \cap B)}{\mu(B)}}$&lt;/li&gt;
&lt;li&gt;$i_\mu(A, B) = \lg\par{\frac{\mu(A\cap B)}{\mu(A)\mu(B)}}$&lt;/li&gt;
&lt;li&gt;$\mb{H}_\mu(\mf{A}) = \sum_{A\in\mf{A}} \mu(A) h_\mu(A)$&lt;/li&gt;
&lt;li&gt;$\mb{H}_\mu(\mf{A}\mid B) = \sum_{A\in\mf{A}} \mu(A\mid B) h_\mu(A\mid B)$&lt;/li&gt;
&lt;li&gt;$\mb{H}_\mu(\mf{A}\mid\mf{B}) = -\sum_{A\in\mf{A}}\sum_{B\in\mf{B}} \mu(A\cap B) h_\mu(A\mid B)$&lt;/li&gt;
&lt;li&gt;$\mb{I}_\mu(\mf{A}, \mf{B}) = \sum_{A\in\mf{A}}\sum_{B\in\mf{B}} \mu(A\cap B) i_\mu(A, B)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $A,B\subseteq\H$ and $\mf{A},\mf{B}$ are two partitions of $\H$. Note that $\abs{\H}$ disappears because it becomes $\mu(\H) = 1$.&lt;/p&gt;
&lt;h2 id=&#34;optimal-compression&#34;&gt;Optimal Compression&lt;/h2&gt;
&lt;p&gt;Now we see that Bayesian information theory is mathematically equivalent to Shannon&amp;rsquo;s information theory, where a probability measure $\mu$ is used to measure the sizes of hypothesis sets (sets of predictions).&lt;/p&gt;
&lt;p&gt;However, what is the connection between narrowing down hypothesis sets and optimal compression? Given probability measure $\mu$ on $\H$, the $\mu$-probability of finite observation $x\in\X^*$ is $\mu(\dom{\H}{x})$. (We can abuse notation and write $\mu(x)$ where $x$ is shorthand for $\dom{\H}{x}$ when given as the argument to $\mu$.) Then is $h_\mu(\dom{\H}{x})$ the optimal compressed length of $x$?&lt;/p&gt;
&lt;p&gt;For finite strings, optimal compression isn&amp;rsquo;t a well defined notion. According to algorithmic information theory, we use the shortest program that outputs $x$ as the compressed representation of $x$, but the length of that shortest program depends on our arbitrary choice of programming language. We can achieve an encoded length of approximately $h_\mu(\dom{\H}{x})$ by using &lt;a href=&#34;https://en.wikipedia.org/wiki/Arithmetic_coding&#34;target=&#34;_blank&#34;&gt;arithmetic coding&lt;/a&gt;, with $\mu$ as the provided measure (ignoring the length of the arithmetic decoder program itself).&lt;/p&gt;
&lt;p&gt;Shannon&amp;rsquo;s information theory operates in the domain of random data, and provides optimal code lengths &lt;em&gt;in expectation&lt;/em&gt;. In the Bayesian information theory I&amp;rsquo;ve outlined above, we are not working with randomness, but non-determinism (the agent&amp;rsquo;s predictions are not uniquely determined). However, as the data length goes to infinity, these two conceptions of probability become intertwined.&lt;/p&gt;
&lt;p&gt;Let $\o\in\X^\infty$ be an infinite sequence, and an agent observed the finite prefix $\o_{1:n}$. If the agent has a hypothesis set $\H$ with probability measure $\mu$, the agent can use arithmetic coding w.r.t. $\mu$ to achieve compressed length $-\lg \mu(\o_{1:n})$, and limiting compression rate&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\limsup_{n\to\infty} \frac{1}{n}\lg\par{\frac{1}{\mu(\o_{1:n})}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If $\o$ is $\mu$-typical, than this will be the optimal compression rate achievable (according to algorithmic information theory). However, if $\o$ is not $\mu$-typical then the agent&amp;rsquo;s compression rate will be worse than the optimum.&lt;/p&gt;
&lt;p&gt;Let $\X = \set{0,1}$. For each bit of data $\o_n$, the approximate compressed length of that bit is $-\lg\mu(\o_n \mid \o_{&amp;lt;n})$. So if the agent gains more than 1 bit of information from $\o_n$, arithmetic coding w.r.t. $\mu$ will actually assign more than one bit to $\o_n$ in the compressed representation. If the agent&amp;rsquo;s info gain remains high in the long run, this &amp;ldquo;compression&amp;rdquo; of $\o$ will end up being longer than $\o$ itself (specifically, the compression of $\o_{1:n}$ will be longer than $n$ as $n\to\infty$). This gives us a precise sense about whether the agent is doing a good job at predicting the part of $\o$ that can be predicted: If the agent&amp;rsquo;s $\mu$-compression rate is better than the length of the data itself then the agent is predicting the data at least better than random.&lt;/p&gt;
&lt;p&gt;A mixture distribution can be viewed as a hedge against bad compression. Suppose $\o$ is not $\mu$-typical. If instead of using $\mu$ to predict $\o$, we had a set of distributions $\M$ of which $\mu$ is a member, and we use the mixture $p = \sum_{\nu\in\M} w_\nu \nu$ to predict $\o$. If $\o$ is typical w.r.t. at least one $\nu\in\M$, then $\o$ is also $p$-typical. The difference between using $\nu$ and $p$ to compress $\o$ is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\limsup_{n\to\infty} \lg\par{\frac{1}{p(\o_{1:n})}} - \lg\par{\frac{1}{\nu(\o_{1:n})}} \,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is a constant cost in compression length, and becomes negligible in the long run as $n\to\infty$, i.e. the compression rate using $p$ and $\nu$ is the same. So using a mixture to compress $\o$ may incur additional cost (extra bits) initially, in the long run it is no worse than using the &amp;ldquo;true&amp;rdquo; hypothesis $\nu$ (there may be more than one &amp;ldquo;true&amp;rdquo; hypothesis in $\M$). A good strategy is then to make $\M$ as large as possible. This is the premise behind Solomonoff induction, where $\M$ is the set of all semicomputable semimeasures.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deconstructing Bayesian Inference</title>
      <link>https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\H}{\mc{H}}&lt;br&gt;
\newcommand{\P}{\mc{P}}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
$$&lt;/p&gt;
&lt;h1 id=&#34;constructing-bayesian-inference&#34;&gt;Constructing Bayesian Inference&lt;/h1&gt;
&lt;p&gt;Before deconstructing Bayesian inference, I will present the general definition. At any point, feel free to look at the &lt;a href=&#34;#use-cases&#34;&gt;#Use Cases&lt;/a&gt; section for examples of Bayesian inference to use as intuition pumps.&lt;/p&gt;
&lt;p&gt;An &amp;ldquo;agent&amp;rdquo; here refers to a physical entity that tries to predict the future. An agent can be a robot or a biological organism. Either way, the agent receives a stream of sensory data over the course of its life. The agent&amp;rsquo;s goal is to predict the future of this stream, in some capacity. Predictions about high-level objects and states of the world are made indirectly by predicting their effect on the sensory stream. That is to say, all prediction can be rolled into predicting the sensory stream. Though the agent may also act to influence the future of its sensory stream, for simplicity, I will consider only prediction and not actions.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s work in discrete time.&lt;br&gt;
Let $\X$ be the set of possible observations at each time-step.&lt;br&gt;
The agent&amp;rsquo;s sensory stream is a sequence $(x_1, x_2, x_3, \dots) \in \X^\infty$. I&amp;rsquo;m assuming the sensory stream is infinite, i.e. the agent lives forever. This is not a reasonable assumption but it&amp;rsquo;s workable. The alternative (assuming the stream is finite) is more complicated because it requires the agent to predict the end time of its own stream (presumably when it will die).&lt;/p&gt;
&lt;p&gt;Let $\o\in\X^\infty$.&lt;br&gt;
$\o_i$ denotes the $i$-th binary value in $\o$.&lt;br&gt;
$\o_{(i_1,i_2,\dots)}$ denotes the sub-sequence of $\o$ indexed by the sequence $(i_1,i_2,\dots)$.&lt;br&gt;
$\o_{i:j} = \o_{(i,i+1,\dots,j-1,j)}$ denotes the slice of $\o$ starting from $i$ and ending at $j$ (inclusive).&lt;br&gt;
$\o_{&amp;lt;i} = \o_{1:i-1}$ denotes the slice up to position $i-1$.&lt;br&gt;
$\o_{\leq i} = \o_{1:i}$ includes $i$.&lt;br&gt;
$\o_{&amp;gt;i} = \o_{i+1:\infty}$ denotes the unbounded slice starting at position $i+1$.&lt;/p&gt;
&lt;p&gt;Let $\X^* = \X^0 \cup \X^1 \cup \X^2 \cup \X^3 \cup \dots$ be the union of all finite cartesian products of $\X$, i.e. the set of all finite sequences of any length.&lt;br&gt;
Let $x,y\in\X^*$ be finite sequences.&lt;br&gt;
Denote $\abs{x}$ as the length of $x$.&lt;br&gt;
Denote $x`y = (x_1, \dots, x_\abs{x}, y_1, \dots, y_\abs{y})$ as the sequence concatenation of $x$ and $y$.&lt;br&gt;
Denote $x \sqsubset y$ to mean that $x = y_{1:\abs{x}}$, i.e. $x$ is a prefix of $y$.&lt;/p&gt;
&lt;h2 id=&#34;defining-a-hypothesis&#34;&gt;Defining A Hypothesis&lt;/h2&gt;
&lt;p&gt;As the agent gains sensory data, the agent will try to explain that data with various hypotheses. What constitutes a hypothesis is perhaps an open problem in epistemology. I will give the standard idea of &amp;ldquo;hypothesis&amp;rdquo; within Bayesian epistemology.&lt;/p&gt;
&lt;p&gt;A hypothesis is a &lt;strong&gt;probability measure&lt;/strong&gt;, $\mu$, over all possible sensory streams, $\X^\infty$. That means we can compute the $\mu$-probability of any set of infinite sequences, i.e. $\mu(A)$ for $A \subseteq \X^\infty$. Typically there is a set of hypotheses $\H$ under consideration, i.e. $\H$ is a set of different $\mu$.&lt;/p&gt;
&lt;p&gt;For any $\o\in\X^\infty$, the prefix $\o_{1:n}$ is a partial sensory sequence. The marginal $\mu$-probability of $\o_{1:n}$ is $\mu(\Gamma_{\o_{1:n}})$ where $\Gamma_{\o_{1:n}} = \set{\z\in\X^\infty \mid \o_{1:n}\sqsubset\z}$ is the set of all infinite sequences starting with $\o_{1:n}$, called a &lt;strong&gt;cylinder set&lt;/strong&gt;. For notational simplicity, I will write  $\mu(x)$ to denote $\mu(\Gamma_{x})$ when $x$ is a finite sequence, so $\mu(\o_{1:n})$ denotes $\mu(\Gamma_{\o_{1:n}})$.&lt;/p&gt;
&lt;p&gt;The quantity $\mu(\o_n \mid \o_{&amp;lt;n}) = \mu(\o_{1:n})/\mu(\o_{&amp;lt;n}) = \mu(\o_{1:n})/\mu(\o_{1:n-1})$ is the conditional $\mu$-probability of $\o_n$ given $\o_{&amp;lt;n}$ was already observed.&lt;/p&gt;
&lt;p&gt;$\mu$ defined in this way will guarantee that the following holds:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(\o_{&amp;lt;n}) = \int_\X \mu(\o_{&amp;lt;n}`\chi)d\chi\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;(replace integral with sum for countable $\X$.)&lt;/p&gt;
&lt;p&gt;Note that for most $\o\in\X^\infty$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lim_{n\to\infty} \mu(\o_{1:n}) = 0\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is similar to how a probability distribution on the real unit interval may assign 0 probability to any particular real number. Here $\o_{1:n}$ acts like an interval, which has probability mass, and the full sequence $\o$ acts like a single point, which does not have probability mass.&lt;/p&gt;
&lt;p&gt;A hypothesis $\mu$ ideally represents some primitive way the world might be. We might say a hypothesis probability $\mu(\o_{1:n})$ is &lt;em&gt;objective&lt;/em&gt; in some sense. Hypotheses that assign probabilities to outcomes are supposing the universe has inherent randomness that cannot be known before hand (we could also have deterministic hypotheses that assign probabilities of either 0 or 1).&lt;/p&gt;
&lt;p&gt;Bayesian inference distinguishes between two kinds of uncertainty: that due to inherent randomness in the universe, and that due to lack of knowledge of the agent. This distinction between kinds of uncertainty may not be precise (and I&amp;rsquo;m not sure I agree that such a distinction should be made), but this is typically how hypothesis probabilities are conceptually distinguished from subjective data probabilities (which I&amp;rsquo;ll get to in a moment).&lt;/p&gt;
&lt;p&gt;In this framing, the goal of Bayesian inference is to calculate one&amp;rsquo;s certainty (or uncertainty) that each hypothesis being considered explains the observed data. This kind of uncertainty is represented with probabilities just like irreducible uncertainty, but the former is in principle reducible to certainty. As the agent observes data, these probabilities change, and hence the agent&amp;rsquo;s uncertainty about each hypothesis changes over time (potentially approaching certainty).&lt;/p&gt;
&lt;h2 id=&#34;defining-bayesian-inference&#34;&gt;Defining Bayesian Inference&lt;/h2&gt;
&lt;p&gt;Let $\X^\infty$ be the set of all possible observations (infinite data sequences).&lt;br&gt;
Let $\H$ be a set of hypotheses (probability measures on $\X^\infty$).&lt;/p&gt;
&lt;p&gt;I will induce a joint probability distribution on $\H$ and $\X^\infty$.&lt;br&gt;
Let $p(\mu)$ be the &lt;strong&gt;prior probability&lt;/strong&gt; of $\mu\in\H$.&lt;br&gt;
Let $p(\o_{1:n} \mid \mu) = \mu(\o_{1:n})$ be the &lt;strong&gt;data probability&lt;/strong&gt; of $\o_{1:n}$ under hypothesis $\mu$. This is read as, &amp;ldquo;the probability of data $\o_{1:n}$ given hypothesis $\mu$.&amp;rdquo; Here, &amp;ldquo;giving the hypothesis&amp;rdquo; is equivalent to specifying what data distribution should be used to calculate the probability of $\o_{1:n}$.&lt;/p&gt;
&lt;p&gt;$p(\mu,\o_{1:n}) = p(\mu)p(\o_{1:n} \mid \mu) = p(\mu)\mu(\o_{1:n})$ is the joint probability of $\mu$ and $\o_{1:n}$. From here, we can compute any other quantity of interest.&lt;/p&gt;
&lt;p&gt;$$p(\o_{1:n}) = \int_\H p(\mu,\o_{1:n})d\mu = \int_\H p(\mu)\mu(\o_{1:n})d\mu$$&lt;/p&gt;
&lt;p&gt;is called the &lt;strong&gt;subjective (Bayesian) data probability&lt;/strong&gt; of $\o_{1:n}$ (replace integrals with sums if $\H$ is discrete). $p(\o_{1:n})$ does not condition on a hypothesis, and is the sum/integral over every hypothesis probability $\mu(\o_{1:n})$ weighted by the prior $p(\mu)$. You can think of $p(\o_{1:n})$ as a weighted-average over hypothesis probabilities.&lt;/p&gt;
&lt;p&gt;If a hypothesis probability $\mu(\o_{1:n})$ is &lt;em&gt;objective&lt;/em&gt; in some sense (a hypothesis represents the way the world actually might be), then a prior probability $p(\mu)$ represents the agent&amp;rsquo;s state of certainty/uncertainty about $\mu$ being true (how the world actually is). If the agent knew that $\mu$ was the case, then the agent would just use $\mu$ to predict the future sensory data, where $\mu$-probabilities are due to inherent randomness in the data. However, since the agent has uncertainty about which hypothesis is the case, $p(\o_{1:n})$ represents the agent&amp;rsquo;s total uncertainty about $\o_{1:n}$ occurring, which is a combination of inherent randomness in the data and the agent&amp;rsquo;s lack of knowledge about which hypothesis is true.&lt;/p&gt;
&lt;p&gt;Another quantity of interest:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(\mu\mid\o_{1:n}) &amp;amp;= p(\mu,\o_{1:n})/p(\o_{1:n}) \\ \\&lt;br&gt;
&amp;amp;= \frac{p(\mu)p(\o_{1:n}\mid\mu)}{p(\o_{1:n})}\\ \\&lt;br&gt;
&amp;amp;= p(\mu)\frac{\mu(\o_{1:n})}{p(\o_{1:n})}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is called the &lt;strong&gt;hypothesis posterior probability&lt;/strong&gt; (or more commonly, just &amp;ldquo;&lt;strong&gt;posterior&lt;/strong&gt;&amp;quot;) of hypothesis $\mu$ given data $\o_{1:n}$. This quantity represents the agent&amp;rsquo;s certainty/uncertainty about hypothesis $\mu$ after observing $\o_{1:n}$. If we are viewing $p(\mu)$ as a weight, then the weight on $\mu$ updates to $p(\mu\mid\o_{1:n})$ when $\o_{1:n}$ is observed. In this sense, the agent updates its knowledge (confidence) about the hypotheses in $\H$ when data is observed. The last equation is a prescription for an update rule on these uncertainty weights on $\mu$, taking the form $w&#39; = \gamma w$, where $\gamma = \mu(\o_{1:n})/p(\o_{1:n})$ is a multiplicative adjustment on the prior weight $w$.&lt;/p&gt;
&lt;p&gt;Observing additional data amounts to appending $\o_{n+1:m}$ to $\o_{1:n}$, and the weight on $\mu$ updates again to $p(\mu\mid\o_{1:m})$. This iterative process of observing more data and updating can go on forever, so long as the data space $\X^\infty$ consists of infinite sequences.&lt;/p&gt;
&lt;p&gt;As a side note, you should recognize $p(\mu\mid\o_{1:n})=p(\o_{1:n}\mid\mu)p(\mu)/p(\o_{1:n})$ as the classic form of &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem#Statement_of_theorem&#34;target=&#34;_blank&#34;&gt;Bayes rule&lt;/a&gt; (typically used to explain Bayesian inference). Usually, it is written like this: $p(H \mid D) = p(D \mid H)p(H)/p(D)$, where $H$ is a random variable for hypotheses (often parameter $\Theta$ is used in place of $H$), and $D$ is a random variable for datasets. In my opinion this form hides the sequential nature of Bayesian inference and is pedagogically confusing for that reason.&lt;/p&gt;
&lt;p&gt;In general, the agent wants to predict what sensory data will occur after $\o_{\leq n}$ has been observed. For that, we need the &lt;strong&gt;data posterior probability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(\o_{&amp;gt;n} \mid \o_{\leq n}) &amp;amp;= \int_\H p(\mu, \o_{&amp;gt;n} \mid \o_{\leq n})d\mu \\&lt;br&gt;
&amp;amp;= \int_\H p(\o_{&amp;gt;n} \mid \mu, \o_{\leq n})p(\mu\mid\o_{\leq n})d\mu \\&lt;br&gt;
&amp;amp;= \int_\H p(\mu\mid\o_{\leq n})p(\o_{&amp;gt;n} \mid \o_{\leq n},\mu)d\mu \\&lt;br&gt;
&amp;amp;= \int_\H p(\mu\mid\o_{\leq n})\mu(\o_{&amp;gt;n} \mid \o_{\leq n})d\mu \\&lt;br&gt;
&amp;amp;= \int_\H p(\mu)\frac{\mu(\o_{\leq n})}{p(\o_{\leq n})}\mu(\o_{&amp;gt;n} \mid \o_{\leq n})d\mu\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Like the subjective data probability $p(\o_{\leq n})$, the data posterior probability $p(\o_{&amp;gt;n} \mid \o_{\leq n})$ is subjective, in the sense that it reflects the agent&amp;rsquo;s uncertainty about the outcome, as well as any irreducible randomness. These two quantities have the same form: $p(\o_{\leq n})$ averages over $p(\mu)\mu(\o_{\leq n})$, and $p(\o_{&amp;gt;n} \mid \o_{\leq n})$ averages over $p(\mu\mid\o_{\leq n})\mu(\o_{&amp;gt;n} \mid \o_{\leq n})$. Either way, $p(\o_{\leq n})$ and $p(\o_{&amp;gt;n} \mid \o_{\leq n})$ are both weighted averages over all hypothesis probabilities, i.e. the inherent randomness of each hypothesis is weighted by the agent&amp;rsquo;s uncertainty about that hypothesis. The only difference is that the posterior conditions everything on the observed $\o_{\leq n}$.&lt;/p&gt;
&lt;h1 id=&#34;deconstructing-bayesian-inference&#34;&gt;Deconstructing Bayesian inference&lt;/h1&gt;
&lt;p&gt;To review, we assumed the agent has a set of hypotheses $\H$, where each hypothesis $\mu\in\H$ is a probability measure on infinite data sequences $\X^\infty$. For partial observation $\o_{1:n}$, each hypothesis assigns probability $\mu(\o_{1:n})$. Putting a prior on $\H$ allows us to compute $p(\o_{1:n})$, which is the &amp;ldquo;average&amp;rdquo; $\mu$-probability of $\o_{1:n}$. The resulting distribution $p$ is called the subjective data distribution.&lt;/p&gt;
&lt;h2 id=&#34;removing-hypotheses&#34;&gt;Removing hypotheses&lt;/h2&gt;
&lt;p&gt;Question: What is the difference between a hypothesis $\mu$ and a subjective data distribution $p$? Do we need hypotheses at all?&lt;/p&gt;
&lt;p&gt;They appear to have the same form: $\mu(\o_{1:n})$ vs $p(\o_{1:n})$. Above I gave a hand-wavy interpretational difference between the two. However, it is the case that hypotheses are not strictly necessary for predicting the continuation of a data sequence (equivalent to the singleton hypothesis space $\H = \set{p}$).&lt;/p&gt;
&lt;p&gt;The subjective data distribution $p$ can be defined directly by specifying the distribution $p(x_n \mid x_{&amp;lt;n})$ on $x_n\in\X$ for all finite $x_{&amp;lt;n} \in \X^*$. That is because for all $x_{1:n}\in\X$, the probability factorizes: $p(x_{1:n}) = p(x_{n}\mid x_{&amp;lt;n})p(x_{n-1}\mid x_{&amp;lt;n-1})\dots p(x_2\mid x_{&amp;lt;2})p(x_1)$. This amounts to filling in values for each node in a tree. For example, if $\X=\set{0,1}$, then we have a binary tree:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210317161633.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Each node in the tree is assigned a conditional probability distribution, i.e. for each node at position $x_{&amp;lt;n}$ (where $x_{&amp;lt;n} = (x_1, x_2,\dots,x_{n-1})$ encodes a path from the root), the probability $p(\chi \mid \dots)$ is specified for each $\chi\in\X$.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Each node&amp;rsquo;s distribution, $p(\chi \mid \dots)$, can be chosen independently from every other node (there are no constraints on these conditional distributions). Therefore, since the entire tree uniquely determines $p(x_{&amp;lt;n})$ (by walking down the edges of the tree corresponding to the observed data sequence $x_{&amp;lt;n}$ and multiplying the conditional probabilities at each node in the path, i.e. $p(x_i \mid x_{&amp;lt;i})$ at node $x_{&amp;lt;i}$), a Bayesian agent is free to choose any arbitrary conditional predictions it wants.&lt;/p&gt;
&lt;p&gt;Clearly, if we are only concerned with data predictions of the form $p(\o_{n:m}\mid \o_{&amp;lt;n})$, then we do not actually need hypotheses at all. Providing a tree specifying the subjective conditional data probabilities is enough to perform Bayesian inference.&lt;/p&gt;
&lt;p&gt;This reveals something peculiar about Bayesian inference: &lt;strong&gt;A Bayesian agent chooses its predictions for all eventualities beforehand, and never deviates for all time. Furthermore these predictions can be totally arbitrary.&lt;/strong&gt; There does not appear to be any actual learning taking place, since the agent just follows a path down the tree as data comes in and provides predetermined prediction probabilities. In the Bayesian perspective, learning IS narrowing down a pre-defined possibility space with data. If decision making using Bayesian inference is rational, perhaps rationality amounts to consistency, i.e. never deviating from pre-chosen predictions.&lt;/p&gt;
&lt;p&gt;One might argue that freely choosing a subjective data distribution without defining hypotheses is not Bayesian. My reply is that I&amp;rsquo;ve shifted what we are counting as hypothesis. In this perspective, a hypothesis is a single data sequence continuation. In this sense, all hypotheses are deterministic data sequences (though they may be algorithmically random), and the subjective data distribution is counting up the contributions of all these hypotheses to each finite length prediction. More on this in &lt;a href=&#34;https://danabo.github.io/blog/posts/bayesian-information-theory/&#34;&gt;Bayesian information theory&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;removing-probabilities&#34;&gt;Removing probabilities&lt;/h2&gt;
&lt;p&gt;We can go a step further and ask, do we need probabilities at all?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s imagine that the agent has the same predetermined tree as above, except each node is assigned a single prediction instead of a probability distribution over $\X$. Let $x\in\X^*$ and denote $\hat{\x}_x\in\X$ as the prediction given the sequence $x$ is observed. Choosing $\hat{\x}_x$ for each $x$ uniquely defines a tree, where $x = (x_1, x_2, \dots)$ encodes a path from the root to a node, and $\hat{\x}_x$ is the value assigned to that node.&lt;/p&gt;
&lt;p&gt;Consider an example. Suppose $\X = \set{0,1}$ and that we have the following prediction tree:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210329223311.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;This tree has infinite depth.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In the figure, each node is assigned a prediction. Without any data, the agent predicts $0$. Given $0$ is observed, the agent predicts $1$. Given $01$ is observed, the agent predicts $0$. And so on.&lt;/p&gt;
&lt;p&gt;Suppose we observe $010$, which falls in line with all of the agent&amp;rsquo;s predictions up to that point. The prediction given $010$ is $1$. Suppose we observe $0$ next (for a total observation of $0100$). Then the agent&amp;rsquo;s prediction of $1$ is wrong, but that is okay since the agent has sub-tree at $0100$ and can go on making predictions from there.&lt;/p&gt;
&lt;p&gt;Is this setup technically Bayesian? We could convert this deterministic prediction tree to a subjective data distribution that assigns a probability 0 or 1 to all outcomes. So in this example, $p(x_1=0) = 1$ and $p(x_1=1)=0$, corresponding to a prediction of $0$ given nothing. $p(x_2=0\mid 0)=0$ and $p(x_2=1\mid 0)=1$, and $p(x_3=0\mid 01)=1$ and so on.&lt;/p&gt;
&lt;p&gt;The problem is that $p$ defined in this way is degenerate. Suppose the agent wrongly predicts the continuation of $010$ to be $1$ when it is actually $0$. Then&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(0101) &amp;amp;= p(x_4=1\mid 010)p(x_3=0\mid 01)p(x_2=1\mid 0)p(x_1=0) \\ &amp;amp;= 1\cdot 1\cdot 1\cdot 1 = 1\,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;But $p(0100) = p(x_4=0 \mid 010)\dots = 0\cdot 1\cdot 1\cdot 1 = 0$. So from then on, the subjective probability of the data continuing $0100$ is 0, but the conditional probabilities may not be. If $p$ is defined as a measure on $\X^\infty$ instead of indirectly via the conditional probabilities specified in the tree, then $p$ cannot simultaneously have well-defined conditional probabilities and zero probability unconditionally (e.g. $p(x_5=0 \mid 0100) = 1$ while $p(01000) = 0$ and $p(0100) = 0$).&lt;/p&gt;
&lt;p&gt;We might conclude that this kind of deterministic prediction tree does not formally conform to the definition of Bayesian inference I gave earlier. However, this tree is the limit of the probabilistic prediction tree above as each conditional probability $p(x_i \mid x_{&amp;lt;i})$ goes to 0 or 1. Just as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirac_delta_function&#34;target=&#34;_blank&#34;&gt;Dirac delta distribution&lt;/a&gt; is not technically a function, but can be defined as the limit of Gaussian functions as their variance goes to 0, we might be able to do the same with deterministic prediction trees.&lt;/p&gt;
&lt;p&gt;Note that an agent that makes the same prediction no matter what is observed (a rigid agent) is technically Bayesian, with a subjective data distribution that puts all probability mass on the prediction sequence $\o\in\X^\infty$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x_{1:n}) = \begin{cases}1 &amp;amp; x_{1:n} = \o_{1:n} \\ 0 &amp;amp; x_{1:n} \neq \o_{1:n}\end{cases}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Such an agent is not interesting, as it does not even behave as though it learns.&lt;/p&gt;
&lt;p&gt;We shall see that an agent that does behave as though it learns (conditionally determines its predictions based on past history) need not provide Bayesian probabilities on its predictions for prediction uncertainty to naturally emerge.&lt;/p&gt;
&lt;h1 id=&#34;reconstructing-bayesian-inference&#34;&gt;Reconstructing Bayesian Inference&lt;/h1&gt;
&lt;h2 id=&#34;non-determinism&#34;&gt;Non-Determinism&lt;/h2&gt;
&lt;p&gt;To add clarity to what I want to explain next, let&amp;rsquo;s represent the deterministic prediction tree above as a prediction function $f : \X^* \to \X$, so that $f(x) = \hat{\x}_x$ is the prediction given data sequence $x\in\X^*$. That is to say, $f(x)$ returns the value at node $x$ in the tree. We can think of $f$ as an assignment function of values to nodes.&lt;/p&gt;
&lt;p&gt;Suppose we want to predict what will happen further out into the future. Specifically, suppose we observe $\o_{&amp;lt;n}\in\X^*$ and we want to predict the outcome $\o_m$ at time $m &amp;gt; n$. We don&amp;rsquo;t have access to the outcome $\o_n$ or anything after it. That is to say, the prediction tree gives the prediction $f(\o_{&amp;lt;n})$ for step $n$, but to predict step $m$ we need $\o_{&amp;lt;m} = \o_{&amp;lt;n}`\o_{n:m-1}$.&lt;/p&gt;
&lt;p&gt;You might say that the solution is to fill in our missing data with predictions, i.e. choose $\hat{\o}_n=f(\o_{&amp;lt;n})$ as the outcome for step $n$, and $\hat{\o}_{n+1} = f(\o_{&amp;lt;n}`\hat{\o}_n)$ as the outcome for step $n+1$, etc., so that our prediction at step $m$ is $\hat{\o}_m=f(\o_{&amp;lt;n}`\hat{\o}_{n:m-1})$. However, I&amp;rsquo;d argue that this is not actually the agent&amp;rsquo;s prediction as defined by the prediction tree. If the agent instead observed $\o_{n:m-1} \neq \hat{\o}_{n:m-1}$, the agent might predict something different from $\hat{\o}_m$. We could define $\hat{\o}_m$ as the agent&amp;rsquo;s prediction of step $m$ given $\o_{&amp;lt;n}$. By doing so, we&amp;rsquo;d be assigning infinite sequences to each node $x$ in the prediction tree, rather than single outcomes, so that we may query the prediction of arbitrarily many timesteps given ONLY data $x$. We can represent this prediction tree by the function $g:\X^*\to\X^\infty$, which returns an infinite sequence in $\X^\infty$ instead of a single element of $\X$, such that $[g(x_{&amp;lt;n})]_{&amp;lt;n} = x_{&amp;lt;n}$ and $[g(x_{&amp;lt;n})]_n$ is the next-step prediction for $x_{&amp;lt;n}$. The sequence $[g(x_{&amp;lt;n})]_{\geq n}$  is the multi-step prediction for $x_{&amp;lt;n}$. Now it is clear that $[g(\o_{&amp;lt;m})]_m$ and $[g(\o_{&amp;lt;n})]_m$ are potentially different predictions, and we need to distinguish between them.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210330173612.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;The tree encoded by $g$, where each node is assigned an infinite sequence that begins with the location of the node in the tree. In this example, each sequence is the argmax prediction starting at that node, i.e. assuming every next-step prediction is correct.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In general, if we ask an agent for some prediction of any time-step, the agent can produce some sort of answer. However, if we wanted to know what answer the agent would produce if all requisite data was available (call this the agent&amp;rsquo;s best prediction), that may not be determinable with the data currently available. We are uncertain about what the agent&amp;rsquo;s best prediction will be (given all requisite data). A truthful agent would be just as uncertain as we are about its own future predictions.&lt;/p&gt;
&lt;p&gt;Let $\mc{F}_m(\o_{&amp;lt;n}) = \set{f(\o_{&amp;lt;n}`\tilde{\o}_{n:m-1}) \mid \tilde{\o}_{n:m-1}\in\X^{m-n+1}}$ be the set of all predictions the agent could make about step $m$ given data $\o_{&amp;lt;n}$. If this set contains more than one element, then the agent&amp;rsquo;s prediction at $m$ is not uniquely determined by $\o_{&amp;lt;n}$, i.e. non-deterministic.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210329223333.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;This is the non-deterministic prediction encoded by $\mc{F}_2 : \X^* \to \X$, where the untransformed tree encoded by $f:\X^*\to\X$ is our prediction tree example from above. $\mc{F}_2$ is obtained by superimposing the two sub-trees of $f$ at depth 1, i.e. directly under the root node.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This understanding of the word &amp;ldquo;non-deterministic&amp;rdquo; is standard in computer science, exemplified by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Nondeterministic_Turing_machine&#34;target=&#34;_blank&#34;&gt;non-deterministic Turing machine&lt;/a&gt;, which defines a set of possible state transitions and tape operations given any state (of both the tape and automata), rather than single one determined uniquely by each state. In general, something is regarded as non-deterministic if it can take on more than one possible value, i.e. it is not determined.&lt;/p&gt;
&lt;p&gt;I distinguish non-determinism from randomness, the latter of which can be defined as maximal incompressibility (called &lt;a href=&#34;http://www.scholarpedia.org/article/Algorithmic_randomness&#34;target=&#34;_blank&#34;&gt;algorithmic randomness&lt;/a&gt;). Often these two concepts are conflated. Probability distributions can represent both non-determinism (by quantifying relative amounts of possibilities) or degrees of randomness (via Martin-Lof randomness). The key insight is that probability need not represent both at the same time. If Bayesian probability is generally understood as quantifying prediction non-determinism, then we can view the &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_interpretations&#34;target=&#34;_blank&#34;&gt;Bayesian-frequentist distinction&lt;/a&gt; as stemming from this conceptual decoupling of non-determinism from randomness.&lt;/p&gt;
&lt;p&gt;I make the same connection between Bayesian uncertainty and non-determinism in &lt;a href=&#34;https://danabo.github.io/blog/posts/classical-vs-bayesian-reasoning/#the-bayesian-perspective&#34;&gt;Classical vs Bayesian Reasoning#the-bayesian-perspective&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-general-case&#34;&gt;The General Case&lt;/h3&gt;
&lt;p&gt;We can go a step further. Why are we assuming that a prediction of step $n$ can be made given $\o_{&amp;lt;n}$? Plenty of pertinent information may not be available in the data at all (or the agent does not know how to extract such information from the data, e.g. encrypted information). An honest agent would account for every input needed to make a certain prediction, even inputs that are not available in $\o_{&amp;lt;n}$.&lt;/p&gt;
&lt;p&gt;In general, we are left with a non-deterministic next-step prediction tree represented by $f : \Z \times \X^* \to \X$ where $\Z$ is an auxiliary input space (input in addition to the observed data stream), which is usually some latent space (set of possible data or world-states not observed). Each node $x$ in the prediction tree is assigned the prediction set $f(\Z, x) = \set{f(z,x) \mid z\in\Z}$. If the prediction set for $x$ is singleton (contains one element), then that prediction is deterministic. $f$ represents a prediction tree where each node is assigned a set of next-step predictions instead of just one.&lt;/p&gt;
&lt;h2 id=&#34;the-return-of-probability&#34;&gt;The Return Of Probability&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://danabo.github.io/blog/posts/classical-vs-bayesian-reasoning/#the-bayesian-axiom&#34;&gt;Classical vs Bayesian Reasoning#the-bayesian-axiom&lt;/a&gt;, I introduced the Bayesian axiom - an informal axiom of epistemology - which states that the relative fraction of model (in our case $f$) inputs which produce each output can be regarded as knowledge. In this case, if the agent&amp;rsquo;s prediction $f(\Z, \o_{&amp;lt;n})$ is not uniquely determined, we might want to proceed with our decision making anyway. Supposing we have some normalized measure $\mu$ on $\Z$ (ideally the uniform measure if there is one), then the relative fraction of $\Z$ that produces each prediction $\chi\in\X$ is&lt;/p&gt;
&lt;p&gt;$$p(\chi \mid x) = \mu\set{f(z,x) \mid z\in\Z \and f(z,x) = \chi}\,.$$&lt;/p&gt;
&lt;p&gt;Though the agent&amp;rsquo;s prediction is not uniquely determined, we&amp;rsquo;ve quantified the relative number of possible auxiliary inputs to $f$ that give each prediction. These quantities can be used for decision making (e.g. most likely outcome or expected return).&lt;/p&gt;
&lt;p&gt;We have recovered the probabilistic prediction tree paradigm from earlier without presupposing that predictions should be probabilistic. Though not defined to be probabilistic, an agent&amp;rsquo;s prediction function that requires inputs which are not on hand naturally results in non-determinism. Quantifying that non-determinism using relative sizes of possibility sets naturally results in probabilities.&lt;/p&gt;
&lt;p&gt;With this perspective in mind, we could interpret any explicitly defined subjective data distribution to be implying the existence of a deterministic prediction function. Bayesian probabilities represent the number of inputs to this prediction function that produce the corresponding outputs. Given $p$, a canonical deterministic prediction function $f$ is the decoder for a compressed representation of the data, with a uniform distribution on the compression.&lt;/p&gt;
&lt;p&gt;To be more specific, let $p$ be a probability measure on $\X^\infty$. Let $f : \Z^\infty \to \X^\infty$ be an &lt;a href=&#34;https://en.wikipedia.org/wiki/Arithmetic_coding&#34;target=&#34;_blank&#34;&gt;arithmetic decoder&lt;/a&gt; using $p$, i.e. $f$ takes as input an infinite compressed sequence $z_{1:\infty}\in\Z^\infty$ and outputs the decompressed sequence $x_{1:\infty}\in\X^\infty$. For finite inputs and outputs, use the cylinder set $f(\Gamma_{z_{1:n}})$. Putting a uniform probability measure on $\Z^\infty$ results in measure $p$ on $\X^\infty$. Now we can draw a connection between probability in three different contexts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Shannon Information&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;If $x_{1:\infty}$ is sampled randomly from $p$, then $z_{1:\infty}$ s.t. $f(z_{1:\infty}) = x_{1:\infty}$ achieves the optimal compression of $x_{1:\infty}$ according to Shannon. That is to say, $\abs{z_{1:n}} = n \approx -\lg p(f(\Gamma_{z_{1:n}}))$, and these quantities converge as $n\to\infty$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Algorithmic Randomness&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;If input $z_{1:\infty}$ is algorithmically random, then output $f(z_{1:\infty})$ is $p$-random (or $p$-typical, see &lt;a href=&#34;https://en.wikipedia.org/wiki/Algorithmically_random_sequence&#34;target=&#34;_blank&#34;&gt;wiki&lt;/a&gt;). $z_{1:\infty}$ is a shortest algorithmic compression of $f(z_{1:\infty})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Non-determinism&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;Our prediction of $x_n$ given $x_{&amp;lt;n}$ depends on the compressed representation of $x_{1:n}$, an input $z_{1:k}$ to $f$ s.t. $f(\Gamma_{z_{1:k}}) = \Gamma_{x_{1:n}}$. Our non-deterministic prediction of $x_{n}$ given $x_{&amp;lt;n}$ is the set $f^{-1}(\Gamma_{x_{1:n}})$, which is all compressions compatible with $x_{1:n}$. This is quantified with the probability $p(x_n \mid x_{&amp;lt;n}) = \lambda(f^{-1}(\Gamma_{x_{1:n}})) / \lambda(f^{-1}(\Gamma_{x_{&amp;lt;n}}))$ where $\lambda$ is the uniform probability measure on $\Z^\infty$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;use-cases&#34;&gt;Use Cases&lt;/h1&gt;
&lt;p&gt;This section runs through a few canonical examples of Bayesian inference which can be used as intuition pumps while reading the previous sections.&lt;/p&gt;
&lt;h2 id=&#34;inferring-bias-on-a-coin&#34;&gt;Inferring bias on a coin&lt;/h2&gt;
&lt;p&gt;Let $\X = \set{0,1}$ be the outcome of a coin toss, and $\o \in \X^\infty$ be an infinite sequence of coin toss outcomes.&lt;/p&gt;
&lt;p&gt;Let $\mc{B}_\t$ be the Bernoulli distribution on $\X$ with parameter $\t\in[0,1]$ which is the probability of $1$, so $\mc{B}_\t(1) = \t$ and $\mc{B}_\t(0) = 1-\t$.&lt;/p&gt;
&lt;p&gt;The hypothesis $\mu_\t$ is the product of Bernoulli distributions:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu_\t(x_{1:n}) = \prod_{i=1}^n\mc{B}_\t(x_i) = \t^{\sum_i x_i}(1-\t)^{n-\sum_i x_i}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The hypothesis set is $\H = \set{\mu_\t}_{\t\in[0,1]}$. These hypotheses are i.i.d. w.r.t. sequence position, i.e. $\mu_\t(x_n \mid x_{&amp;lt;n}) = \mu_\t(x_n)$.&lt;/p&gt;
&lt;p&gt;Subjective data probability:&lt;br&gt;
$$&lt;br&gt;
p(x_{1:n}) = \int_0^1 p(\t)\mu_\t(x_{1:n}) d\t = \int_0^1 p(\t)\t^{\sum_i x_i}(1-\t)^{n-\sum_i x_i} d\t&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Data posterior:&lt;br&gt;
$$&lt;br&gt;
p(x_n \mid x_{&amp;lt;n}) = \int_0^1 p(\t\mid x_{&amp;lt;n})\mu_\t(x_n \mid x_{&amp;lt;n})d\t = \int_0^1 p(\t\mid x_{&amp;lt;n})\mc{B}_\t(x_n)d\t&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Hypothesis posterior:&lt;br&gt;
$$&lt;br&gt;
p(\t\mid x_{1:n}) = p(\t)\frac{\mu_\t(x_{1:n})}{p(x_{1:n})}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Note that the hypotheses are i.i.d. w.r.t. data position but the subjective data distribution $p$ is not. That is to say, $\mu_\t(x_n \mid x_{&amp;lt;n}) = \mu_\t(x_n)$ but $p(x_n \mid x_{&amp;lt;n}) \neq p(x_n)$.&lt;/p&gt;
&lt;h2 id=&#34;solomonoff-induction&#34;&gt;Solomonoff Induction&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s continue the coin tossing example, but instead of coin tossing, suppose any arbitrary process that produces a binary data stream. That is to say, we are allowing dependencies between bits in the data sequence.&lt;/p&gt;
&lt;p&gt;Bernoulli hypotheses are blind to patterns in the ordering of bits in the data sequence because $\mu_\t(x_{1:n}) = \mu_\t(\mathrm{permute}(x_{1:n}))$ where $\mathrm{permute}(x_{1:n})$ is any permutation (re-ordering) of $x_{1:n}$. For example, we observe a very long sequence of alternating $0$s and $1$s, i.e. $0101010101\dots$, then the Bernoulli hypothesis where $\t=1/2$ will get large posterior weight. This is the hypothesis that the data is totally random (maximum entropy). Clearly, the data is highly patterned and predictable.&lt;/p&gt;
&lt;p&gt;What hypotheses should we add to $\H$? Is it possible to have a hypothesis for every possible data pattern? Ray Solomonoff&amp;rsquo;s answer is yes, and this is achieved by the hypothesis set containing all semicomputable semimeasures on $\X^\infty$.&lt;/p&gt;
&lt;p&gt;A probability measure $\mu$ on $\X^\infty$ is &lt;a href=&#34;https://en.wikipedia.org/wiki/Computable_function&#34;target=&#34;_blank&#34;&gt;&lt;strong&gt;computable&lt;/strong&gt;&lt;/a&gt; if there exists a program (Turing machine) which outputs the probability $\mu(x_{1:n})$ of any input, $x_{1:n} \in \X^n$ (for any $n\in\mb{N}$), to the requested precision, $\e &amp;gt; 0$, in finite time and then halts. Alternatively, a probability measure is computable if there exists a program that outputs each possible outcome $x_{1:n} \in \X^n$ (for any $n\in\mb{N}$) with probability $\mu(x_{1:n})$, given a stream of uniformly random input bits (i.i.d. Bernoulli(1/2) data).&lt;/p&gt;
&lt;p&gt;A probability measure $\mu$ on $\X^\infty$ is &lt;a href=&#34;https://en.wikipedia.org/wiki/Semicomputable_function&#34;target=&#34;_blank&#34;&gt;&lt;strong&gt;semicomputable&lt;/strong&gt;&lt;/a&gt; if there exists a program (Turing machine) which outputs an infinite monotonic (increasing or decreasing) sequence of rational numbers $(\hat{q}_i)_{i\in\mb{N}}$ s.t. $\hat{q}_i$ converges to $\mu(x_{1:n})$ as $i\to\infty$, for any $x_{1:n} \in \X^n$ (for any $n\in\mb{N}$). All computable measures are semicomputable. A semicomputable measure is not computable if the error $\e_i = \abs{\hat{q}_i - \mu(x_{1:n})}$ at position $i$ in the output sequence cannot be computably determined. If this error could be determined, this program can be converted into the one above that approximates $\mu(x_{1:n})$ to the desired error in finite time. In other words, a semicomputable measure can be approximated to arbitrary accuracy, but you may not be able to determine how close any given approximation is, whereas a computable measure can be arbitrarily approximated with known error.&lt;/p&gt;
&lt;p&gt;A semimeasure $\mu$ on $\X^\infty$ is like the measure we defined earlier in &lt;a href=&#34;#defining-a-hypothesis&#34;&gt;#Defining A Hypothesis&lt;/a&gt; except for one difference: Rather than strict equality we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x_{&amp;lt;n}) \geq \int_\X \mu(x_{&amp;lt;n}`\chi)d\chi\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;For $\X = \set{0,1}$, this property becomes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Measure: $\mu(x_{&amp;lt;n}) = \mu(x_{&amp;lt;n}`0) + \mu(x_{&amp;lt;n}`1)$&lt;/li&gt;
&lt;li&gt;Semimeasure: $\mu(x_{&amp;lt;n}) \geq \mu(x_{&amp;lt;n}`0) + \mu(x_{&amp;lt;n}`1)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that $0 \leq \mu(x_{&amp;lt;n}) \leq 1$ still holds. We just allow probabilities to sum to less than 1.&lt;/p&gt;
&lt;p&gt;The set of all computable measures is conceptually nicer to think about than (and is a subset of) the set of all semicomputable semimeasures, but is not practically useful because it cannot be enumerated by a program, i.e. is &lt;a href=&#34;https://en.wikipedia.org/wiki/Recursively_enumerable_set&#34;target=&#34;_blank&#34;&gt;&lt;strong&gt;computably enumerable&lt;/strong&gt;&lt;/a&gt; (c.e.). To enumerate this set, you&amp;rsquo;d have to determine which programs halt after emitting their approximation of $\mu(x_{1:n})$.&lt;/p&gt;
&lt;p&gt;The set of all semicomputable semimeasures can be enumerated by a program, by enumerating all programs in lexicographic order, running them all simultaneously (called &lt;a href=&#34;https://en.wikipedia.org/wiki/Dovetailing_%28computer_science%29&#34;target=&#34;_blank&#34;&gt;dovetailing&lt;/a&gt;, see also &lt;a href=&#34;https://en.wikipedia.org/wiki/Recursively_enumerable_set#Examples&#34;target=&#34;_blank&#34;&gt;c.e. set&lt;/a&gt;), and continually filtering out the programs whose output does not compute a valid semimeasure. The virtue of using semimeasures rather than measures is that we don&amp;rsquo;t need to wait for the programs being enumerated to halt. If one is found to violate the semimeasure property (probabilities sum to greater than 1) it is filtered out. Otherwise it remains and we don&amp;rsquo;t have to check that the probabilities it assigns to various outcomes sum to exactly 1 (which would require waiting for it to halt).&lt;/p&gt;
&lt;p&gt;Let hypothesis set $\H$ be the set of all semicomputable semimeasures. Then the subjective data distribution is also a semicomputable semimeasure:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x_{1:n}) = \sum_{\mu\in\H} p(\mu)\mu(x_{1:n})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Solomonoff makes a prescription on what prior to use:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(\mu) = 2^{-K(\mu)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $K(\mu)$ is the &lt;a href=&#34;http://www.scholarpedia.org/article/Algorithmic_complexity#Prefix_complexity&#34;target=&#34;_blank&#34;&gt;prefix-free Kolmogorov complexity&lt;/a&gt; of hypothesis $\mu\in\H$. This prior weights hypotheses inversely by complexity - a sort of Occam&amp;rsquo;s razor.&lt;/p&gt;
&lt;h2 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;We have a family of functions $\set{h_\t}_{\t\in\T}$ with signature $h_\t : \X \to \Phi$ (for example, $h_\t$ could be a &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network&#34;target=&#34;_blank&#34;&gt;neural network&lt;/a&gt;), where $\Phi$ is the parameter space for a probability distribution $\P$ on the space $\Y$. That is to say, $\P(y \mid h_\t(x))$ is the probability of $y\in\Y$ given $x\in\X$ according to the model $h_\t(x)$. We call $\X$ the input space, and $\Y$ the target (output) space.&lt;/p&gt;
&lt;p&gt;We observe a data sequence $x_1y_1x_2y_2x_3y_3\dots x_n$ ending in $x_n$. Let $D = x_1y_1x_2y_2x_3y_3\dots x_{n-1}y_{n-1}$ be the training data. $x_n$ is called the test input. The goal is to predict the next element in the sequence, $y_n$.&lt;/p&gt;
&lt;p&gt;We can take a Bayesian approach by defining the following hypothesis space:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\H = \set{\mu_\t \mid \t\in\T}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\mu_\t$ is a probability measure on $\X\times\Y\times\X\times\Y\dots = (\X\times\Y)^\infty$ s.t. for all $\o\in(\X\times\Y)^\infty$ and for all $(x_i,y_i)\in\o$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu_\t(y_i \mid x_i) = \P(y_i \mid h_\t(x_i))\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\mu_\t$ is i.i.d. w.r.t. sequence position, i.e. $\mu_\t(y_i \mid x_i) = \mu_\t(y_i \mid x_1y_1\dots x_i)$. That is to say, all of our hypotheses assume the data is drawn i.i.d. (and so the data sequence $D$ becomes a data&lt;em&gt;set&lt;/em&gt;, in the sense that it is unordered). If we are doing supervised learning, then the marginal probabilities $\mu_\t(x_i)$ should all be uniform (the hypothesis is indifferent to which input $x\in\X$ comes next in the sequence). On the other hand, if we are doing unsupervised or semi-supervised learning, then $\mu_\t(x_i)$ should also depend on the model output $h_\t(x_i)$.&lt;/p&gt;
&lt;p&gt;Finally, put a prior $p$ on the parameter space $\T$. Then the subjective probability of the test output $y_n$ given test input $x_n$ is the data posterior of $y_n$ given the data sequence $D`x_n = x_1y_1x_2y_2x_3y_3\dots x_n$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(y_n \mid D`x_ny_n) &amp;amp;= \int_\T p(\t\mid D`x_n)\mu_\t(y_n \mid D`x_n)d\t \\&lt;br&gt;
&amp;amp;= \int_\T p(\t\mid D`x_n)\mu_\t(y_n \mid x_n)d\t&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where the parameter posterior is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(\t\mid D`x_n) &amp;amp;= p(\t)\frac{\mu_\t(D`x_n)}{p(D`x_n)}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;In the supervised case $\mu_\t(x_1) = \mu_\t(x_2) = \dots = \mu_\t(x_n) = \gamma$, and so $\mu_\t(D`x_n) = \gamma\mu_\t(D)$.  Likewise, $p(D`x_n) = \int_\T p(\t)\mu_\t(D`x_n) d\t = \gamma\int_\T p(\t)\mu_\t(D) d\t$, and so the fraction $\mu_\t(D`x_n)/p(D`x_n)$ simplifies to $\mu_\t(D)/p(D)$. Then the parameter posterior reduces to the &lt;a href=&#34;http://mlg.eng.cam.ac.uk/zoubin/bayesian.html&#34;target=&#34;_blank&#34;&gt;classic form&lt;/a&gt;: $p(\t\mid D`x_n) = p(\t\mid D) = p(\t)p(D\mid \t)/p(D)$. where $p(D\mid \t) = \mu_\t(D)$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Classical vs Bayesian Reasoning</title>
      <link>https://danabo.github.io/blog/posts/classical-vs-bayesian-reasoning/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/classical-vs-bayesian-reasoning/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mc{E}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\Iff}{\Leftrightarrow}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;My goal is to identify the core conceptual difference between someone who accepts &amp;ldquo;Bayesian reasoning&amp;rdquo; as a valid way to obtain knowledge about the world, vs someone who does not accept Bayesian reasoning, but does accept &amp;ldquo;classical reasoning&amp;rdquo;. By classical reasoning, I am referring to the various forms of boolean logic that have been developed, starting with Aristotelian logic, through propositional logic like that of Frege, and culminating in formal mathematics (e.g. higher order type theory). In such logics, the goal is to uniquely determine the truth values of things (such as theorems and propositions) from givens.&lt;/p&gt;
&lt;p&gt;My thesis is that the difference between Bayesian and classical reasoners comes down to how they deal with non-determined objects (e.g. if you cannot determine the truth value of something from your givens). The classical reasoner will shrug their shoulders and say &amp;ldquo;the answer cannot be determined, collect more givens or modify your definitions&amp;rdquo;. The Bayesian reasoner will regard the proportion of self-consistent instantiations of unknowns that make the target proposition true as epistemologically salient. That is to say, the Bayesian reasoner continues on without uniquely determined truth values, and the classical reasoner does not.&lt;/p&gt;
&lt;p&gt;This difference extends beyond epistemology into the realm of machine learning. Classical epistemology is interested in truth (i.e. uniquely determined quantities), whereas Bayesian epistemology is interested in degrees of certainty. In machine learning, the givens and unknowns in question are not boolean valued, but have arbitrary data types (e.g. vectors of reals). The classical learner is interested in what numbers can be uniquely determined from data, and the Bayesian learner is interested in proportions of possibilities that result in each number.&lt;/p&gt;
&lt;p&gt;This philosophical difference leads to a practical methodological difference. A classical reasoner/learner will define universes such that unknowns can be uniquely determined. Otherwise, the definitions are not useful. A Bayesian reasoner/learner will define universes such that calculating posterior probabilities are tractable. Otherwise, the definitions are not useful.&lt;/p&gt;
&lt;p&gt;A note on the definition of &lt;strong&gt;model&lt;/strong&gt;. In mathematics, a model is an instantiation of an unspecified object which satisfies given axioms. In machine learning, a model refers to a family of instantiations of free parameters, i.e. a model is the set of definitions which invoke free variables that are to be determined.&lt;/p&gt;
&lt;h1 id=&#34;propositional-logic&#34;&gt;Propositional logic&lt;/h1&gt;
&lt;h2 id=&#34;the-math-perspective&#34;&gt;The math perspective&lt;/h2&gt;
&lt;p&gt;I will be using the example in Russell and Norvig&amp;rsquo;s &lt;a href=&#34;http://aima.cs.berkeley.edu/&#34;target=&#34;_blank&#34;&gt;Artificial Intelligence: A Modern Approach&lt;/a&gt; (3rd edition), chapter 7.&lt;/p&gt;
&lt;p&gt;We are introduced to &amp;ldquo;wumpus world&amp;rdquo;, a &lt;a href=&#34;http://gridworld.info&#34;target=&#34;_blank&#34;&gt;grid world&lt;/a&gt; containing an enemy called the wumpus, death pits, and gold.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223110223.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
The relevant rules are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the agent starts in the bottom left corner&lt;/li&gt;
&lt;li&gt;the agent can only see what is contained in the cell it currently occupies&lt;/li&gt;
&lt;li&gt;the agent will detect a &amp;ldquo;breeze&amp;rdquo; if it&amp;rsquo;s cell is adjacent to a pit&lt;/li&gt;
&lt;li&gt;if the agent moves into a pit, it dies&lt;/li&gt;
&lt;li&gt;the goal is to get to the gold&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose the agent moves right:&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223110534.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
No breeze is detected in the starting cell [1,1], so the agent knows there is no pit up or to the right. In cell [2,1], there is a breeze, so the agent knows there is a pit above or to the right (or both).&lt;/p&gt;
&lt;p&gt;Each case can be defined by the following propositions:&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210223110745.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210223110745.png&#34; width=&#34;300&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
We define the following boolean variables:&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210223110852.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210223110852.png&#34; 
         alt=&#34;$P_{x,y}$ is instantiated as a different variable for each coordinate, i.e. $P_{1,1}, P_{1,2}, P_{2,1},\ldots$ are all different variables.&#34; width=&#34;500&#34;/&gt;&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;caption&#34;&gt;
            &lt;p&gt;$P_{x,y}$ is instantiated as a different variable for each coordinate, i.e. $P_{1,1}, P_{1,2}, P_{2,1},\ldots$ are all different variables.&lt;/p&gt;
        &lt;/span&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;model&lt;/strong&gt; (in the math sense) is an instantiation of these variables (in contrast to the machine learning sense where a model is the entire definition of the game and all the variables). There are 4 variables for each coordinate, and 16*4 = 64 variables in total. Thus a model can be viewed as a length 64 boolean vector. Suppose $m$ is such a vector. Then if $\a_1$ is true for $m$, we say that $m$ is a model of $\a_1$.&lt;/p&gt;
&lt;p&gt;Let $M(\a)$ be the set of all models (i.e. length 64 boolean vectors) satisfying some arbitrary sentence $\a$. Let $\beta$ be another sentence. We say that $\a$ &lt;strong&gt;entails&lt;/strong&gt; $\beta$, notated $\a \models \beta$, iff $M(\a) \subseteq M(\beta)$.&lt;/p&gt;
&lt;p&gt;Diagrammatically, we can depict the sets $M(\a_1)$ and $M(\a_2)$ (referring to the sentences above), as well as our knowledge base (KB) (i.e. what is given):&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223113244.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
In our wumpus world, the &amp;ldquo;sentences&amp;rdquo; above can be formally states as propositions, along with the rules for the game:&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223111036.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
We can prove simple things like &amp;ldquo;there is no pit in [1,2]&amp;rdquo; using the rules of logical inference. The following propositions are true in all models where $R_1,\ldots,R_5$ are true:&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223112618.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
Each $R_i$ is entailed by the proceeding $R_j$ for $j &amp;lt; i$. A sequence of such propositions resulting in a desired proposition is a proof. From $R_{10}$, we can conclude $\neg P_{1,2}$. This sequence of propositions is a proof of $\a_1$ = &amp;ldquo;there is no pit in [1,2]&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We can also verify the statement $\a_1$ is true with brute force instead of logical inference: by enumerating all models ($2^{64}$ of them), selecting the models which satisfy $R_1,\ldots,R_5$ (i.e. $M(R_1\and\ldots\and R_5)$), and then checking if $\neg P_{1,2}$ is true in all of them.&lt;/p&gt;
&lt;p&gt;Notice that we cannot prove $P_{2,2}$ or its negation $\neg P_{2,2}$ given $R_1,\ldots,R_5$. That is because some models of $R_1\and\ldots\and R_5$ are consistent with $P_{2,2}$ while others are consistent with $\neg P_{2,2}$. That is to say, the truth value of variable $P_{2,2}$ is not uniquely determined by the givens $R_1,\ldots,R_5$. Speaking informally, we do not have enough information to know whether there is a pit at [2,2]. Therefore a rational agent would not make decisions based on $P_{2,2}$ being true or false. This is a core tenet of classical logic, whereas we shall see later, a Bayesian reasoner might be able to do more with the same information.&lt;/p&gt;
&lt;h2 id=&#34;the-machine-learning-perspective&#34;&gt;The machine learning perspective&lt;/h2&gt;
&lt;p&gt;In machine learning, a &lt;strong&gt;model&lt;/strong&gt; is a function $f : \O \to \X$, where $\O$ is called the state set, and $\X$ is called the observation set. This is different from the math notion of a model we saw above.&lt;/p&gt;
&lt;p&gt;Typically $\O$ and $\X$ are each the Cartesian products of other primitive types (which are sets, e.g. the reals, the integers, the booleans, etc.). Thus elements of $\O$ and $\X$ are typically tuples. Each element of the tuple is called a dimension. Generally, $\O$ and $\X$ are high-dimensional (elements of $\O$ and $\X$ are very long tuples, possibly infinite).&lt;/p&gt;
&lt;p&gt;An element $\o \in \O$ is called a state, and is considered to be a possible state of the world, where the world is the model. Often, $\O = \T\m\E$, where $\T$ is called the parameter set, and $\E$ is the noise set (both sets are themselves multi-dimensional). In the typical ML formulation, $\t\in\T$ is explicitly represented but $\e\in\E$ is not, because $\t$ is what is being &amp;ldquo;solved for&amp;rdquo; while $\e$ represents random inputs. In my formulation, I combine both into a single state $\o = (\t,\e)$. If $\o$ is some tuple, then a subtuple of $\o$ is called a substate, so $\t$ and $\e$ are substates of $\o = (\t, \e)$.&lt;/p&gt;
&lt;p&gt;An element $x \in \X$ is called an observation. Usually we are only given a partial observation. If $\X = T_1 \m T_2 \m T_3 \m \dots$ for primitive types $T_1, T_2, T_3, \ldots$, then a full observation is $x = (t_1, t_2, t_3, \ldots) \in \X$. A partial observation is a subset of elements in the tuple $x$. We denote partial observations with subscripts: $x_{1,5,10}$ is the tuple $(t_1, t_5, t_{10})$. We can also take slices: $x_{a:b} = (t_a, t_{a+1}, \ldots, t_{b-1}, t_b)$ is the slice from index $a$ to $b$. The shorthand $x_{&amp;gt;a}$ is the tuple of all indices larger than $a$, and $x_{&amp;lt;a}$ is the tuple of all indices less than $a$. It is sometimes convenient to think of a partial observation $x_I$ (for index tuple $I$) as a subset of $\X$, i.e. the set of all $x\in\X$ satisfying the partial observation. For example, $x_{1,5,10} = \set{(t_1, t_2, \ldots)\in\X \mid t_1 = x_1 \and t_5 = x_5 \and t_{10} = x_{10}}$. Let $x_{a:b}`x_{x:d} = x_{a:b,c:d}$ denote tuple concatenation.&lt;/p&gt;
&lt;p&gt;The goal of machine learning is to determine an unobserved partial observation  from an observed partial observation. If the unobserved part is going to be observed in the future, we call this prediction (if it happened in the past, we call this retrodiction). If the unobserved part is atemporal, or never observed, we call this inference. An unobservable partial observation is called latent.&lt;/p&gt;
&lt;h3 id=&#34;unsupervised-learning&#34;&gt;Unsupervised learning&lt;/h3&gt;
&lt;p&gt;The observation space is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\X = \Xi_1\m\Xi_2\m\dots\m\Xi_i\m\dots&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\x_i \in \Xi_i$ is called an &lt;em&gt;example&lt;/em&gt; (I am using &amp;ldquo;xi&amp;rdquo;, $\x$, instead of $x$ since $x$ already denotes a full observation).&lt;/p&gt;
&lt;p&gt;We are given a partial observation $D$, called the dataset:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
D = (\x_1, \x_2,\x_3, \ldots, \x_n)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Given model $f : \O \to \X$ and dataset $D$ (regarding $D$ as a subset of $\X$), then $\inv{f}(D)$ is the set of all states in $\O$ compatible with $D$.&lt;/p&gt;
&lt;p&gt;Typically in machine learning, the dataset $D$ does not uniquely determine a state $\o\in\O$. To further narrow down the possibilities, additional constraints are added. Typically, $\O = \T\m\E$, where the $\T$ component of the state tuple is narrowed down further by maximizing the data probability $p_\t(D)$ w.r.t. $\t\in\T$, and $\e \sim p_\t(D)$ is randomly chosen from the distribution. Additionally, the maximization of $p_\t(D)$ may be &amp;ldquo;regularized&amp;rdquo; by jointly minimizing some real-valued function $L(\t)$. Even then, $\t\in\T$ may not be uniquely determined (as is the case in deep learning), and so $\t$ will be arbitrarily chosen from the remaining possibilities.&lt;/p&gt;
&lt;p&gt;In the case of unsupervised learning, $f$ is called a generative model, and we use it to generate unobserved examples. Assume we&amp;rsquo;ve narrowed down the possibility space $\O$ to one state $\o^*$. We simply looking at&lt;/p&gt;
&lt;p&gt;$$f(\o^*) = (\x_1, \x_2, \ldots, \x_n, \x_{n+1}, \x_{n+2}, \dots)$$&lt;/p&gt;
&lt;p&gt;which provides $\x_{n+1}, \x_{n+2}, \dots$ outside of the partial observation $D$. Note that $\o$ typically contains a choice of noise $\e\in\E$, which injects randomness into the generated examples (generative models are normally thought of as probability distributions, and generating examples is a process of sampling from $p_{\t^*}(\x_i)$ for chosen parameter $\t^*$).&lt;/p&gt;
&lt;p&gt;Because $D$ does not uniquely determine an input $\o$ to $f$ (i.e. $\inv{f}(D)$ is not singleton), but a particular $\o^* \in \inv{f}(D)$ is chosen anyway, this results in some difficulties. Some $\o$ will produce generated examples $\x_{n+1}, \x_{n+2}, \dots$ which &amp;ldquo;look like the examples in $D$&amp;rdquo; where other choices of $\o$ (still compatible with $D$) will not. We say that $f(\o)$ generalizes if it outputs the unobserved partial observation that humans consider to be correct or appropriate (e.g. looks like the data in $D$). This is all very subjective, and it is very difficult to provide appropriate constraints (like the ones I mentioned above) so that for all possible $D$, the resulting $\o_D^*$ generalizes (i.e. many humans agree that $\x_{n+1}, \x_{n+2}, \dots$ &amp;ldquo;look like&amp;rdquo; $D$).&lt;/p&gt;
&lt;h3 id=&#34;supervised-learning&#34;&gt;Supervised learning&lt;/h3&gt;
&lt;p&gt;The observation space is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\X = \Xi_1\m\P_1\m\Xi_2\m\P_2\m\ldots\m\Xi_i\m\P_i\m\ldots&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\x_i \in \Xi_i$ is called an &lt;em&gt;input&lt;/em&gt;, and $\y_i\in\P_i$ is the associated &lt;em&gt;target&lt;/em&gt; (or &lt;em&gt;label&lt;/em&gt;) for $\x_i$. (usually the inputs and targets are notated with $x$ and $y$ - but I am using Greek)&lt;/p&gt;
&lt;p&gt;We are given a partial observation $D$, called the dataset:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
D = (\x_1, \y_1, \x_2, \y_2, \x_3, \y_3, \ldots, \x_n, \y_n)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Given model $f : \O \to \X$ and dataset $D$ (regarding $D$ as a subset of $\X$), then $\inv{f}(D)$ is the set of all states in $\O$ compatible with $D$.&lt;/p&gt;
&lt;p&gt;If we are subsequently given a &amp;ldquo;test input&amp;rdquo; $\x_{n+1} \in \Xi_{n+1}$, we want to predict $\y_{n+1} \in \P_{n+1}$. We can do so if $\inv{f}(D`\x_{n+1})$ uniquely determines $\y_{n+1}$, i.e. $D`\x_{n+1}`\y_{n+1} = f(\inv{f}(D`\x_{n+1}))$ (i.e. $\inv{f}(D`\x_{n+1}) = \inv{f}(D`\x_{n+1}`\y_{n+1})$)&lt;/p&gt;
&lt;h3 id=&#34;boolean-logic&#34;&gt;Boolean logic&lt;/h3&gt;
&lt;p&gt;The ML model is $f : \O\to\X$ where $\O = \B^{64}$ is the set of all length-64 boolean vectors, corresponding to the values of all the boolean variables $P_{x,y},W_{x,y},B_{x,y},S_{x,y}$ for all 16 grid cells. Any state $\o\in\O$ is a math model in the sense we used above. The observations are what is given, what we wish to know, and potentially everything we can know in principle. We were given the follow propositions (meaning they are observed as true):&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
R_1 &amp;amp;= \neg P_{1,1} \\&lt;br&gt;
R_2 &amp;amp;= (B_{1,1} \Iff (P_{1,2}\or P_{2,1})) \\&lt;br&gt;
R_3 &amp;amp;= (B_{2,1} \Iff (P_{1,1}\or P_{2,2}\or P_{3,1})) \\&lt;br&gt;
R_4 &amp;amp;= \neg B_{1,1} \\&lt;br&gt;
R_5 &amp;amp;= B_{2,1}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We want to know many things: the location of the gold, the wumpus, and all the pits. Under more immediate consideration is whether there are pits in any of the cells [2,1], [2,2], [3,1]. So let&amp;rsquo;s say the propositions of immediate interest are&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
Q_1 &amp;amp;= P_{2,1} \\&lt;br&gt;
Q_2 &amp;amp;= P_{2,2} \\&lt;br&gt;
Q_3 &amp;amp;= P_{3,1} \\&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then the observation set is $\X = \B^8$, and a full observation is the boolean tuple $x = (R_1, R_2, R_3, R_4, R_5, Q_1, Q_2, Q_3)$. (Note that one ML-model&amp;rsquo;s full observation is another ML-model&amp;rsquo;s partial observation. If later on we care about propositions about other cells on the board, we can just augment $\X$ with additional dimensions, thereby updating $f$ to $f&#39;$) We are given the partial observation $x_{1:5} = (\1, \ldots, \1)$, i.e. $R_1=\1,\ \dots,\ R_5=\1$. Can we uniquely determine the remaining parts of $x$?&lt;/p&gt;
&lt;p&gt;The set of math models (subset of $\O$) consistent with $x_{1:5}$ being true is:&lt;br&gt;
$$&lt;br&gt;
\inv{f}(x_{1:5}) = \inv{f}(\set{(\1, \dots, \1)}\m \B^3)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Above we proved that $P_{2,1} = \1$, and so $x_6 = Q_1 = \1$ must be the case for all states in $\inv{f}(x_{1:5})$. However, $x_{7:8} = (Q_2, Q_3) = (P_{2,2}, P_{3,1})$ is not uniquely determined in $\inv{f}(x_{1:5})$.&lt;/p&gt;
&lt;p&gt;Note that&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
R_2 &amp;amp;= (B_{1,1} \Iff (P_{1,2}\or P_{2,1})) \\&lt;br&gt;
R_3 &amp;amp;= (B_{2,1} \Iff (P_{1,1}\or P_{2,2}\or P_{3,1}))&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;are the rules of the game, i.e. that a breeze must be adjacent to a pit. In the ML framework, including the rules as partial observations means that the ML model $f$ considers possible worlds where the rules of the game are different - in fact $f$ allows for all possible games played on a 4x4 grid with pits, breezes, etc. It is equivalent to bake the rules of the game into $f$ as a domain restriction on $\O$, i.e. define $f&#39;$ whose domain is all states which obey our rules $R_2, R_3$.&lt;/p&gt;
&lt;p&gt;In the math framework, a theorem is a proposition that is true in all math models. In the ML framework, a theorem is a boolean dimension of $\X$ (output to $f$) which is true for all states (inputs to $f$). For a given boolean output dimension of $f$, that dimension becomes a theorem of $f&#39;$, the domain restriction of $f$ to all inputs where the output dimension is true (if there are no such inputs then this boolean dimension corresponds a paradox, a proposition that is not true in any math model).&lt;/p&gt;
&lt;p&gt;The general case is $f : \B^a \to \B^b$ where $a,b$ may be finite or infinite cardinalities. Given a partial observation $x_{1:n}$, we can ask whether any dimensions of $x_{&amp;gt;n}$ are also uniquely determined. A proof of $x_i = \1$ is a way of showing that $x_i$ is uniquely determined from $x_{1:n}$ without brute force enumeration of all states (inputs to $f$). If we care about part $\o_{1:m}$ of the input to $f$ as well, then in the ML framework, that equates to passing $\o_{1:m}$ through $f$ to $x_{j_1, \dots, j_m}$ as the identity function, and then trying to uniquely determine that partial observation.&lt;/p&gt;
&lt;h2 id=&#34;the-bayesian-perspective&#34;&gt;The Bayesian perspective&lt;/h2&gt;
&lt;p&gt;Continuing with our ML model $f : \O \to \X$ for wumpus world, what should the agent do next? Using non-Bayesian logic, we could not uniquely determine if cells [3,1] or [2,2] contain pits or not. It then seems most reasonable to travel to cell [1,2] to gather more information. Let&amp;rsquo;s suppose we do that, and find [1,2] also contains a breeze (I&amp;rsquo;m disregarding what is depicted in figure 7.2 and making up my own scenario). Then any of cells [1,3], [2,2], and [3,1] may contain pits. There is nowhere else to go without traveling over one of these dangerous cells. What do we do?&lt;/p&gt;
&lt;p&gt;A Bayesian would suggest counting up and comparing the number of states for which each possible observation is true. Suppose we know there are exactly 3 pits on the board. We&amp;rsquo;ve ruled out [1,1], [1,2], [2,1], and so there are 13 remaining cells that contain the 3 pits.&lt;/p&gt;
&lt;p&gt;State of knowledge:&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210223212901.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210223212901.png&#34; width=&#34;200&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
Possible pit configurations of [3,1], [2,2] and [3,1] (1 = pit, 0 = no pit):&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210223213359.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210223213359.png&#34; width=&#34;300&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
There are 10 remaining cells not depicted. Any pits not in [3,1], [2,2] and [3,1] will be in the remaining 10. Let&amp;rsquo;s count up the total number of states corresponding to each configuration:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;[1,3]&lt;/th&gt;
&lt;th&gt;[2,2]&lt;/th&gt;
&lt;th&gt;[3,1]&lt;/th&gt;
&lt;th&gt;Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;${10 \choose 2} = 45$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;${10 \choose 1} = 10$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;${10 \choose 1} = 10$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;${10 \choose 1} = 10$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;${10 \choose 0} = 1$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The total count is 76 states. As fractions (called probabilities), we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(0,1,0) &amp;amp;= 45/76 \\&lt;br&gt;
p(1,0,1) &amp;amp;= 10/76 \\&lt;br&gt;
p(1,1,0) &amp;amp;= 10/76 \\&lt;br&gt;
p(0,1,1) &amp;amp;= 10/76 \\&lt;br&gt;
p(1,1,1) &amp;amp;= 1/76&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The fraction of states where the middle cell [2,2] has a pit is $66/76$. The fraction of states where [3,1] has a pit is $21/76$, and likewise for [1,3]. We say that [2,2] is more likely to have a pit than [1,3] or [3,1]. If the agent is forced to choose one of the three cells to move to, [2,2] is the most dangerous option (highest probability of falling into a pit) and [3,1],[3,1] are equally less-dangerous.&lt;/p&gt;
&lt;p&gt;The difference between the classical and Bayesian paradigms is now clear. A classical agent does not distinguish between these three options, since none of these cells can be proved to be pit-free (this property is not uniquely determined from the givens). The Bayesian agent doesn&amp;rsquo;t need unique determination to have knowledge about the pit-ness of these cells, and concludes that [2,2] is more likely to have a pit than [3,1] or [1,3].&lt;/p&gt;
&lt;h2 id=&#34;probability-notation&#34;&gt;Probability notation&lt;/h2&gt;
&lt;p&gt;$\newcommand{\obs}{\mathrm{Data}}$I&amp;rsquo;m now going to regard logical propositions as functions of state $\o$. For example, $R_1(\o)$ returns true if $\o$ is a math-model of proposition $R_1$, and false otherwise.&lt;/p&gt;
&lt;p&gt;Let $\obs = R_1\and\dots\and R_5$. Then $\obs(\o)$ is true iff $\o$ is a math-model of our givens $R_1$ through $R_5$. If I write $P_{x,y}$, take that now to be a function of $\o$ as well.&lt;/p&gt;
&lt;p&gt;Let $p$ be the uniform probability measure on $\O$. Using random variable notation (see &lt;a href=&#34;http://zhat.io/articles/primer-probability-theory#random-variables&#34;target=&#34;_blank&#34;&gt;zhat&lt;/a&gt; for probability theory and &lt;a href=&#34;https://www.lesswrong.com/posts/W8YscokXMiDnLKJ96/bayesian-inference-on-1st-order-logic&#34;target=&#34;_blank&#34;&gt;LW&lt;/a&gt; for conditional probability notation) the fraction of states where the middle cell [2,2] has a pit is $p(P_{2,2}=\1\mid\obs=\1) = 66/76$. The fraction of states where [3,1] and [1,3] have a pit respectively is $p(P_{1,3} = \1\mid\obs=\1) = p(P_{3,1} = \1\mid\obs=\1) = 21/76$.&lt;/p&gt;
&lt;p&gt;In general for finite state sets $\O$, any ML-model $f : \O\to\X$ can be regarded as a random variable (or a tuple of random variables), i.e. a function from states to observables. I always assume a uniform probability measure, which corresponds to naive counting like in the example above. Let $f_{a:b}$ denote the slice of the tuple-valued random variable $f$ from index $a$ to $b$. Generally we want to determine the probability of some (unobserved) partial observation $x_{n:m}$ given the (observed) partial observation $x^*_{1:n}$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; p(f_{n:m} = x_{n:m} \mid f_{1:n} = x^*_{1:n}) \\&lt;br&gt;
&amp;amp;\quad= p\set{\o\in\O \mid f(\o) \in x^*_{1:n}`x_{n:m}} / p\set{\o\in\O \mid f(\o) \in x^*_{1:n}} \\&lt;br&gt;
&amp;amp;\quad= p(\inv{f}(x^*_{1:n}`x_{n:m})) / p(\inv{f}(x^*_{1:n})) \\&lt;br&gt;
&amp;amp;\quad= \abs{\inv{f}(x^*_{1:n}`x_{n:m})} / \abs{\inv{f}(x^*_{1:n})}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;In general, the classically rational agent only regards uniquely determined partial observables (output dimensions on it&amp;rsquo;s ML-model $f$) as knowledge, and does not act on undetermined partial observables. In contrast, the Bayesian rational agent takes the relative proportion of possible states that produce each outcome as knowledge.&lt;/p&gt;
&lt;p&gt;How does the Bayesian agent pull this off? Do these &amp;ldquo;Bayesian&amp;rdquo; probabilities really constitute knowledge? We can turn the question around and ask if uniquely determined observables really constitute knowledge. It is rare for something to be uniquely determined in practice. I suspect that many of the difficulties encountered in applications of statistical inference and machine learning are because of this. A classical reasoner needs to make simplifications and assumptions in service of being able to then uniquely determine something of interest. It seems to me that most of informal rational thought comes down to some version of choosing an ML-model s.t. something of interest can be uniquely determined. Described in this way, classical reasoning sounds biased. On the flip side, in practice Bayesian ML-models requires special simplifications and assumptions to be computationally tractable, so a similar sort of ML-model-choosing bias occurs.&lt;/p&gt;
&lt;p&gt;Note that the size of $\O$ and the ML-model $f$ determine how many states produce each partial observation. Ostensibly these things are arbitrarily chosen by the agent. The classical reasoner objects that state-counts (probabilities) don&amp;rsquo;t constitute actual knowledge about the world, but are an artifact of the choice of ML-model $f$, and so it is inappropriate to treat these quantities as knowledge. The Bayesian reasoner counters that a classical ML-model (e.g. boolean logic) is also arbitrarily chosen. Unique determination is a property of $f$, not reality, and depends on the arbitrary simplifications and assumptions made by the agent. Thus, the Bayesian reasoner concludes, my approach is no less rational than yours.&lt;/p&gt;
&lt;p&gt;The classical reasoner would counter that the parts of the ML-model output which are observed can be arbitrarily inspected for &amp;ldquo;goodness of fit&amp;rdquo; to reality. Unique determination is much less fragile (i.e. stable w.r.t. modeling inaccuracies) than state-counts. Unique determination is robust against worst-case modeling errors (though in practice this is clearly not true).&lt;/p&gt;
&lt;p&gt;Punchline: any representation of the world whose predictions are not uniquely determined by data on hand can be viewed as Bayesian, if we are willing to provide a measure for calculating sizes of sets of model states, and then to use size-of-possibilities as decision making criteria.&lt;/p&gt;
&lt;h1 id=&#34;the-bayesian-axiom&#34;&gt;The Bayesian Axiom&lt;/h1&gt;
&lt;p&gt;This epistemological debate is still raging. The efficacy of classical reasoning has been argued about for the last two and a half millennia. Bayesian reasoning is a more modern invention that, depending on how you count it, has been going on for 100 to 300 years (early 20th century Bayesians to Thomas Bayes). The frontier of contemporary inquery is the complex: from brains to human societies to high-dimensional physical systems, the neat and orderly unique-determination of classical reasoning is hard to come by. For this reason, Bayesian reasoning is gaining traction and is posturing to topple classical reasoning as the common-sense epistemological default.&lt;/p&gt;
&lt;p&gt;Given the unsettled nature of these questions, it is my opinion that the &amp;ldquo;state-counts as knowledge&amp;rdquo; premise be taken as the &amp;ldquo;Bayesian axiom&amp;rdquo;. This neatly delineates classical and Bayesian epistemology down to one difference: Bayesian epistemology is classical epistemology plus an additional axiom. Some may accept this axiom and other&amp;rsquo;s may reject it, leading to different kinds of reasoning and knowledge. I don&amp;rsquo;t know if we will ever be able to determine that this axiom should or should not be used. In that case, like &lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_postulate&#34;target=&#34;_blank&#34;&gt;Euclid&amp;rsquo;s fifth axiom&lt;/a&gt;, &amp;ldquo;flat&amp;rdquo; and &amp;ldquo;curved&amp;rdquo; rationality shall forever remain parallel self-consistent options.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variational Solomonoff Induction</title>
      <link>https://danabo.github.io/blog/posts/variational-solomonoff-induction/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/variational-solomonoff-induction/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\kl}[2]{D_{KL}\left(#1\ \| \ #2\right)}&lt;br&gt;
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\ }&lt;br&gt;
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}}\ }&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\ve}{\varepsilon}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\sm}{\mathrm{softmax}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Free_energy_principle&#34;target=&#34;_blank&#34;&gt;free energy principle&lt;/a&gt; is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Variational_Bayesian_methods&#34;target=&#34;_blank&#34;&gt;variational Bayesian method&lt;/a&gt; for approximating posteriors. Can free energy minimization combined with program synthesis methods from machine learning tractably approximate &lt;a href=&#34;https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference&#34;target=&#34;_blank&#34;&gt;Solomonoff induction&lt;/a&gt; (i.e. universal inference)? In these notes, I explore what the combination of these ideas looks like.&lt;/p&gt;
&lt;h1 id=&#34;machine-learning&#34;&gt;Machine learning&lt;/h1&gt;
&lt;p&gt;I want to make an important clarification about &amp;ldquo;Bayesian machine learning&amp;rdquo;. First, I&amp;rsquo;ll briefly define some &amp;ldquo;modes&amp;rdquo; of machine learning.&lt;/p&gt;
&lt;p&gt;In parametric machine learning, we have a function $f_\t$ parametrized by $\t\in\T$. Let $q_\t(D)$ be a probability distribution on datasets $D$ defined in terms of $f_\t$. For supervised learning, $q_\t(D) = \prod_{(x,y)\in D} Pr(y; f_\t(x))$ is the product of probabilities of each target $y$ given distribution parameters $f_\t(x)$, e.g. $f_\t(x)$ returns the mean and variance of a Gaussian over $y$. For unsupervised learning, $f_\t(x)$ might return a real number which serves as the log-probability of each $x \in D$. In general $f_\t$ can be any kind of parametric ML model, but these days it is likely to be a neural network.&lt;/p&gt;
&lt;p&gt;Typical usage &amp;ldquo;modes&amp;rdquo; of $q_\t$ in machine learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MLE&lt;/strong&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;target=&#34;_blank&#34;&gt;maximum likelihood&lt;/a&gt;): Training produces hypothesis with highest data likelihood.
&lt;ul&gt;
&lt;li&gt;$\t\in\T$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$\t^* = \argmax{\t}\log q_\t(D)$ for dataset $D$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MAP&lt;/strong&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation&#34;target=&#34;_blank&#34;&gt;maximum a posteriori&lt;/a&gt;): Training produces model with highest posterior probability.
&lt;ul&gt;
&lt;li&gt;$\t\in\T$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$\t^* = \argmax{\t}\left[\log q_\t(D) + \log p(\t)\right]$ for dataset $D$ and prior $p(\t)$.&lt;/li&gt;
&lt;li&gt;$\log p(\t)$ can be viewed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Regularization_%28mathematics%29&#34;target=&#34;_blank&#34;&gt;regularizer&lt;/a&gt;. MAP is just MLE plus regularization - the most typical form of parametric machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bayesian Inference&lt;/strong&gt;: Prior over the parameter induces a posterior over parameters given data.
&lt;ul&gt;
&lt;li&gt;$\t\in\T$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$p(\t \mid D) = \frac{q_\t(D) p(\t)}{\int_\T q_\t(D) p(\t) d\t}$ is the exact posterior on hypotheses $\T$ given dataset $D$.&lt;/li&gt;
&lt;li&gt;Unlike in MLE and MAP, there is no notion of optimal parameter $\t^*$. Instead we have much more information: $p(\t \mid D)$ &amp;ldquo;scores&amp;rdquo; every parameter in $\T$, and all the scores taken together constitute our information.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variational Bayes&lt;/strong&gt; (free energy minimization): training produces a (approximate) posterior distribution over hypotheses.
&lt;ul&gt;
&lt;li&gt;$z \in \mc{Z}$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$\tilde{\t} = \argmin{\t}\kl{q_\t(z)}{p(z \mid D)}$ is the target parameter for dataset $D$ and model $p(D,z)$. This is assumed to be intractable to find.&lt;/li&gt;
&lt;li&gt;$\t^* = \argmin{\t}\kl{q_\t(z)}{p(z)} - \E_{z\sim f_\t(z)}\left[\lg p(D \mid z)\right]$ is the approximation. This is what we find through optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The variational Bayes &amp;ldquo;usage mode&amp;rdquo; is clearly different from the others. MLE and MAP are fitting $f_\t$ to the data, i.e. finding a single $\t^*\in\T$ that maximizes the probability of the data under $q_\t$. Bayesian inference is finding a distribution $p(\t \mid D)$ on $\T$ which represents the model&amp;rsquo;s beliefs about various parameters $\t\in\T$ being likely or unlikely as explanations of the data. This is not the same as fitting $f_\t$ to data, since we are not choosing any particular parameter in $\T$. Variational Bayes uses $q_{\t^*}$ as an approximation of $p(\t \mid D)$, where $\t^*\in\T$ is the optimal parameter of distribution $q_\t(z)$ and $z\in\mc{Z}$ is a hypothesis.&lt;/p&gt;
&lt;p&gt;In the first three modes, $\T$ are hypotheses and we are either selecting one or finding a distribution over them. In the variational Bayes mode, $\T$ are not hypotheses. Instead we introduce $\mc{Z}$ as the hypothesis space and $\T$ is the parameter space for the approximate posterior $q_\t(z)$ on $\mc{Z}$, i.e. $q_\t(z)$ approximates $p(z\mid D)$. We don&amp;rsquo;t have $\mc{Z}$ in the first three modes, and we are interested in $p(\t \mid D)$ rather than $p(z \mid D)$. Also in the first three modes, $q_\t(D)$ is a distribution on what is observed, datasets $D$, rather than over latent $\mc{Z}$.&lt;/p&gt;
&lt;h2 id=&#34;what-is-bayesian-machine-learning&#34;&gt;What is Bayesian machine learning?&lt;/h2&gt;
&lt;p&gt;Conventionally, a Bayesian model has a prior probability distribution over it&amp;rsquo;s parameters, and inference involves finding posterior distributions. This corresponds to the Bayesian inference mode above. Out of the four modes, MLE is definitively non-Bayesian. MAP might be called semi-Bayesian, simply because there is a prior on parameters $p(\t)$, but only the argmax of the posterior is being found, rather than a full posterior. The variational Bayes mode is where things get wonky. There are two models: $q_\t(z)$ and $p(z, D)$. The first is parametrized and is optimized greedily with something like gradient descent, as in the MLE or MAP cases. The second is Bayesian.&lt;/p&gt;
&lt;p&gt;Is variational Bayes a Bayesian ML method? In one sense yes, in another sense no. It&amp;rsquo;s efficacy depends on $q_\t(z)$ being a good approximation of the posterior $p(z \mid D)$, and whether $q_\t(z)$ is a good approximation depends on the efficacy of the chosen machine learning method (e.g. neural networks trained with gradient descent). I&amp;rsquo;d expect $q_\t(z)$ to be a non-Bayesian model (If it were Bayesian, how then do you tractably approximate it? That is the very thing we are trying to do with $p(z, D)$.) So then the efficacy of variational Bayes comes down to the properties of non-Bayesian machine learning. If at the end of the day, point-estimates of parameters are always doing the heavy lifting (i.e. generalizing well), why be Bayesian in the first place?&lt;/p&gt;
&lt;h1 id=&#34;solomonoff-induction&#34;&gt;Solomonoff induction&lt;/h1&gt;
&lt;p&gt;I learned about this topic from &lt;a href=&#34;https://www.springer.com/gp/book/9781489984456&#34;target=&#34;_blank&#34;&gt;An Introduction to Kolmogorov Complexity and Its Applications&lt;/a&gt; and &lt;a href=&#34;http://www.hutter1.net/ai/uaibook.htm&#34;target=&#34;_blank&#34;&gt;Universal Artificial Intelligence&lt;/a&gt;. I recommend both books as references.&lt;/p&gt;
&lt;p&gt;There are different formulations of Solomonoff induction, each utilizing a hypothesis space containing all programs - but different kinds of programs for each formulation. I outline three of them: &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt;, &lt;a href=&#34;#version-2&#34;&gt;#Version 2&lt;/a&gt;, &lt;a href=&#34;#version-3&#34;&gt;#Version 3&lt;/a&gt;. Only an understanding of &lt;a href=&#34;#version-2&#34;&gt;#Version 2&lt;/a&gt; is needed for the subsequent sections.&lt;/p&gt;
&lt;h2 id=&#34;notation&#34;&gt;Notation&lt;/h2&gt;
&lt;p&gt;Let $\B = \{0,1\}$ be the binary alphabet, $\B^n$ be the set of all length-$n$ binary strings, and $\B^\infty$ be the set of all infinite binary strings.&lt;/p&gt;
&lt;p&gt;Let $\B^* = \B^0 \times \B^1 \times \B^2 \times \B^3 \times\ldots$ be the set of all finite binary strings of any length.&lt;br&gt;
$\epsilon$ is the empty string, i.e. $\B^0 = \{\epsilon\}$.&lt;/p&gt;
&lt;p&gt;Let $x \sqsubseteq y$ denote that binary string $x$ is a prefix of (or equal to) binary string $y$.&lt;br&gt;
Let $x`y$ denote the string concatenation of $x$ and $y$.&lt;br&gt;
Let $x_{a:b}$ denote the slice of $x$ from position $a$ to $b$ (inclusive).&lt;br&gt;
Let $x_{&amp;lt;n} = x_{1:n-1}$ denote the prefix of $x$ up to $n-1$.&lt;br&gt;
Let $x_{&amp;gt;n} = x_{n+1:\infty}$ denote the &amp;ldquo;tail&amp;rdquo; of $x$ starting from $n+1$.&lt;/p&gt;
&lt;h2 id=&#34;version-1&#34;&gt;Version 1&lt;/h2&gt;
&lt;p&gt;Let $U$ be a universal machine, i.e. if $z\in\B^*$ is a program then $U(z) \in \B^*$ is some binary string, or $U(z)$ is undefined because $U$ does not halt on $z$. We do not give program $z$ input, but $z$ can include &amp;ldquo;data&amp;rdquo;, in the sense that it&amp;rsquo;s program specifies a binary string that gets loaded into memory when the program starts.&lt;/p&gt;
&lt;p&gt;Let $p(z)$  be a prior on finite binary strings.&lt;br&gt;
Then for observation $x \in \B^*$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p(x \mid z) = \begin{cases}1 &amp;amp; x \sqsubseteq U(z) \\ 0 &amp;amp; \mathrm{otherwise}\end{cases}$&lt;/li&gt;
&lt;li&gt;$p(z, x) = p(x \mid z)p(z) = \begin{cases}p(z) &amp;amp; x \sqsubseteq U(z) \\ 0 &amp;amp; \mathrm{otherwise}\end{cases}$&lt;/li&gt;
&lt;li&gt;$p(x) = \sum_{z\in\B^*} p(x,z) = \sum_{z \in \B^*;\ x \sqsubseteq U(z)} p(z)$&lt;/li&gt;
&lt;li&gt;$p(z \mid x) = p(z, x)/p(x) = \begin{cases}p(z)/p(x) &amp;amp; x \sqsubseteq U(z) \\ 0 &amp;amp; \mathrm{otherwise}\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$p(x)$ is the data probability.&lt;br&gt;
$p(z)$ is the prior.&lt;br&gt;
$p(z\mid x)$ is the posterior.&lt;/p&gt;
&lt;p&gt;If we observe $x \in \B^*$, we may ask for the probability of subsequently observing some $y\in\B^*$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(y \mid x) &amp;amp;= \frac{1}{p(x)}\sum_{z\in\B^*}p(x`y,z) \\&lt;br&gt;
&amp;amp;= \frac{1}{p(x)}\sum_{z \in \B^*;\ x`y \sqsubseteq U(z)} p(z) \\&lt;br&gt;
&amp;amp;= \sum_{z \in \B^*;\ x`y \sqsubseteq U(z)} p(z \mid x)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;What prior $p(z)$ should we choose? Solomonoff recommends&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(z) = 2^{-\ell(z)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\ell(z)$ is the length of program $z$. This has the effect of putting more prior probability on short programs, essentially encoding a preference for &amp;ldquo;simple&amp;rdquo; explanations of the data. This prior also has the benefit that it is fast to calculate $p(z)$ for any $z$. In general, choice of prior is a matter of taste, and should depend on practical considerations like tractability and regularizations such as &amp;ldquo;simplicity&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;version-2&#34;&gt;Version 2&lt;/h2&gt;
&lt;p&gt;Let $\mu$ be a probability measure on $\B^\infty$, meaning $\mu$ maps (measurable) subsets of $\B^\infty$ to probabilities. $\mu$ can be specified by defining it&amp;rsquo;s value on the &lt;strong&gt;cylinder sets&lt;/strong&gt; $\Gamma_x = \set{\o \in \B^\infty \mid x \sqsubset \o}$ for every $x\in\B^*$, i.e. the set of all infinite binary strings starting with $x$. I&amp;rsquo;ll let $\mu(x)$ be a shorthand denoting $\mu(\Gamma_x)$. Then $\mu(x)$ is the probability of finite string $x$. For any such measure $\mu$, it must be the case that&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x) = \mu(x`0) + \mu(x`1)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x) \geq \mu(x`y)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $x,y\in\B^*$.&lt;/p&gt;
&lt;p&gt;$\mu$ is a &lt;strong&gt;semimeasure&lt;/strong&gt; iff it satisfies&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x) \geq \mu(x`0) + \mu(x`1)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $x \in \B^*$. That is to say, if $\mu$ is a semimeasure then probabilities may sum to less than one (this is like supposing that some probability goes missing).&lt;/p&gt;
&lt;p&gt;The following are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mu$ is &lt;strong&gt;computable&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;There exists some program which computes the probability $\mu(x)$ for all inputs $x$.&lt;/li&gt;
&lt;li&gt;There exists some program which outputs $x$ with probability $\mu(x)$ (for all $x$) when given uniform random input bits.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\mu$ is &lt;strong&gt;semicomputable&lt;/strong&gt; (a.k.a. enumerable) if there exists some program which approximates the probability $\mu(x)$ (for all $x$) by outputting a sequence of rational numbers $\set{\hat{p}_n}$ approaching $\mu(x)$, but where it is impossible to determine how close the sequence is to $\mu(x)$ at any point in time. That is to say, you cannot know the error sequence $\varepsilon_n = \abs{\mu(x) - \hat{p}_n}$, but you know that $\varepsilon_n \to 0$ as $n\to\infty$. In contrast, if $\mu$ is computable then there exists a program that outputs both the sequence of rationals $\set{\hat{p}_n}$ AND the errors $\set{\varepsilon_n}$ (computability implies there exists a program that takes a desired error $\varepsilon&amp;gt;0$ as input and outputs in finite time (i.e. halts) the corresponding approximation $\hat{p}$ s.t. $\varepsilon&amp;gt;\abs{\mu(x)-\hat{p}}$).&lt;/p&gt;
&lt;p&gt;Let $\mc{M}$ be the set of all semicomputable semimeasures on infinite binary sequences. Let $p(\mu)$ be a prior on $\mc{M}$.&lt;/p&gt;
&lt;p&gt;Then for observation $x \in \B^*$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p(x \mid \mu) = \mu(x)$&lt;/li&gt;
&lt;li&gt;$p(x, \mu) = p(x \mid \mu)p(\mu) = p(\mu)\mu(x)$&lt;/li&gt;
&lt;li&gt;$p(x) = \sum_{\mu \in \mc{M}} p(x \mid \mu) = \sum_{\mu \in \mc{M}} p(\mu)\mu(x)$&lt;/li&gt;
&lt;li&gt;$p(\mu \mid x) = p(\mu)\frac{\mu(x)}{p(x)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$p(x)$ is the data probability.&lt;br&gt;
$p(\mu)$ is the prior.&lt;br&gt;
$p(\mu\mid x)$ is the posterior.&lt;/p&gt;
&lt;p&gt;If we observe $x \in \B^*$, we may ask for the probability of subsequently observing some $y\in\B^*$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(y \mid x) &amp;amp;= \sum_{\mu \in \mc{M}} p(y,\mu\mid x) \\&lt;br&gt;
&amp;amp;= \sum_{\mu \in \mc{M}} p(\mu\mid x)\mu(y \mid x) \\&lt;br&gt;
&amp;amp;= \sum_{\mu \in \mc{M}}p(\mu)\frac{\mu(x)}{p(x)}\mu(y\mid x)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\mc{M}$ is all semicomputable semimeasures, rather than all computable measures, because the former is a decidable set while the latter is not, i.e. in practice the former set of hypotheses can be feasibly enumerated by enumerating all programs, while the latter cannot be. If we required only measures, then we could not decide which programs produced proper measures (if the program doesn&amp;rsquo;t halt on $x$, that is like saying the probability that would have gone to strings starting with $x$ &amp;ldquo;disappears&amp;rdquo;). Allowing non-halting programs means we don&amp;rsquo;t have to filter out programs which don&amp;rsquo;t halt. Similar issue for computable vs semicomputable.&lt;/p&gt;
&lt;p&gt;Versions 1 and 2 are equivalent. That is to say, we can get the same data distribution $p(x)$ for the right choice of prior in each version.&lt;/p&gt;
&lt;p&gt;A typical choice of the prior in this version is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(\mu) = 2^{-K(\mu)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $K(\mu)$ is the &lt;a href=&#34;https://www.math.wisc.edu/~jmiller/Notes/contrasting.pdf&#34;target=&#34;_blank&#34;&gt;&lt;strong&gt;prefix-free Kolmogorov complexity&lt;/strong&gt;&lt;/a&gt; of $\mu$, i.e. the length of the shortest program that (semi)computes $\mu$ (given a space of prefix-free programs, i.e. program strings contain their own length information).&lt;/p&gt;
&lt;h2 id=&#34;version-3&#34;&gt;Version 3&lt;/h2&gt;
&lt;p&gt;As you might guess, this will also turn out to be (sorta) equivalent to the first two versions. This is like version 2, except instead of considering a hypothesis to be a program that samples data sequences given uniform random input bits, a hypothesis is such a program packaged together with a particular infinite input sequence. Thus, hypotheses are in a sense infinite programs.&lt;/p&gt;
&lt;p&gt;Let $U$ be a &lt;strong&gt;universal monotone machine&lt;/strong&gt;. That means $U$ can execute infinitely long programs in a streaming fashion by producing partial outputs as the program is read. Let $z \in \B^\infty$. Then for every finite prefix $z_{1:n}$, we get a partial output $U(z_{1:n}) = \o_{1:m} \in \B^m$. We require that $U(z_{1:n}) \sqsubseteq U(z_{1:n&#39;})$ for $n \leq n&#39;$. The output of $z$ is infinite if $m\to\infty$ as $n\to\infty$, is finite if $m &amp;lt; \infty$ for all $n$, or is undefined if $U$ does not halt on any $z_{1:n}$.&lt;/p&gt;
&lt;p&gt;Let $\tilde{z}$ be a program that samples from $\mu$ from version 2, i.e. $\tilde{z}$ takes uniform random bits as input and outputs some $x_{1:m}$ with probability $\mu(x_{1:m})$. Then we can produce an infinite &amp;ldquo;version 3&amp;rdquo; program by appending an infinitely long uniform random sequence $u$, i.e. $z = \tilde{z}`u$ where $U(\tilde{z}) = \epsilon$ (empty string, i.e. executing $\tilde{z}$ outputs nothing), and $U(\tilde{z}`u_{1:n&#39;})$ outputs some prefix of $x$ (if we let $x$ be infinite) by running $\tilde{z}$ on input $u_{1:n&#39;}$.&lt;/p&gt;
&lt;p&gt;If we feed uniform random bits into $U$, then $U$ itself samples from a distribution over infinite data sequences. The induced distribution is $p(x_{1:m})$ which is a Solomonoff distribution that we can use for universal inference. This is equivalent to putting a uniform prior on the infinite programs $\B^\infty$, i.e.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(z_{1:n}) = 2^{-n}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;For partial observation $x_{1:m} \in \B^*$ (the remaining part of $x$ is the unobserved future),&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x_{1:m}) = \sum_{\zeta\in\Phi(x_{1:m})} 2^{-\ell(\zeta)}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\ell(\zeta)$ is the length of finite string $\zeta$.&lt;/li&gt;
&lt;li&gt;$\Phi(x_{1:m})$ is a prefix-free set of all finite sequences $\zeta$ which output at least $x_{1:m}$ when fed into $U$ (i.e. for all $z_{1:n} \in \B^*$ if $x_{1:m} \sqsubseteq U(z_{1:n})$ then $z_{1:n} \in \bigcup_{\zeta\in\Phi(x_{1:m})} \Gamma_\zeta$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To calculate $p(x_{1:m})$, we ostensibly want to sum up the prior probabilities of all programs which output at least $x_{1:m}$, but remember that our programs are infinitely long, and the prior probability of any infinite program is 0 (because $2^{-n}\to 0$ as $n\to\infty$). The sum above performs a &lt;a href=&#34;https://en.wikipedia.org/wiki/Lebesgue_integration&#34;target=&#34;_blank&#34;&gt;Lebesgue integral&lt;/a&gt; over the infinite programs $\B^\infty$ by dividing them into &amp;ldquo;intervals&amp;rdquo; (i.e. sets of programs sharing the same prefix - geometrically an interval if you consider an infinite binary sequence to be a real number between 0 and 1) and summing up the lengths (prior probabilities) of the intervals. The function $\Phi$ is a convenient construction for producing this set of intervals for us. Finding $\Phi(x_{1:m})$ is complex, and not especially important to go into.&lt;/p&gt;
&lt;p&gt;The joint distribution is&lt;/p&gt;
&lt;p&gt;$$p(x_{1:m}, z_{1:n}) = \sum_{\zeta\in\Phi(x);\ z_{1:n}\sqsubseteq\zeta} 2^{-\ell(\zeta)}\,.$$&lt;/p&gt;
&lt;p&gt;From here, we can straightforwardly compute the probability of the data under partial hypothesis $z_{1:n}$:&lt;/p&gt;
&lt;p&gt;$$p(x_{1:m} \mid z_{1:n}) =p(x_{1:m}, z_{1:n})/p(z_{1:n}) = \sum_{\zeta\in\Phi(x);\ z_{1:n}\sqsubseteq\zeta} 2^{n-\ell(\zeta)}\,.$$&lt;/p&gt;
&lt;p&gt;And finally the data posterior of the future slice $x_{m:s}$ given $x_{&amp;lt;m}$ (for $m&amp;lt;s$):&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x_{m:s}\mid x_{&amp;lt;m}) = \frac{1}{p(x_{&amp;lt;m})}\sum_{\zeta\in\Phi(x_{1:s})} 2^{-\ell(\zeta)}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;h1 id=&#34;variational-solomonoff-induction&#34;&gt;Variational Solomonoff induction&lt;/h1&gt;
&lt;p&gt;Suppose we observe finite sequence $x_{1:t} \in \B^*$ and we want to find the posterior $p(h \mid x_{1:t})$. Usually this is intractable to calculate, and in the case of Solomonoff induction, the posterior is not even computable. We can get around this limitation by approximating the posterior with a parametrized distribution $q_\t(h)$ over hypotheses $h\in\mc{H}$. For now I will be agnostic as to what kind of hypothesis space $\mc{H}$ is, and it can be any of the hypothesis spaces discussed above: &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt;, &lt;a href=&#34;#version-2&#34;&gt;#Version 2&lt;/a&gt;, &lt;a href=&#34;#version-3&#34;&gt;#Version 3&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this case, let&amp;rsquo;s find $\t^*\in\T$ that minimizes the KL-divergence $\kl{q_\t(h)}{p(h\mid x_{1:t})}$ so that $q_\t(h)$ is as close as possible to $p(h\mid x_{1:t})$. Note that $q_\t(h)$ does not depend on $x_{1:t}$ because we find $\t^*$ after $x_{1:t}$ is already observed ($x_{1:t}$ is like a constant w.r.t. this optimization), whereas the joint distribution $p(h, x)$ is defined up front before any data is observed.&lt;/p&gt;
&lt;p&gt;However, if $p(h \mid x_{1:t})$ is intractable to calculate, then so is $\kl{q_\t(h)}{p(h\mid x_{1:t})}$. With a few tricks, we can find an alternative optimization target that is tractable. Rewriting the KL-divergence:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp;\kl{q_\t(h)}{p(h\mid x_{1:t})} \\&lt;br&gt;
&amp;amp;\quad= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h\mid x_{1:t})}\right)\right] \\&lt;br&gt;
&amp;amp;\quad= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h)}\right)-\lg p(x_{1:t} \mid h) + \lg p(x_{1:t})\right] \\&lt;br&gt;
&amp;amp;\quad= \kl{q_\t(h)}{p(h)} - \E_{h\sim q_\t}\left[\lg p(x_{1:t} \mid h)\right] - \lg \frac{1}{p(x_{1:t})} \\&lt;br&gt;
&amp;amp;\quad= \mc{F}[q_\t] - \lg \frac{1}{p(x_{1:t})} \,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\mc{F}[q_\t]$ is defined as&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q_\t] &amp;amp;= \kl{q_\t(h)}{p(h)} - \E_{h\sim q_\t}\left[\lg p(x_{1:t} \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h)}\right)-\lg p(x_{1:t} \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h,x_{1:t})}\right)\right]\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\mc{F}[q_\t]$ is called the &lt;strong&gt;variational free energy&lt;/strong&gt;. It depends explicitly on choice of parameter $\t$, but also keep in mind it depends implicitly on the observation $x_{1:t}$ and distribution $p(h, x_{1:t})$.&lt;/p&gt;
&lt;p&gt;Because $\lg \frac{1}{p(x_{1:t})}$ (called the &lt;strong&gt;surprise&lt;/strong&gt; of $x_{1:t}$) is positive and constant (because observation $x_{1:t}$ is constant), then minimizing $\mc{F}[q_\t]$ to $\lg \frac{1}{p(x_{1:t})}$ guarantees that $\kl{q_\t(h)}{p(h\mid x_{1:t})}$ is 0 (KL-divergence cannot be negative), which in turn guarantees that $q_\t(h)$ and $p(h\mid x_{1:t})$ are equal distributions on $\mc{H}$. If our optimization process does not fully minimize $\mc{F}[q_\t]$, then $q_\t(h)$ will approximate $p(h\mid x_{1:t})$ with some amount of error.&lt;/p&gt;
&lt;p&gt;The optimization procedure we want to perform is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp;\argmin{\t\in\T} \mc{F}[q_\t] \\&lt;br&gt;
=\ &amp;amp; \argmin{\t\in\T} \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h,x_{1:t})}\right)\right] \\&lt;br&gt;
=\ &amp;amp; \argmax{\t\in\T}\set{ \E_{h \sim q_\t}\left[R(h)\right] + \mb{H}_{h \sim q_\t}[q_\t(h)]}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This now has the form of a one-timestep reinforcement learning objective, where $R(h) = \lg p(h,x_{1:t})$ is the reward for &amp;ldquo;action&amp;rdquo; $h$, and $\mb{H}_{h \sim q_\t}[q_\t(h)]$ is the entropy of $q_\t(h)$. Here $q_\t(h)$ is called the &lt;strong&gt;policy&lt;/strong&gt;, i.e. the distribution actions are sampled from. Maximizing this objective jointly maximizes expected reward under the policy and entropy of the policy. An entropy term is typically added to RL objectives as a regularizer to encourage exploration (higher entropy policy means more random actions), but in this case the entropy term comes included.&lt;/p&gt;
&lt;p&gt;We can use standard &lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html&#34;target=&#34;_blank&#34;&gt;policy gradient methods&lt;/a&gt; (e.g. &lt;a href=&#34;https://deepmind.com/research/publications/impala-scalable-distributed-deep-rl-importance-weighted-actor-learner-architectures&#34;target=&#34;_blank&#34;&gt;IMPALA&lt;/a&gt;) to maximize the above RL objective (equivalent to minimizing free energy), so long as $q_\t(h)$ is fast to sample from, and the reward $R(h) = \lg p(h,x_{1:t})$ is fast to compute. We can control both.&lt;/p&gt;
&lt;h1 id=&#34;tractability&#34;&gt;Tractability&lt;/h1&gt;
&lt;p&gt;Tractability depends on our choice of $\mc{H}$ and prior $p(h)$. What operations do we want to be tractable? Typically we want:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To approximate hypothesis posteriors: $q_{\t^*}(h) \approx p(h \mid x_{1:t})$.&lt;/li&gt;
&lt;li&gt;To approximate predictive data distributions (data posterior): $p(x_{&amp;gt;t} \mid x_{1:t})$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;hypothesis-posterior&#34;&gt;Hypothesis posterior&lt;/h2&gt;
&lt;p&gt;The approximation $q_{\t^*}(h)$ allows us to do this. The tractability of finding a good parameter $\t^*$ for $q$ using policy gradient methods will require that the reward $R(h) = \lg p(h,x_{1:t})$ is fast to calculate.&lt;/p&gt;
&lt;p&gt;We can write the reward as the sum of two terms:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lg p(h,x_{1:t}) = \lg p(h) + \lg p(x_{1:t} \mid h)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then we need fast calculation of prior probabilities $p(h)$, and data probabilities under hypotheses, $p(x_{1:t} \mid h)$.&lt;/p&gt;
&lt;h2 id=&#34;data-posterior&#34;&gt;Data posterior&lt;/h2&gt;
&lt;p&gt;We want to approximate $p(y \mid x)$, i.e. the probability of observing string $y$ after observing $x$. This is similar to the problem of calculating $p(x)$, the data probability.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x) = \E_{h\sim p(h)} [p(x\mid h)]\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If it were fast to compute $p(x\mid h)$ for a given $h$, and fast to sample from the prior $p(h)$, then we can approximate the data probability with Monte Carlo sampling:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x) \approx \hat{p}(x) = \sum_{h \in H^{(k)}} p(x\mid h)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $H^{(k)} = \set{h_1, h_2, \ldots, h_k} \sim p(h)$ is an i.i.d. sample from $p(h)$ of size $k$.&lt;/p&gt;
&lt;p&gt;In the same way, we can approximate the data posterior using the identity&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y \mid x) = \E_{h\sim p(h \mid x)} [p(y\mid x, h)]\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The Monte Carlo approximation is:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y\mid x) \approx \hat{p}(y\mid x) = \sum_{h \in H^{(k)}} p(y\mid x, h)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $H^{(k)} = \set{h_1, h_2, \ldots, h_k} \sim q_{\t^*}(h)$ is an i.i.d. sample drawn from the optimized approximate posterior $q_{\t^*}(h)$. So approximating the data posterior requires approximating the hypothesis posterior.&lt;/p&gt;
&lt;p&gt;$p(y\mid x, h)$ is the conditional data distribution under hypothesis $h$. If $h$ is a probability measure, then $p(x \mid h) = h(x)$ and $p(y\mid x, h) = h(y\mid x)$.&lt;/p&gt;
&lt;p&gt;For approximating the data distribution, we need fast sampling from hypothesis priors $p(h)$ and fast data-under-hypothesis probabilities $p(x \mid h)$.&lt;/p&gt;
&lt;p&gt;For approximating the data posterior, we need fast approximate posteriors $q_{\t^*}(h)$, and we need hypothesis data-conditionalization $p(y\mid x, h)$ to be fast.&lt;/p&gt;
&lt;h2 id=&#34;generalization&#34;&gt;Generalization&lt;/h2&gt;
&lt;p&gt;Speed is necessary but not sufficient for tractability. The approximations we find need to be good ones. Choosing an appropriate model $q$, which is a distribution over programs, is within the realm of program synthesis and machine learning. These days, program synthesis is done with neural language models on code tokens.&lt;/p&gt;
&lt;p&gt;Can neural networks approximate the true posterior $p(h \mid x)$? This is a generalization problem. The optimized generative model on programs, $q_{\t^*}(h)$, will have been trained on finitely many programs. Whether $q_{\t^*}(h&#39;) \approx p(h&#39; \mid x)$ for some program $h&#39;$ unseen during training will depend entirely on the generalization properties of the particular program synthesizer that is used in $q_\t$.&lt;/p&gt;
&lt;p&gt;The difficulty of applying machine learning to program synthesis is dealing with reward sparsity and generalizing highly non-smooth functions. To maximize reward $R(h) = \lg p(h,x_{1:t})$, the model $q$ needs to upweight programs $h$ that jointly have a high prior $p(h)$ and high likelihood $p(x_{1:t} \mid h)$. If the prior $p(h)$ is simple, perhaps $q$ can learn that function. On the other hand, if this prior encodes information about how long $h$ runs for (as I discuss in the &lt;a href=&#34;https://danabo.github.io/blog/posts/variational-solomonoff-induction/#prior&#34;&gt;Variational Solomonoff Induction#prior&lt;/a&gt; section), the prior is then not even computable. Same for $p(x_{1:t} \mid h)$. Without actually running $h$ on $x_{1:t}$, determining the output will not be possible in general. For $q$ to predict these things based on $h$&amp;rsquo;s code but without running $h$ is in general impossible. The function $p(h \mid x_{1:t})$ (as a function of $h$) highly chaotic, and we cannot expect $q$ to generalize in any strong sense. Innovations in program synthesis are still needed to do even somewhat well.&lt;/p&gt;
&lt;p&gt;As a reinforcement learning problem, maximizing this reward suffers from sparsity issues. Most programs will be trivial, in the sense that they just output constant values, or nothing. I expect that Solomonoff induction doesn&amp;rsquo;t start to become effective until you get to programs of moderate length that exhibit interesting behavior. In the context of this reinforcement learning problem, that means the policy $q$ needs to find moderately long programs with moderately high reward. When training first starts, it can take an excessive amount of episodes before any non-trivial reward is discovered. This can make reinforcement learning intractable. Innovations are needed here too.&lt;/p&gt;
&lt;h1 id=&#34;choices&#34;&gt;Choices&lt;/h1&gt;
&lt;p&gt;To summarize the requirements we found above:&lt;br&gt;
Is there a choice of $\mc{H}$ and $p(x,h)$ s.t.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prior $p(h)$ is fast to calculate and sample from.&lt;/li&gt;
&lt;li&gt;Approximate hypothesis posterior $q_{\t^*}(h)$ is fast to sample from.&lt;/li&gt;
&lt;li&gt;Hypothesis data-probability $p(x\mid h)$ is fast to calculate.&lt;/li&gt;
&lt;li&gt;Hypothesis data-conditionalization $p(y\mid x, h)$ is fast to calculate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;program-space&#34;&gt;Program space&lt;/h2&gt;
&lt;p&gt;Hypotheses can be deterministic or stochastic. Deterministic hypotheses would be represented by deterministic programs. Stochastic hypotheses can either be represented by stochastic programs (output is non-deterministic) or by deterministic programs that output probabilities. I think we should choose the latter.&lt;/p&gt;
&lt;p&gt;If our hypotheses are deterministic, then we get Solomonoff induction &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt;. Conditionalization is easy because $p(y \mid x, h) = p(y`x \mid h) = 1$ if $h$ outputs $x$ and $0$ otherwise. However, the vast majority of programs will not output $x$, so the reward $R(h) = \lg p(h,x)$ will be very sparse. That is to say, the reward will be $-\infty$ most of the time (in practice you would clip and scale the reward to something reasonable). This is bad for policy gradient methods and will result in high gradient variance (learning will be extremely slow).&lt;/p&gt;
&lt;p&gt;We should use stochastic hypotheses. If we use non-deterministic programs, conditionalization is hard. Thus we should use programs that output their probabilities.&lt;/p&gt;
&lt;p&gt;The choice of $\mc{H}$ is equivalent to choosing a programming language plus syntax rules so that only valid programs can be constructed. In this case, we want to restrict ourselves to programs that will obey the properties of probability measures $\mu$ on infinite sequences: $\mu(x) = \mu(x`0) + \mu(x`1)$ and  $\mu(x) \geq \mu(x`y)$.&lt;/p&gt;
&lt;p&gt;To ensure this, I propose that programs $h$ take the form of auto-regressive language models. That is to say these programs read in a sequence of input tokens and for each token output a vector of real numbers with the same length as the token space. Passing that vector through a softmax induces a probability distribution over the next input token. The programs maintain their own internal state. A language should be chosen such that all generated programs can be guaranteed to take this form.&lt;/p&gt;
&lt;p&gt;If the program has output probabilities $\hat{p}_1, \ldots, \hat{p}_t$ for input $x_{1:t}$ but does not halt to produce $\hat{p}_{t+1}$, then the probability $\mu_h(x_{1:n})$ for $n&amp;gt;t$ is undefined, and the induced measure $\mu_h$ becomes a semimeasure.&lt;/p&gt;
&lt;h2 id=&#34;prior&#34;&gt;Prior&lt;/h2&gt;
&lt;p&gt;Weighting by program length suffices as a prior:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(h) = 2^{-\ell(h)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;One difficulty in working with programs is long-running execution. This can make computing data probabilities take a long time. One remedy is to down-weight long-running programs in the prior. &lt;a href=&#34;http://www.scholarpedia.org/article/Universal_search&#34;target=&#34;_blank&#34;&gt;Levin search&lt;/a&gt; is an alternative to Solomonoff induction where the prior is weighted solely by runtime. We can mix both kinds of priors.&lt;/p&gt;
&lt;p&gt;This is straightforward in Solomonoff induction &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt; where each program takes no input and outputs a deterministic string. Let $p(h) = 2^{-\ell(h)} / f(\tau(h))$ where $\tau(h)$ is the total runtime of $h$, and $f$ is an increasing function that goes to infinity. For example, if $f(t) = 2^{t}$, then we have prior $2^{-\ell(h)-\tau(h)}$. If you wanted to compute $p(x)$ within some precision $\ve &amp;gt; 0$, you can enumerate all programs $h\in\mc{H}$ by length and run them in parallel (called dovetailing). For each program, stop execution when $2^{-\ell(h)-\tau(h)} &amp;lt; \ve$. Shorter programs will be given more runtime over longer programs. (Thank you Lance Roy for the helpful discussion about this.)&lt;/p&gt;
&lt;p&gt;However, if we are using the programs I previously suggested that output data probabilities, then these programs may be fast on some inputs and slow on others. I don&amp;rsquo;t have a solution, but a reasonable suggestion is to do some kind of heuristic analysis of the programs on some sample inputs and assign a slowness penalty in the prior.&lt;/p&gt;
&lt;h1 id=&#34;lifelong-learning&#34;&gt;Lifelong learning&lt;/h1&gt;
&lt;p&gt;Solomonoff induction is a framework for general-purpose life-long learning, which is a paradigm where an intelligent agent learns to predict it&amp;rsquo;s future (or gain reward) from only one continuous data stream. The agent must learn on-line, and there are no independence assumptions (the data is a timeseries).&lt;/p&gt;
&lt;p&gt;In the variational setup outlined above, we converted the problem of Bayesian inference into a reinforcement learning problem. At time $t$, data $x_{1:t}$ is observed, and a policy $q_{\t^*}$ on programs is trained using policy gradient methods. However, every $t$ requires its own $q_{\t_t^*}$, thus we would need to perform RL training at every timestep. One way to speed this up is to reuse policies from previous timesteps. That is to say, at time $t+1$ perform $\argmax{\t\in\T}\set{ \E_{h \sim q_\t}\left[R(h)\right] + \mb{H}_{h \sim q_\t}[q_\t(h)]}$ using Monte Carlo gradient descent starting from the previous parameter $\t_t^*$. This can be considered fine-tuning. However, this may fail to work if the posterior changes drastically between timesteps.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Active inference tutorial (actions)</title>
      <link>https://danabo.github.io/blog/posts/active-inference-tutorial-actions/</link>
      <pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/active-inference-tutorial-actions/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\kl}[2]{D_{KL}\left(#1\ \| \ #2\right)}&lt;br&gt;
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\ }&lt;br&gt;
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}}\ }&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\atup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\r}{\rho}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Previous attempts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;a href=&#34;https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/&#34;&gt;Free Energy Principle 1st Pass&lt;/a&gt;, I used a &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt; to try to understand the free energy formalism. I figured out the &amp;ldquo;timeless&amp;rdquo; and actionless case, but I became confused when actions and time were added.&lt;/li&gt;
&lt;li&gt;In &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;Free energy principle and Solomonoff induction&lt;/a&gt;, I tried to translate between the formalism presented in &lt;a href=&#34;https://danijar.com/apd/&#34;&gt;https://danijar.com/apd/&lt;/a&gt; (which is a deep learning collaboration between  Danijar Hafner and Karl Friston) and the tutorial &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt;. I also tried to make the connection to Solomonoff induction.&lt;/li&gt;
&lt;li&gt;In &lt;a href=&#34;https://danabo.github.io/blog/posts/variational-solomonoff-induction/&#34;&gt;Variational Solomonoff Induction&lt;/a&gt;, I thought about whether free energy (as variational inference) could be applied to deep program synthesis to approximate Solomonoff induction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the same tutorial as before, &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;, I will go through the free energy formalism again and try to work out time and actions.&lt;/p&gt;
&lt;h1 id=&#34;review&#34;&gt;Review&lt;/h1&gt;
&lt;p&gt;To recap what I figured out in &lt;a href=&#34;https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/&#34;&gt;Free Energy Principle 1st Pass&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;Suppose $o\in\mc{O}$ is the observation space and $s\in\mc{S}$ is the hypothesis/state space (to use the notation of the &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt;). For now, let&amp;rsquo;s assume that $\mc{S}$ is the hidden state space of the environment in some timestep. $p(o,s)$ is the agent&amp;rsquo;s model of the environment, relating observations to hidden states, and the prior probability of the environment being in any particular hidden state. Let&amp;rsquo;s also ignore questions about the meaning of these probabilities (objective or subjective) and where they come from. If it&amp;rsquo;s easier to think about, assume these probabilities are subjective.&lt;/p&gt;
&lt;p&gt;If $o$ is observed (one timestep only), then we want to calculate the posterior probability $p(s\mid o)$ of each $s\in\mc{S}$. If this is intractable to do, we can instead find an approximation $q_o(s)$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
q_o := \argmin{q \in \mc{Q}} \kl{q(s)}{p(s\mid o)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\mc{Q}$ is some space of distributions that you choose $q_o$ from. Presumably $\mc{Q}$ is restricted somehow, otherwise the solution is $q=p$ in which case you are doing exact Bayesian inference.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\kl{q(s)}{p(s\mid o)} &amp;amp;= \E_{s\sim q}\left[\lg\frac{q(s)}{p(s\mid o)}\right] \\&lt;br&gt;
&amp;amp;= \E_{s\sim q}\left[\lg\frac{q(s)}{p(s)}\right] + \E_{s\sim q}\left[\lg \frac{1}{p(o \mid s)}\right] + \E_{s\sim q}[p(o)]\\&lt;br&gt;
&amp;amp;= \kl{q(s)}{p(s)} - \E_{s\sim q}\left[\lg p(o \mid s)\right] - \lg\frac{1}{p(o)} \\&lt;br&gt;
&amp;amp;= \mc{F}[q] - \lg\frac{1}{p(o)}\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= \kl{q(s)}{p(s)} - \E_{s\sim q}[\lg p(o \mid s)]\\&lt;br&gt;
&amp;amp;= \E_{s\sim q}\left[\lg\frac{q(s)}{p(s,o)}\right]&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is called &lt;strong&gt;variational free energy&lt;/strong&gt;. $\kl{q(s)}{p(s)}$ is called &lt;strong&gt;accuracy&lt;/strong&gt; and $\E_{s\sim q}[\lg p(o \mid s)]$ is called &lt;strong&gt;complexity&lt;/strong&gt;. $\lg\frac{1}{p(o)}$ is called &lt;strong&gt;surprise&lt;/strong&gt; (self-information of the observation $o$).&lt;/p&gt;
&lt;p&gt;Minimizing $\mc{F}[q]$ w.r.t. $q$ minimizes $\kl{q(s)}{p(s\mid o)}$. The surprise $\lg\frac{1}{p(o)}$ is constant w.r.t. this minimization. (Remember this is all assuming $o$ is given and fixed.)&lt;/p&gt;
&lt;h1 id=&#34;new-stuff&#34;&gt;New stuff&lt;/h1&gt;
&lt;p&gt;In the &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt;, we have free energy defined just as in my notes, except that everything now depends on policy $\pi$:&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210207121048.png&#34; alt=&#34;&#34;&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210207121055.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
The policy $\pi$ is a probability distribution over actions (, e.g. $\pi(a_t \mid o_{1:t}, a_{1:t-1})$. More on that later.&lt;/p&gt;
&lt;h2 id=&#34;interaction-loop&#34;&gt;Interaction loop&lt;/h2&gt;
&lt;p&gt;It is not clear to me whether $o,s$ are sequences over time, or just one time-step. In &lt;a href=&#34;https://danabo.github.io/blog/posts/variational-solomonoff-induction/&#34;&gt;Variational Solomonoff Induction&lt;/a&gt; I showed how to interpret $s$ as a hypothesis that explains an infinite sequence of observations, i.e. $s$ is not a sequence but $o$ is. When I write $o_{1:\infty}$, that is an observation sequence over time. When I write $s_{1:\infty}$ that is a state sequence over time. I&amp;rsquo;ll use $h$ later to denote a time-less hypothesis on observation sequences $o_{1:\infty}$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s unpack the agent-environment interaction loop. Given policy $\pi$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(o_{1:\infty},s_{1:\infty}\mid\pi) = \sum_{a_{1:\infty}}\prod_t p(o_t,s_t\mid a_t,s_{t-1})\pi(a_t\mid o_{1:t-1},a_{1:t-1})&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $p(o_t,s_t\mid a_t,s_{t-1})$ is the probability of next observation and hidden state given input action $a_t$ and previous state, and $\pi(a_t\mid o_{1:t-1},a_{1:t-1})$ is the probability of the agent taking action $a_t$ given its entire history $o_{1:t-1},a_{1:t-1}$ (alternative we can give the agent internal state and condition on that).&lt;/p&gt;
&lt;p&gt;On the other hand, if $s$ (or $h$) is a time-less hypothesis: then we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(o_{1:\infty},h\mid\pi) = \sum_{a_{1:\infty}}\prod_t p(o_t\mid a_{1:t},o_{1:t-1},h)p(h)\pi(a_t\mid o_{1:t-1},a_{1:t-1})&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $p(h)$ is the prior on hypothesis $h$.&lt;/p&gt;
&lt;p&gt;Below, I will leave it ambiguous whether $s$ is a sequence of states or a time-less hypothesis. The math should be the same either way.&lt;/p&gt;
&lt;h2 id=&#34;active-inference&#34;&gt;Active inference&lt;/h2&gt;
&lt;p&gt;How are actions chosen? This is the big question I could not penetrate in my previous attempts. From the tutorial:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When inferring optimal actions, however, one cannot simply consider current observations. This is because actions are chosen to bring about preferred future observations. This means that, to infer optimal actions, a model must predict sequences of future states and observations for each possible policy, and then calculate the expected free energy (ð¸ð¹ð¸) of those different sequences of future states and observations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is talking about taking an expectation over future states and observations. Let&amp;rsquo;s assume $p(o_{1:\infty}, s_{1:\infty} \mid \pi)$ is the true environment dynamics. We are introduced to a new term $q(o_{1:\infty}\mid\pi)$ which are the agent&amp;rsquo;s observation preferences over time given a particular policy.&lt;/p&gt;
&lt;p&gt;The text is saying we want to choose policy $\pi$ to maximize $G(\pi)$. What&amp;rsquo;s troubling is that there is another term, $p(\pi)$, a prior over policies. If we are choosing policies freely, what does this prior represent?&lt;/p&gt;
&lt;p&gt;The text says that the preferred policy also minimizes free energy:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Since âpreferredâ here formally translates to âexpected by the modelâ, then the policy expected to produce preferred observations will be the one that maximizes the accuracy of the model (and hence minimizes ð¸ð¹ð¸).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;exact-inference&#34;&gt;Exact inference&lt;/h2&gt;
&lt;p&gt;To simplify things, let&amp;rsquo;s suppose the agent can do perfect Bayesian inference, so that $q_o(s\mid\pi) = p(s \mid o,\pi)$. Let&amp;rsquo;s see what happens if we plug in $p(s\mid o,\pi)$ for $q_o(s\mid \pi)$ in our free energy definition:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mc{F} = \E_{s \sim p(s\mid o,\pi)}\left[\lg \frac{p(s\mid o,\pi)}{p(s,o\mid\pi)}\right] = \E_{s \sim p(s\mid o,\pi)}\left[\lg \frac{1}{p(o\mid \pi)}\right] = \lg \frac{1}{p(o\mid \pi)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is just the surprise (i.e. self-information due to observing $o$). Minimizing free energy means choosing $\pi$ to maximize the data likelihood:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\pi^* := \argmax{\pi} p(o\mid\pi)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Remember that $\mc{F}$ depends on a fixed $o$, which is what has already been observed. If $o$ is not observed, then we are talking about future $o$, and we need to take an expectation w.r.t. $o$, e.g.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\pi^* := \argmin{\pi} \E_{o\sim p(o\mid\pi)}\lg\frac{1}{p(o\mid\pi)} = \argmin{\pi}\mb{H}[p(o\mid\pi)]&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is saying, choose a policy s.t. the future is as predictable as possible, i.e. minimizes entropy over observations, i.e. minimizes future expected surprise.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s introduce the agent&amp;rsquo;s preferences, encoded as a distribution on observations. The tutorial (and other texts) use $q(o)$, but I&amp;rsquo;m using $\r(o)$, because this is a very different thing from the approximate posterior $q_o$. Specifically $\r(o)$ is given and held fixed, while $q_o(s)$ depends on the particular observation $o$, as well as choice of optimization space $\mc{Q}$. In short, $q_o(s)$ is an output, while $\rho(o)$ is given.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s replace $p(o\mid\pi)$ with $\rho(o)$ (this should not depend on $\pi$). So instead of taking an expectation w.r.t. the model probabilities for $o$, we are taking an average weighted by preference for $o$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\pi^* &amp;amp;:= \argmin{\pi} \E_{o\sim \r(o)}\lg\frac{1}{p(o\mid\pi)} \\&amp;amp;= \argmin{\pi} H(\r(o), p(o\mid\pi)) \\&amp;amp;= \argmin{\pi} \left\{\kl{\r(o)}{p(o\mid\pi)} + \mb{H}[\r(o)]\right\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross_entropy&#34;target=&#34;_blank&#34;&gt;cross-entropy&lt;/a&gt; of $q(o)$ and $p(o\mid\pi)$ (average number of bits if you encode a stream $o_{1:\infty}$ under $p$ while actually sampling from $p$). Since $\r(o)$ is fixed, then we are minimizing $\kl{\r(o)}{p(o\mid\pi)}$. That is to say, choose policy (thereby choosing actions) that make the environment (as the agent believes it to be) dynamics $p(o\mid\pi)$ conform to preferences $\r(o)$.&lt;/p&gt;
&lt;h3 id=&#34;reward-equivalence&#34;&gt;Reward equivalence&lt;/h3&gt;
&lt;p&gt;According to  &lt;a href=&#34;https://danijar.com/apd/,&#34;&gt;https://danijar.com/apd/,&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\r(o_{1:n}) \propto \exp(r(o_{1:n}))&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $r(o_{1:n})$ is the total reward received for observations $o_{1:n}$. Written another way, $\r(o_{1:n}) = \exp(r(o_{1:n}))/\mc{Z}$ for some normalization constant $\mc{Z}$, so then $r(o_{1:n}) = \ln\r(o_{1:n}) + \mc{C}$ for some constant offset $\mc{C}$.&lt;/p&gt;
&lt;p&gt;If we are just trying to maximize expected total reward $r(o_{1:n})$ w.r.t. the environment model, we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\pi^* &amp;amp;:= \argmax{\pi} \E_{o \sim p(o \mid \pi)} [r(o_{1:n})] \\&lt;br&gt;
&amp;amp;= \argmax{\pi} \E_{o_{1:n},a_{1:n} \sim p(o_{1:n},a_{1:n} \mid \pi)} [\ln\r(o_{1:n})]\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;So far, I am not seeing anything conceptually new here. Storing agent preferences in a probability distribution $\r(o)$ is not really any different from storing preferences in a reward $r(o)$, and the two are easily converted into each other.&lt;/p&gt;
&lt;h2 id=&#34;putting-it-all-together&#34;&gt;Putting it all together&lt;/h2&gt;
&lt;p&gt;Now let&amp;rsquo;s suppose that $o$ has not yet been observed as before, but use approximate (future) posterior $q_{o,\pi}(s)$ and compute expected future free energy under preference $\rho(o)$.&lt;/p&gt;
&lt;p&gt;We get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
G[\pi]&amp;amp;=\E_\rho[\mc{F}[o,\pi]] \\&lt;br&gt;
&amp;amp;= \E_{o\sim\rho}\E_{s \sim q_{o,\pi}}\left[\lg\frac{q_{o,\pi}(s)}{p(s,o\mid\pi)}\right] \\&lt;br&gt;
&amp;amp;= \E_{o\sim\rho}\kl{q_{o,\pi}(s)}{p(s\mid\pi)} - \E_{o\sim\rho}\E_{s\sim q_{o,\pi}}\left[\lg p(o \mid s,\pi)\right]&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $q_{o,\pi}$ is the optimal approximate posterior for the given observation $o$ and policy $\pi$ used to obtain $o$. From the perspective of $q_{o,\pi}$, $o$ is already observed using policy $\pi$ which determines the probability of that observation.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
q_{o,\pi} := \argmin{q} \mc{F}[o,\pi] = \argmin{q}\E_{s \sim q}\left[\lg\frac{q(s)}{p(s,o\mid\pi)}\right]&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;I believe the tutorial paper has a typo, where $p(o,s,\pi)$ should be $p(o,s,\mid\pi)$.&lt;/p&gt;
&lt;p&gt;We are choosing $\pi$ to minimize $G[\pi]$, which is just the expected free energy under $\rho(o)$ (preference for future observations).&lt;/p&gt;
&lt;p&gt;Do the optimizations on $\pi$ and $q$ interact? It seems like they don&amp;rsquo;t. $\pi$ is an outer optimization that depends on running the optimization on $q$ internally. There is not a single $q$, but many of them which the optimization on $\pi$ iterates through. So then what is the significance of connecting free energy minimization ($q$) to active inference ($\pi$)? If the policy optimization part were reformulated in terms of RL, we really just have a fancy kind of approximate Bayesian model combined with RL. The action learning and model updating are totally independent.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://arxiv.org/abs/1911.10601&#34;&gt;https://arxiv.org/abs/1911.10601&lt;/a&gt; for a discussion about the equivalence between variational approaches to RL and active inference.&lt;/p&gt;
&lt;h1 id=&#34;the-meaning-of-p&#34;&gt;The meaning of $p$&lt;/h1&gt;
&lt;p&gt;If $p(o_{1:\infty},s_{1:\infty}\mid\pi)$ is a dynamics model of the environment, how would the agent know it? Or alternatively, how are different hypotheses for environment dynamics handled in this framework?&lt;/p&gt;
&lt;p&gt;The two cases are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$p(o_{1:\infty},s_{1:\infty}\mid\pi)$ is the literal true dynamics of the environment.&lt;/li&gt;
&lt;li&gt;$p(o_{1:\infty},s_{1:\infty}\mid\pi)$ is the agent&amp;rsquo;s dynamics model of the environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first case is unreasonable, because we cannot assume any agent knows the truth. The second case does not allow the agent to update its ontology, i.e. change the state space $\mc{S}$ and it&amp;rsquo;s beliefs about how observations interact with, $p(o\mid s)$ and $p(s\mid o)$.&lt;/p&gt;
&lt;p&gt;We could suppose there is a latent $h$ for the environment hypothesis which is being marginlized, e.g. $p(o_{1:n},s_{1:n},h)$, but then $p(o_{1:n}\mid s_{1:n}) = \E_{h\sim p(h)}p(o_{1:n}\mid s_{1:n},h)$, which we can generally expect to be intractable but is required for free energy calculation. The free energy approximation was supposed to be tractable. Now do we have to approximate the approximation?&lt;/p&gt;
&lt;p&gt;I think the time-less hypothesis formulation is better, i.e. $p(o_{1:\infty}, h)$, because it allows the hypothesis to invent its own states (because states are no longer explicitly defined), and put emphasis on not just the present, but possible states in the past and future, i.e. the agent may be more interested in inferring past or future states. Furthermore, states may not be well defined things. I have a model of the world filled with all sorts of objects, each having independent states until they interact. I cannot comprehend thinking of everything I know as one gigantic state.&lt;/p&gt;
&lt;p&gt;Something I&amp;rsquo;ve heard hinted at elsewhere is that the agent, as a physical system, expresses some Bayesian prior $p$ and preferences $\r$ in an objective sense. What is the nature of this mapping between physical makeup and active-inference description? Is this entirely based on the agent&amp;rsquo;s behavior, or if we looked inside an agent, we could determine its model and preferences? I expect that if we look at behavior alone, then $p$ and $\r$ are underspecified.&lt;/p&gt;
&lt;p&gt;So then what about the agent&amp;rsquo;s physical makeup gives it a model $p$ and preferences $\r$? The optimization process to find $q_{o,\pi}$ must be physically carried out, and so presumably this could be observed. In optimizing for $q_{o,\pi}$, the agent would actually be engaged in two processes that implicitly specify $p$. Splitting free energy into accuracy and complexity:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Explaining observations: $\E_{h\sim q}\left[\lg p(o_{1:n} \mid h,a_{1:n})\right]$&lt;br&gt;
The agent thinks of hypotheses $h$ (sampling them from $q$) to explain observations $o_{1:n}$ given actions $a_{1:n}$.&lt;/li&gt;
&lt;li&gt;Regularization: $\kl{q(h)}{p(h)}$&lt;br&gt;
The agent updates its hypothesis generator $q$, implicitly conforming to $p$ which represents the agent&amp;rsquo;s grand total representation capacity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under this perspective, the agent&amp;rsquo;s ability to modify its own hypothesis generator $q(h)$ is somehow described by $p(h)$, which is fixed throughout the agent&amp;rsquo;s lifetime (unless the agent can self-modify). For a particular hypothesis $h$, the data probability $p(o_{1:n}\mid h)$ is the likelihood of the data under $h$. So $p$ is simultaneously encoding the agent&amp;rsquo;s theoretical capacity to generate hypotheses (which it never fully reaches because of limitations on $q(h)$) and the meaning of every hypothesis it can come up with. It is unclear to me whether $p(h, o_{1:n})$ can be uniquely determined given an agent&amp;rsquo;s physical makeup.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m also unconvinced about the way behavior is handled in this framework. Why think in terms of policies $\pi$ rather than actions $a_{1:n}$? Is the space of policies fixed through the agent&amp;rsquo;s lifetime? If $\pi$ is supposed to represent some kind of high level strategy, then how does the agent learn different kinds of strategies (updating its ontology). This is the same problem that Bayesian inference faces, that $q$ ostensibly fixes. But now we need to fix the same problem again for $\pi$.&lt;/p&gt;
&lt;p&gt;Question: $G[\pi]$ appears to be intractable to compute or optimize directly. Why do we not have a variational approximation to this as well?&lt;/p&gt;
&lt;p&gt;Why not just do RL? What is gained by &amp;ldquo;active inference&amp;rdquo;, which seems to me to be secretly RL on top of variational Bayes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Free Energy Principle 1st Pass</title>
      <link>https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/</link>
      <pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\kl}[2]{D_{KL}\left(#1\ \| \ #2\right)}&lt;br&gt;
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\ }&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\atup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Related notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;The free-energy principle a unified brain theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;Friston&amp;#39;s Free Energy (Active Inference)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hackmd version of this note: &lt;a href=&#34;https://hackmd.io/@z5RLVXyrTg-JLCnL9c_xOQ/r1KSFUjkO&#34;&gt;https://hackmd.io/@z5RLVXyrTg-JLCnL9c_xOQ/r1KSFUjkO&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;my-current-understanding&#34;&gt;My current understanding&lt;/h1&gt;
&lt;h2 id=&#34;note-on-probability-notation&#34;&gt;Note on probability notation&lt;/h2&gt;
&lt;p&gt;These are my informal notes. Probability notation can be cumbersome and overly verbose. As is customary in machine learning and many of the sciences, I&amp;rsquo;m not going to bother using probability notation correctly. That is to say, I&amp;rsquo;m going to mangle and confuse probability measures, random variables, and probability mass/density functions. Hopefully readers can figure out the intended meaning of my notation from context, and I try to clarify when needed.&lt;/p&gt;
&lt;p&gt;In general, figuring out good notational conventions for probability in many fields (I&amp;rsquo;m looking at you machine learning, but the neuroscience free energy literature also has this problem) seems to be an unsolved problem, and one that I&amp;rsquo;d like to work on at some point! However that is not my concern here. I&amp;rsquo;m just taking the easiest route to expressing my knowledge. If you want to know what &amp;ldquo;proper&amp;rdquo; probability notation looks like, check out &lt;a href=&#34;http://zhat.io/articles/primer-probability-theory&#34;target=&#34;_blank&#34;&gt;my post on probability theory&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;bayesian-basics&#34;&gt;Bayesian basics&lt;/h2&gt;
&lt;p&gt;Suppose $\mc{H}$ is a set of possible hidden states, and $\mc{X}$ is a set of possible observations. Each $h\in\mc{H}$ &lt;strong&gt;explains&lt;/strong&gt; data $x\in\mc{X}$ through the &lt;strong&gt;conditional data distribution&lt;/strong&gt; $p_{X \mid H}(x \mid h)$, or also notated $p_{X \mid H=h}(x)$. A &lt;strong&gt;Bayesian mixture&lt;/strong&gt; is the mixture distribution&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p_{X,H}(x,h) = p_H(h)\cdot p_{X \mid H}(x \mid h)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $p_H(h)$ is called the &lt;strong&gt;prior&lt;/strong&gt;. We can then compute the &lt;strong&gt;posterior&lt;/strong&gt; $p_{H \mid X}(h\mid x)$. The marginal distribution $p_X$ is called the &lt;strong&gt;subjective data distribution&lt;/strong&gt;, since it is partly determined by the choice of prior (if we assume the prior is subjective, i.e. quantifies personal belief). $p_X$ is an agent&amp;rsquo;s best prediction for what they will observe given what they believe (the prior).&lt;/p&gt;
&lt;h2 id=&#34;the-philosophy-of-bayesian-information&#34;&gt;The philosophy of Bayesian information&lt;/h2&gt;
&lt;p&gt;There are different ways to interpret quantity of information information. In the Bayesian setting, I like to think about the amount by which a possibility space was narrowed down. A probability $p_X(x)$ on $x\in\mc{X}$ represents the fraction of possibilities that remain after observing $x$. If we suppose there is a finite possibility space $\Omega$, and the function $X : \Omega \to \mc{X}$ is a random variable that tells us &amp;ldquo;which $x$&amp;rdquo; a given $\omega\in\Omega$ encodes, then the probability of $x$ is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p_X(x) = \frac{\abs{\set{\omega\in\Omega : X(\omega) = x}}}{\abs{\Omega}}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is the number of $\omega$s that encode $x$ over the total number of possibilities. This setup is supposing that $x$ is a &lt;strong&gt;partial observation&lt;/strong&gt;, meaning that even after observing $x$, we still don&amp;rsquo;t know which $\omega$ was drawn.&lt;/p&gt;
&lt;p&gt;In bits, we have $-\lg p_X(x)$, which is the number of halvings of the possibility space $\Omega$ due to observing $x$. Bits and probabilities can be viewed as different units for the same quantity. $-\lg p_X(x)$ is called the &lt;strong&gt;self-information&lt;/strong&gt; of $x$, and in the Bayesian setting, the &lt;strong&gt;surprise&lt;/strong&gt; due to observing $x$, implying that a higher number of bits makes $x$ more surprising, which makes sense because $x$ caused you to rule out much more of your possibility space.&lt;/p&gt;
&lt;p&gt;The connection between $p_X$, information gain $-\lg p_X(x)$, and a physical agent, is that for an agent to have a possibility space, it must have the physical representational capacity. If we presume that a system bounded in a finite region of space contains finite information, i.e. can only occupy finitely many distinguishable states, then our agent must have a finite possibility space $\Omega$ in its &lt;strong&gt;model&lt;/strong&gt; of the environment. Gaining information $-\lg p_X(x)$ requires that the agent physically update its internal possibility space, reducing it by the amount $p_X(x)$. That is to say, information gain quantifies a physical update to an agent. In this sense, subjective information quantifies a change to an agent due to its model of the environment and observations, whereas objective information quantifies a change in the environment.&lt;/p&gt;
&lt;p&gt;Note that $p_X$ may be the marginal distribution of $p_{X,H}$, in which case $p_X$ is a subjective data distribution. It is not strictly necessary to actually define hypotheses and priors. $p_X$ can be regarded as a prior, as the end result is the same: the agent has some possibility space, reflecting what the agent is capable of believing, and the proportions of each $x\in\mc{X}$ in that possibility space correspond to how confident the agent is in any given outcome relative to other outcomes.&lt;/p&gt;
&lt;p&gt;If $\Omega$ is infinite (countable or uncountable), then we cannot just divide by the size of $\Omega$ to compute probabilities. In this case, we need to provide a measure that tells us how much of the possibility space $\Omega$ any subset is worth, i.e. $P(A)$ for $A \subseteq \Omega$ measures the fraction of $\Omega$ that $A$ is worth even when $\Omega$ and $A$ are infinite. $P$ is called a &lt;strong&gt;probability measure&lt;/strong&gt;, but don&amp;rsquo;t worry about that. The point is that even in the case of infinite possibilities, we can still think of information gain in terms of narrowing down a possibility space.&lt;/p&gt;
&lt;h2 id=&#34;bayesian-surprise&#34;&gt;Bayesian surprise&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Mutual_information&#34;&gt;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Mutual_information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let $\mc{X}$ be data space, $\mc{H}$ be hypothesis space, and $X,H$ be data and hypothesis random variables.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
I[X,H] &amp;amp;= \E_X[\kl{p_{H\mid X}}{p_H}] \\&lt;br&gt;
&amp;amp; = \E_{x\sim X}[H(p_{H\mid X=x}, p_H) - H(p_{H\mid X=x})] \\&lt;br&gt;
&amp;amp; = \E_{x\sim X}[\E_{h \sim p_{H\mid X=x}}[-\lg p_H(h)] - \E_{h \sim p_{H\mid X=x}}[-\lg p_{H\mid X}(h \mid x)]] \\&lt;br&gt;
&amp;amp; = \E_{x,h \sim p_{X,H}}\left[\lg \left( \frac{p_{H\mid X}(h\mid x)}{p_H(h)}\right)\right] \\&lt;br&gt;
&amp;amp; = \sum_{x,h \in \mc{X} \times \mc{H}}p_{X,H}(x,h)\lg \left( \frac{p_{H\mid X}(h\mid x)}{p_H(h)}\right) \\&lt;br&gt;
&amp;amp; = \sum_{x,h \in \mc{X} \times \mc{H}}p_{X,H}(x,h)\lg \left( \frac{p_{X,H}(x,h)}{p_X(x)p_H(h)}\right)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$I[X,H]$ is called &lt;strong&gt;Bayesian surprise&lt;/strong&gt;, which is the expected (over data) KL divergence from your prior to posterior (after observing data), which is itself the expected difference in uncertainty (measured in bits, the number of halvings of the full possibility space).&lt;/p&gt;
&lt;p&gt;Pointwise Bayesian information gain (information gained about hypothesis $h$ from data $x$) is $\lg (1/p_H(h)) - \lg (1/p_{H \mid X}(h \mid x)) = \lg (1/p_X(x)) - \lg (1/p_{X \mid H}(x\mid h))$, which is the change in amount of hypothesis weight (posterior mass) that shifted onto $h$.&lt;/p&gt;
&lt;p&gt;$\lg (1/p_{X \mid H}(x \mid h))$ is the &lt;strong&gt;surprise&lt;/strong&gt; (also &lt;strong&gt;self-information&lt;/strong&gt;) of observing $x$ under $h$. The higher this quantity, the more the possibility space of $h$ was narrowed down by $x$.&lt;/p&gt;
&lt;h2 id=&#34;variational-bayes&#34;&gt;Variational Bayes&lt;/h2&gt;
&lt;p&gt;Variational approximation to calculating the Bayesian posterior:&lt;br&gt;
&lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;The free-energy principle a unified brain theory#free-energy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&#34;&gt;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1901.07945&#34;target=&#34;_blank&#34;&gt;What does the free energy principle tell us about the brain?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose $x\in\mc{X}$ is observed and the posterior $p_{H \mid X=x}$ is intractable to compute. We can instead approximate it by minimizing&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
q_x^* = \argmin{q \in \mc{Q}} \kl{q}{p_{H \mid X=x}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\mc{Q}$ is some set of probability mass functions $q : \mc{H} \to [0, 1]$, chosen for convenience.&lt;/p&gt;
&lt;p&gt;Assuming we cannot perform this minimization directly, we can make use of the identity&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\kl{q}{p_{H \mid X=x}} = \mc{F}[q] - \lg (1/p_X(x))&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= \kl{q}{p_{H,X=x}} \\&lt;br&gt;
&amp;amp;= \E_{h \sim q} \lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right) \\&lt;br&gt;
&amp;amp;= \sum_{h\in\mc{H}} q(h) \lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right)&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is the &lt;strong&gt;free energy&lt;/strong&gt;. $\lg (1/p_X(x))$ is the expected surprise of $x$ across all hidden states (weighted by the prior $p_H$).&lt;/p&gt;
&lt;p&gt;Free energy also equals&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= H(q, p_{H, X=x}) - H(q) \\&lt;br&gt;
&amp;amp;= \E_{h \sim q}\left[\lg\left(\frac{1}{p_{H, X}(h,x)}\right) - \lg\left(\frac{1}{q(h)}\right) \right] \\&lt;br&gt;
&amp;amp;= \sum_{h \in \mc{H}} q(h) \lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right)\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $H(q)$ is the entropy of $q$, and $H(q, p_{H, X=x})$ is the &lt;strong&gt;total energy&lt;/strong&gt;, which is equal to the cross-product of $p_{H, X=x}$ under $q$.&lt;/p&gt;
&lt;p&gt;Note that $p_{H, X=x}(h) = p_{H,X}(h,x)$ is not the same as the conditional distribution $p_{H \mid X=x}(h) = p_{H,X}(h,x) / p_X(x)$, and is not a valid probability distribution because its unnormalized.&lt;/p&gt;
&lt;p&gt;We also have free energy as &lt;strong&gt;complexity&lt;/strong&gt; minus &lt;strong&gt;accuracy&lt;/strong&gt;,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= \kl{q}{p_H} - \E_{h\sim q}\left[\lg p_{X \mid H}(x \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q}\left[\lg\left(\frac{q(h)}{p_H(h)}\right)-\lg p_{X \mid H}(x \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q}\left[\lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right)\right]\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This form of free energy can be used in practice. Given any particular $q$ (e.g. as a neural network), the complexity $\kl{q}{p_H}$ and the accuracy $\E_{h\sim q}\left[\lg p_{X \mid H}(x \mid h)\right]$ can be approximated using Monte-Carlo sampling from $q$. This is assuming we have access to a prior $p_H$ over hidden states and predictive (or generative) distribution $p_{X\mid H}$. If $p_{H\mid X}$ is intractable, then a suitable $q^*$ that approximately and sufficiently minimizes $\mc{F}[q]$ becomes our approximation of that posterior.&lt;/p&gt;
&lt;p&gt;Note that there is a $q^*$ for every partial observation $x_{1:t}$, i.e. we need to perform another minimization to arrive at $q_{x_{1:t}}^*$ for every $t$.&lt;/p&gt;
&lt;h2 id=&#34;bayesian-inference-over-time&#34;&gt;Bayesian inference over time&lt;/h2&gt;
&lt;p&gt;I am basing this on Solomonoff induction (as formulated by Marcus Hutter in his &lt;a href=&#34;http://www.hutter1.net/ai/uaibook.htm&#34;target=&#34;_blank&#34;&gt;book&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We now suppose the agent observes an endless stream of data over time. The full possible set of observations are all infinite sequences $\mc{X}^\infty$ where $\mc{X}$ is the set of possible observations at each point in time, e.g. a frame of sensory data such as a video or audio frame. Let $X_{a:b}$ be the random variable denoting a slice of the data stream from time $a$ to time $b$  (inclusive). $X_{1:n}$ is the first $n$ timesteps of data, and $X_{n+1:\infty}$ is everything that is observed after time $n$. I will also use the shorthands $X_{&amp;lt;n} = X_{1:n-1}$ and $X_{&amp;gt;n} = X_{n+1:\infty}$.&lt;/p&gt;
&lt;p&gt;Now suppose the agent has a &lt;strong&gt;hypothesis space&lt;/strong&gt; $\mc{M}$, which is a set of probability distributions $\mu\in\mc{M}$. We call each $\mu$ a hypothesis. Let $\pi$ be a probability distribution over $\mc{M}$ (the prior). Then we have a mixture distribution&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(X_{1:n}) = \sum_{\mu\in\mc{M}} \pi(\mu)\cdot\mu(X_{1:n})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If we define the joint distribution&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(X_{1:n},\mu) = \pi(\mu)\cdot\mu(X_{1:n})\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;then we have the usual Bayesian quantities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data likelihood: $p(X_{1:n} \mid \mu) = \mu(X_{1:n})$.&lt;/li&gt;
&lt;li&gt;Hypothesis prior: $p(\mu) = \pi(\mu)$.&lt;/li&gt;
&lt;li&gt;Hypothesis posterior: $p(\mu \mid X_{1:n}) = \pi(\mu)\cdot\mu(X_{1:n})/p(X_{1:n})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can think of a finite data sequence $x_{1:n} \in \mc{X}^n$ as a partial observation that the agent updates its mixture weights on:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
w_\mu(x_{1:n}) = \pi(\mu)\frac{\mu(x_{1:n})}{p(x_{1:n})}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where the hypothesis posterior is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(\mu \mid x_{1:n}) = w_\mu(x_{1:n})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We also have a new quantity, the &lt;strong&gt;data posterior&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(X_{&amp;gt;n} \mid X_{1:n}=x_{1:n}) = \sum_{\mu\in\mc{M}} w_\mu(x_{1:n})\mu(X_{&amp;gt;n} \mid X_{1:n}=x_{1:n})\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which has the same form as the prior mixture, except that we reweighted by switching from $\pi(\mu)$ to $w_\mu(x_{1:n})$, and we conditionalized the hypotheses, i.e. switched from $\mu(X_{1:\infty})$ to $\mu(X_{&amp;gt;n} \mid X_{1:n}=x_{1:n})$.&lt;/p&gt;
&lt;p&gt;We can incorporate actions into this framework by specifying that all hypotheses in $\mu\in\mc{M}$ must be distributions over a combined observation and action stream. This stream would be a sequence $(x_1, a_1, x_2, a_2, \ldots)$ of alternating observation $x_t \in \mc{X}$ and action $a_t \in \mc{A}$ at every time step. Note that a hypothesis predicts the next observation $\mu(x_t \mid x_{&amp;lt;t}, a_{&amp;lt;t})$, but we don&amp;rsquo;t ask a hypothesis to predict a next action, i.e. $a_t$ given $x_{&amp;lt;t}, a_{&amp;lt;t}$, since that is the agent&amp;rsquo;s decision to make.&lt;/p&gt;
&lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;Free energy principle and Solomonoff induction&lt;/a&gt;
&lt;h2 id=&#34;hypotheses-vs-states-policies-vs-actions&#34;&gt;Hypotheses vs states; policies vs actions&lt;/h2&gt;
&lt;p&gt;There is a key distinction to make here: a hypothesis $\mu$ is itself a possible universe. $\mu$ is a distribution over all possible infinite data streams $\mc{X}^\infty$. $\mu$ can be arbitrarily complex, and consider all counterfactual latent states in the universe. $\mu$ may encode the dynamics of a time evolving system in its conditional probabilities $\mu(x_n \mid x_{&amp;lt;n})$. In this way $\mu$ is not a hidden state, but an entire possible universe.&lt;/p&gt;
&lt;p&gt;This is in contrast to the idea of a &lt;strong&gt;hidden state&lt;/strong&gt;, which is an unknown state of the universe &lt;strong&gt;at some point in time&lt;/strong&gt;. In the hidden state framework, the environment is defined by a known dynamics distribution $p(x_t, s_t \mid s_{t-1})$ or $p(x_t, s_t \mid s_{t-1}, a_{t-1})$ if we include actions. The only thing that is unknown is $s_{1:t}$. In this framework, knowing $s_{t-1}$ does not mean you know $s_t$ with certainty. In the mixture of hypotheses framework, if you know $\mu$, you know it for all time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: A hypothesis $\mu$ is static and true for all time, and a hidden state $s_t$ evolves and is true only at time $t$.&lt;/p&gt;
&lt;p&gt;We also need to make this distinction between policies and actions. A &lt;strong&gt;policy&lt;/strong&gt; $\pi$ (not to be confused with hypothesis prior above, but this is the conventional notation) is similar to $\mu$, in that it is a probability distribution over alternating observations and actions. $\pi(a_t \mid x_{&amp;lt;t}, a_{&amp;lt;t})$ specifies the agent&amp;rsquo;s action preferences given what it has already seen and done. If we combine an environment $\mu$ and a policy $\pi$, we can fully model the agent-environment interaction loop, i.e. the joint distribution over the space of combined sequences: $\mc{X}\times\mc{A}\times\mc{X}\times\mc{A}\times\ldots$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: A policy $\pi$ is static, i.e. a single agent choice that remains for all time, and an action $a_t$ is a choice made specifically at time $t$.&lt;/p&gt;
&lt;p&gt;However, it is possible to devise a setup where there is a set of possible policies $\Pi$, and the agent keeps &amp;ldquo;changing its mind&amp;rdquo; about which policy $\pi_t \in \Pi$ to use at time $t$. I find this formulation to be a bit redundant, because $\pi_t$ contains information about the agent&amp;rsquo;s preferences in all possible future situations, but if the agent changes its mind in the next step then that information is essentially overridden. Why not just have the agent choose an action $a_t$? It could make sense to impose a restriction that $\pi_t$ cannot evolve quickly over time, so that the policy represents a high-level choice about what to do in some time window. This is one avenue for formulating hierarchical decision making.&lt;/p&gt;
&lt;h1 id=&#34;free-energy-principle-and-time&#34;&gt;Free energy principle and time&lt;/h1&gt;
&lt;p&gt;This is where my understanding falls apart.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve reviewed two sources:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&#34;&gt;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;1-wikipedia&#34;&gt;1. Wikipedia&lt;/h2&gt;
&lt;p&gt;From the first (Wikipedia):&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210124131932.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
I&amp;rsquo;m reiterating Wikipedia&amp;rsquo;s notation here. Overwrite in your brain my usages of $\mu$ and $s$ above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mu\in R$ is the state of the agent at timestep (not to be confused with hypotheses).&lt;/li&gt;
&lt;li&gt;$a \in A$ is an action taken at every timestep.&lt;/li&gt;
&lt;li&gt;$s \in S$ is an observation at each timestep (not to be confused with environment states).&lt;/li&gt;
&lt;li&gt;$\psi \in \Psi$ is the hidden environment state at every timestep.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is the Bayesian inference being done here? In my exposition on Bayesian inference over time, the posteriors of interest are explicitly given. I&amp;rsquo;d like to know what posterior we are interested in approximating with variational free energy here.&lt;/li&gt;
&lt;li&gt;Is this joint minimization being done simultaneously over all timesteps, or is it done in a greedy fashion on every step?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;2-step-by-step-tutorial&#34;&gt;2. Step-by-Step Tutorial&lt;/h2&gt;
&lt;p&gt;From the second: &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The paper opens with an exposition that looks like it matches my own for time-less free energy minimization.&lt;/p&gt;
&lt;p&gt;Starting on page 16, we introduce policy $\pi$ (not to be confused with the prior). I&amp;rsquo;m confused about the relationship between the policy and the time evolution of the environment-agent loop. Does $\pi$ change over time, or is $\pi$ chosen up front and held fixed? Clearly it cannot be held fixed, because then the agent is not utilizing its free energy minimization to alter its behavior.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m also confused about the posterior approximation $q$. Now, $q(s \mid \pi)$ depends on the policy. Does this mean we run the free energy minimization for every $\pi$, each producing a different $q$? If that&amp;rsquo;s so, then why do we not write $q(s_t \mid o_{&amp;lt;t}, \pi)$ to indicate the dependency of $q$ on the observations $o_{&amp;lt;t}$ as well?&lt;/p&gt;
&lt;p&gt;Page 19 adds more confusion to the mix. We are introduced to a score function $G(\pi)$ for choosing policies $\pi$.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210124142608.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
First of all, I thought there is a joint distribution $p(o,s,\pi)$, implying that $p(\pi)$ is a prior which reflects the agent&amp;rsquo;s preferences over policies. So which is it? Does the agent use $G(\pi)$ or $p(\pi)$ to choose its policy? It&amp;rsquo;s also not clear if $\pi$ is chosen once and held fixed for all time, or if the policy changes over time.&lt;/p&gt;
&lt;p&gt;Second, and more perplexing, is that $G(\pi)$ is an expectation over $q(o,s\mid \pi)$, remember that $q$ is an approximate posterior over hidden states $s$. How can $q$ also be a distribution over observations $o$? Trying to make $q$ a joint distribution over $x$ and $h$ in my time-less free energy exposition above doesn&amp;rsquo;t make sense to me.&lt;/p&gt;
&lt;h1 id=&#34;my-open-questions&#34;&gt;My Open Questions&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Is the agent assumed to have a dynamics model of the environment where all that&amp;rsquo;s unknown to the agent is the environment state? that seems unrealistic. if the agent doesn&amp;rsquo;t know the &amp;ldquo;true&amp;rdquo; dynamics model, by what mechanism would the agent improve its dynamics model? The Bayesian posterior approximation is for estimating the effect of its actions on environment state, but this doesn&amp;rsquo;t address how the agent learns about the relationship between action and state.&lt;/li&gt;
&lt;li&gt;How are time and actions incorporated? I understand the time-less variational free energy formulation that I explained above. What I don&amp;rsquo;t understand is what this looks like when applied to an agent-environment interaction loop over time.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>