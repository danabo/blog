<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>personal on Dan&#39;s Notepad</title>
    <link>https://danabo.github.io/blog/tags/personal/</link>
    <description>Recent content in personal on Dan&#39;s Notepad</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>©2021 Daniel Abolafia.</copyright>
    <lastBuildDate>Sat, 06 Feb 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://danabo.github.io/blog/tags/personal/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Shannon vs Universal Compression</title>
      <link>https://danabo.github.io/blog/posts/shannon-vs-universal-compression/</link>
      <pubDate>Thu, 16 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/shannon-vs-universal-compression/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\Z}{\mb{Z}}&lt;br&gt;
\newcommand{\N}{\mb{N}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\g}{\gamma}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\brac}[1]{\left[#1\right]}&lt;br&gt;
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\real}[1]{#1^{(\R)}}&lt;br&gt;
\newcommand{\bin}[1]{#1^{\par{\B^\infty}}}&lt;br&gt;
\newcommand{\binn}[1]{#1^{\par{\B^n}}}&lt;br&gt;
\newcommand{\cyl}[1]{{\overbracket[0.5pt]{\underbracket[0.5pt]{#1}}}}&lt;br&gt;
\newcommand{\int}[2]{\left[#1,\,\,#2\right)}&lt;br&gt;
\newcommand{\len}[1]{\abs{#1}}&lt;br&gt;
\newcommand{\Mid}{\,\middle\vert\,}&lt;br&gt;
\DeclareMathOperator*{\argmin}{argmin}&lt;br&gt;
\DeclareMathOperator*{\argmax}{argmax}&lt;br&gt;
\newcommand{\up}[1]{^{\par{#1}}}&lt;br&gt;
\newcommand{\Km}{Km}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/book/10.1007/978-0-387-49820-1&#34;target=&#34;_blank&#34;&gt;An Introduction to Kolmogorov Complexity and Its Applications&lt;/a&gt; - Li &amp;amp; Vitanyi 2008 (L&amp;amp;V)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf&#34;target=&#34;_blank&#34;&gt;Elements of Information Theory&lt;/a&gt; - Cover &amp;amp; Thomas 2nd ed. 2006 (C&amp;amp;T)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf&#34;target=&#34;_blank&#34;&gt;A Mathematical Theory of Communication&lt;/a&gt; - Claude Shannon 1948 (Shannon)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AIT = Algorithmic Information Theory&lt;/p&gt;
&lt;h1 id=&#34;notation&#34;&gt;Notation&lt;/h1&gt;
&lt;p&gt;Let $\X$ be a finite set of symbols called an alphabet.&lt;br&gt;
Let $\B = \set{0,1}$ be the binary alphabet.&lt;/p&gt;
&lt;p&gt;$\X^\infty$ is the set of all infinite sequences, and&lt;/p&gt;
&lt;p&gt;$$\X^* = \X^0 \cup \X^1 \cup \X^2 \cup \X^3 \cup \dots$$&lt;br&gt;
is the set of all finite sequences of any length.&lt;/p&gt;
&lt;p&gt;For $\o \in \X^*$ or $\o\in\X^\infty$,&lt;br&gt;
$\o_i$ denotes the $i$-th symbol in the sequence $\o=(\o_1,\o_2,\o_3,\dots)$.&lt;br&gt;
$\o_{i:j} = \o_{(i,i+1,\dots,j-1,j)}$ denotes the slice of $\o$ starting from $i$ and ending at $j$ (inclusive).&lt;br&gt;
When $\o$ is infinite, $\o_{i:\infty} = (\o_i,\o_{i+1},\o_{i+2},\dots)$ denotes the infinite sequence starting from position $i$.&lt;br&gt;
$\o_{&amp;lt;i} = \o_{1:i-1}$ denotes the slice up to position $i-1$.&lt;br&gt;
$\o_{\leq i} = \o_{1:i}$ includes $i$. We say that $\o_{\leq i}$ is a &lt;strong&gt;prefix&lt;/strong&gt; of $\o$.&lt;br&gt;
$\o_{&amp;gt;i} = \o_{i+1:\infty}$ denotes the unbounded slice starting at position $i+1$.&lt;/p&gt;
&lt;p&gt;Let $\ell(x)$ denote the length of the finite sequence $x\in\X^*$.&lt;/p&gt;
&lt;p&gt;For $x\in\B^*$ and $y \in \B^*\cup\B^\infty$,&lt;br&gt;
$x \sqsubset y$ means &amp;ldquo;$x$ is a prefix of $y$&amp;rdquo;, and&lt;br&gt;
$x \sqsubseteq y$ means &amp;ldquo;$x$ is a prefix of or equal to $y$&amp;rdquo;.&lt;/p&gt;
&lt;h1 id=&#34;ait-for-infinite-sequences&#34;&gt;AIT for Infinite Sequences&lt;/h1&gt;
&lt;h2 id=&#34;monotone-machines&#34;&gt;Monotone Machines&lt;/h2&gt;
&lt;p&gt;From L&amp;amp;V chapter 4.5, &amp;ldquo;Continuous Sample Space&amp;rdquo;, definition 4.5.2 gives us monotone machines. These are Turing machines equipped with three tapes: the usual read/write work tape, a read-only one-way (can only move the head to the right) input tape, and a write-only one-way output tape.&lt;/p&gt;
&lt;p&gt;Let $M$ be a monotone Turing machine. We can overload this object $M$ to act like a function with signature $M:\B^*\to\B^*$, where for any $x\in\B^*$, we define $M(x)\in\B^*$ to be the longest prefix written to the output tape (the write head position defines the output length) before the input tape head has read past $x$ in some hypothetical infinite input (e.g. $x00000\dots$).&lt;/p&gt;
&lt;p&gt;Such machines $M$ satisfy the monotone property:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
x\sqsubseteq y \implies M(x) \sqsubseteq M(y)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for $x,y\in\B^*$.&lt;/p&gt;
&lt;p&gt;$M$ induces a function $f:\B^\infty\to\B^\infty$. Given $\o\in\B^\infty$, if for all $n\in\N$ there exists an $m\in\N$ s.t.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
f(\o)_{1:n} \sqsubseteq M(\o_{1:m})&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;then every position of the output $f(\o)$ is defined. Note that this need not be the case since $M$ may go into an infinite loop and never output beyond some finite output position. For $\o\in\B^\infty$, let&amp;rsquo;s overload $M$ so that we can write $M(\o) = f(\o)\in\B^\infty$ whenever $f(\o)$ is defined.&lt;/p&gt;
&lt;h2 id=&#34;semimeasures&#34;&gt;Semimeasures&lt;/h2&gt;
&lt;p&gt;Let $\l$ be the uniform measure on $\B^\infty$, and let $\mu = \l_M$ be the measure on $\B^\infty$ induced by monotone machine $M$ reading an input sequence drawn from $\l$. That is to say, when $M$ is fed as input a uniformly random sequence, the induced distribution over output sequences from $M$ is $\mu$.&lt;/p&gt;
&lt;p&gt;$\mu$ is a &lt;strong&gt;semimeasure&lt;/strong&gt; if&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x0) + \mu(x1) \leq \mu(x)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\mu$ is a &lt;strong&gt;measure&lt;/strong&gt; if $\mu(x0) + \mu(x1) = \mu(x)$ for all $x\in\B^*$.&lt;/p&gt;
&lt;p&gt;We further require that $\mu$ be unit normalized (making it a probability semimeasure), i.e. $\mu\big(()\big) \leq 1$ where $()$ is the empty sequence (with equality when $\mu$ is a measure).&lt;/p&gt;
&lt;p&gt;$\mu$ is &lt;strong&gt;semicomputable&lt;/strong&gt; if there exists a Turing machine $\phi(x, k)$ of two arguments, $x\in\B^*$ and $k\in\N$, outputting a rational number, s.t.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lim_{k\to\infty} \phi(x, k) = \mu(x)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and $\phi(x, k)$ is monotonic in $k$.  $\mu$ is called &lt;strong&gt;lower semicomputable&lt;/strong&gt; if $\phi$ is increasing in $k$, and &lt;strong&gt;upper semicomputable&lt;/strong&gt; if $\phi$ is decreasing in $k$.&lt;/p&gt;
&lt;p&gt;$\mu$ is &lt;strong&gt;computable&lt;/strong&gt; iff there exists such a $\phi$ which is increasing in $k$, and another such $\phi&#39;$ which is decreasing in $k$. Equivalently, $\mu$ is &lt;strong&gt;computable&lt;/strong&gt; iff there exists a Turing machine $\psi(x, \e)$ with $\e\in\mb{Q}$ s.t. $\abs{\psi(x, \e) - \mu(x)} &amp;lt; \e$ for all $\e$ and&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lim_{\e \to 0} \psi(x, \e) = \mu(x)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;All measures are semimeasures, and all computable functions are semicomputable.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;L&amp;amp;V Lemma 4.5.1:&lt;br&gt;
If a continuous lower semicomputable semimeasure is a measure, it is computable.&lt;/p&gt;
&lt;p&gt;L&amp;amp;V Theorem 4.5.2:&lt;br&gt;
A semimeasure $\mu$ is lower semicomputable if and only if there is a monotone Turing machine $M$ such that $\mu = \l_M$.&lt;/p&gt;
&lt;p&gt;L&amp;amp;V Definition 4.5.5:&lt;br&gt;
A monotone Turing machine $M$ is $\mu$-regular if the set of sequences $\o \in\B^\infty$ for which $M(\o)$ is defined has $\mu$-measure one.&lt;/p&gt;
&lt;p&gt;L&amp;amp;V Lemma 4.5.4:&lt;br&gt;
(ii) For each computable measure $\mu$ there exists a $\l$-regular monotone machine $M$ such that $\l_M = \mu$; moreover, there is a $\mu$-regular monotone machine $E$ such that $\mu_E = \l$ and $E$ is the inverse of $M$ in the domain of definition of $EM$, and $E(\o) \in\B^\infty$ for all $\o\in\B^\infty$, except for possibly the computable $\o$′s and $\o$’s lying in countably many intervals of $\mu$-measure $0$.&lt;/p&gt;
&lt;p&gt;Note that given a computable measure $\mu$, we can explicitly construct $M$ and $E$ s.t. $\mu = \l_M$ and $\l = \mu_E$ with $E$ being the inverse of $M$. Just let $M$ be the arithmetic decoder and $E$ be the arithmetic encoder for $\mu$.&lt;/p&gt;
&lt;p&gt;Call $E$ the &lt;strong&gt;encoder&lt;/strong&gt; for $\mu$ and call $M$ the &lt;strong&gt;decoder&lt;/strong&gt; for $\mu$.&lt;/p&gt;
&lt;h2 id=&#34;universal-elements&#34;&gt;Universal Elements&lt;/h2&gt;
&lt;p&gt;L&amp;amp;V Definition 4.5.1:&lt;br&gt;
A semimeasure $\mu_0$ is universal (or maximal) for a set of semimeasures $\mc{M}$ if $\mu_0 \in \mc{M}$, and for all $\mu \in \mc{M}$, there exists a constant $c &amp;gt; 0$ such that for all $x\in\B^*$, we have $\mu_0(x) \geq c\mu(x)$ (we say that $\mu_0$ &lt;strong&gt;dominates&lt;/strong&gt; $\mu$).&lt;/p&gt;
&lt;p&gt;We are then told that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the set of computable semimeasures has no universal element&lt;/li&gt;
&lt;li&gt;the set of semicomputable semimeasures has a universal element (actually infinitely many of them), which I&amp;rsquo;ll denote as $\xi$&lt;/li&gt;
&lt;li&gt;$\xi$ is necessarily not computable and not a measure (but it is a semicomputable semimeasure)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We also have that $\xi(x) \geq 2^{−K(\mu)}\mu(x)$ for all semicomputable semimeasures $\mu$ and all $x\in\B^*$. This implies that $\xi$ is a mixture over all semicomputable semimeasures, which is the Solomonoff mixture.&lt;/p&gt;
&lt;p&gt;A monotone Turing machine $U$ is universal if it can emulate any monotone Turing machine (e.g. there could be a prefix free code for monotone Turing machines which, when fed into $U$, causes $U$ to act as the machine encoded from then on out). There are infinitely many universal monotone Turing machines.&lt;/p&gt;
&lt;p&gt;From L&amp;amp;V Definition 4.5.6, we have that when $U$ is a universal monotone Turing machine, then $\xi = \l_U$ is a universal semimeasure.&lt;br&gt;
I.e. $\xi$ is a universal semimeasure iff $U$ is a universal monotone Turing machine (is this an &amp;ldquo;iff&amp;rdquo; relationship?).&lt;/p&gt;
&lt;p&gt;We have an invariance result (analogous to the invariance theorems for the various Kolmogorov complexities):&lt;br&gt;
For any two universal semimeasures $\xi$ and $\xi&#39;$, there exists a constant $c&amp;gt;0$ s.t.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lg 1/\xi(x) \leq \lg 1/\xi&#39;(x) + c&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $x\in\B^*$. This follows directly from the fact that universal semimeasures dominate all semimeasures (and so domination is mutual between universal semimeasures).&lt;/p&gt;
&lt;h2 id=&#34;a-note-about-semimeasures&#34;&gt;A Note About Semimeasures&lt;/h2&gt;
&lt;p&gt;The semimeasure construction can seem esoteric and getting away from anything that was originally meant by probability.&lt;/p&gt;
&lt;p&gt;If we think of a (semi)measure $\mu$ as defined by the act of uniformly drawing an input sequence to a monotone decoder $M$, then really all a semimeasure does is account for the possibility that $M$ goes into an infinite loop while not emitting anything further. That is to say, $\mu(x0) + \mu(x1) \leq \mu(x)$ is hiding the third outcome - that $x$ is the end of a finite length output from $M$ (though we can never know that this is the case because $M$ will never halt). We could instead define $\mu$ s.t. $\mu(x0) + \mu(x1) + \mu(x\emptyset) = \mu(x)$ where $\emptyset$ is a special token denoting that there is no more output from $M$ after $x$ (i.e. the write-tape head never moves beyond position $\ell(x)$ )&lt;/p&gt;
&lt;p&gt;Then we see that if $\mu$ is a semimeasure, we should interpret $\mu(x)$ as the probability of seeing $x$ as the output of $M$ given a uniform random input, where some of the probability mass went to finite sequences shorter than $x$ where $M$ gets stuck.&lt;/p&gt;
&lt;h1 id=&#34;shannon-compression&#34;&gt;Shannon Compression&lt;/h1&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf&#34;target=&#34;_blank&#34;&gt;A Mathematical Theory of Communication&lt;/a&gt;, Claude Shannon proved a result about optimal compression, called the noiseless coding theorem (theorem 9 in the paper). I&amp;rsquo;ll restate the result breifly.&lt;/p&gt;
&lt;p&gt;The setup is that we have a discrete set of symbols, $\X$ (called our alphabet), and we wish to losslessly compress a sequence of data $x_1,x_2,x_3,\dots \in \X$ in order to transmit it over a wire (and the receiver should be able to fully recover the data sequence). Shannon supposes that our data is drawn from a stochastic process, i.e. the data generating process (DGP), defined by a sequence of random variables $X_1,X_2,X_3,\dots$ (where each $n$-th RV is conditioned on the previously drawn outcomes). Another way to define this is to say that we have an infinite sequence of conditional distributions, $x_1\sim p(X_1), x_2\sim p(X_2\mid x_1), x_3\sim p(X_3\mid x_1,x_2),\dots$&lt;br&gt;
If we multiply these conditional distributions together, we get (in the limit), a distribution $p(X_1,X_2,X_3,\dots)$ over infinite data sequences, $\X^\infty$.&lt;/p&gt;
&lt;p&gt;Shannon proposes that we model our DGP as a Markov process. That is to say, we restrict the form of our distribution $p$ so that $p(X_n \mid x_{1:n-1}) = p(X\mid x_{n-k-1:n-1})$ for some $k$, where this distribution is the same at every step $n$ (this is the Markov property), and so $X$ without the subscript is a non-step-specific RV. Shannon demonstrates with English text how we can estimate the $k$-step transition probabilities $p(X \mid x_{1:k})$ from some corpus of data.&lt;/p&gt;
&lt;p&gt;Shannon then supposes that our compression and decompression functions (which he calls transducers) are deterministic finite state machines (FSMs), where each transition is assigned a finite input sequence and a finite output sequence (reading in the target input sequence causes the transition to be taken and for the target output sequence to be emitted). The FSM class of encoders/decoders keeps us within the Markov class of data distributions, since an FSM operating on a Markov process produces a Markov process. (note that i.i.d. data is a special case of the Markov class)&lt;/p&gt;
&lt;p&gt;Shannon further supposes the DGP is an ergodic process, since then asymptotic quantities are well defined. Importantly, we are interested in finding a $\X$ to binary FSM encoder $E$ (each transition has the type $\X^*\to\B^*$) that minimizes the long-run (asymptotic) average number of bits per symbol (in expectation).&lt;/p&gt;
&lt;p&gt;Shannon derives a special quantity $H(p)$ for Markov processes $p$, with units &amp;ldquo;bits per symbol&amp;rdquo;. He calls this quantity entropy after a &lt;a href=&#34;http://www.scholarpedia.org/article/Quantum_entropies&#34;target=&#34;_blank&#34;&gt;suggestion by John von Neumann&lt;/a&gt; that its definition looks awfully similar to thermodynamic entropy. Regardless of its name, Shannon then proves that no FSM encoder can do better than $H(p)$ output bits per input symbol in expectation and in the limit.&lt;/p&gt;
&lt;h2 id=&#34;noiseless-coding-theorem&#34;&gt;Noiseless Coding Theorem&lt;/h2&gt;
&lt;p&gt;Shannon Theorem 9:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let a source have entropy $H$ (bits per symbol) and a channel have a capacity $C$ (bits per second). Then it is possible to encode the output of the source in such a way as to transmit at the average rate $C/H-\e$ symbols per second over the channel where $\e$ is arbitrarily small. It is not possible to transmit at an average rate greater than $C/H$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In our discussion I&amp;rsquo;m assuming the channel is a noiseless binary channel which transmits at 1 bit per time, so that $C=1$.  Then $C/H(p)=1/H(p)$ symbols per time. Because we are assuming 1 bit per time then $1/H(p)$ also has the unit symbols per bit. This theorem then reduces to the following two statements:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$H(p)$ is a lower bound on an &lt;em&gt;average&lt;/em&gt; transmission rate (bits per symbol, which we wish to minimize).&lt;/li&gt;
&lt;li&gt;This lower bound can be realized arbitrarily closely in practice.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The word &amp;ldquo;average&amp;rdquo; is key here. The two statements above are not given in precise mathematical terms, but we can infer from the proof what &amp;ldquo;average&amp;rdquo; should mean formally. If we draw a finite sequence $x_{1:n}$ from our DGP, then a very good encoder $E$ (which gets close to the lower bound of $H(p)$) may not do a good job at compressing $x_{1:n}$ if we&amp;rsquo;ve gotten unlucky (the data is not typical for the DGP). However, in an analog to the strong law of large numbers for ergodic processes, we can say that&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lim_{n\to\infty}\frac{1}{n}\ell(E(X_{1:n})) = \hat{R}_E&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;with $p$-probability 1. That is to say, in the long run, our encoder $E$&amp;rsquo;s compression rate will converge some number $\hat{R}_E$ bits per symbol with probability approaching 1 (as $n\to\infty$).&lt;/p&gt;
&lt;p&gt;$\hat{R}_E$ is of course a property of our choice of encoder $E$. Theorem 9 then tells us that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\hat{R}_E \geq H(p)$.&lt;/li&gt;
&lt;li&gt;For every $\e &amp;gt; 0$, there exists an $E$ s.t. $\hat{R}_E - H(p) &amp;lt; \e$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can have $\hat{R}_E = H(p)$ iff $p$ is &lt;a href=&#34;https://en.wikipedia.org/wiki/Dyadic_distribution&#34;target=&#34;_blank&#34;&gt;dyadic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that while Shannon proposes a way to realize these $E$ in practice that get arbitrarily close to $H(p)$ (&lt;a href=&#34;https://en.wikipedia.org/wiki/Shannon_coding&#34;target=&#34;_blank&#34;&gt;Shannon coding&lt;/a&gt;), his proposal has been subsequently improved. &lt;a href=&#34;https://en.wikipedia.org/wiki/Huffman_coding&#34;target=&#34;_blank&#34;&gt;Huffman coding&lt;/a&gt; has been proven to be the best encoder theoretically possible for i.i.d. DGPs. Shannon&amp;rsquo;s way of minimizing the gap $\hat{R}_E - H(p)$ is to chunk the data into chunk sizes $k$, and then perform Shannon/Huffman coding on those chunks. However, this is quickly intractable for large $k$. A more modern solution would be to perform &lt;a href=&#34;https://en.wikipedia.org/wiki/Arithmetic_coding&#34;target=&#34;_blank&#34;&gt;arithmetic coding&lt;/a&gt; (this is what is actually done today). However, arithmetic encoders/decoders are necessarily not FSMs, and so technically fall outside of the purview of Shannon&amp;rsquo;s result (they are, however, monotonic Turning machines - more on that below).&lt;/p&gt;
&lt;p&gt;It is worth mentioning that the self-information, $-\lg p(x_{n} \mid x_{1:n-1})$, of a symbol $x_n$ has no inherent meaning here. As it is, Huffman coding may assign fewer bits than $-\lg p(x_{n} \mid x_{1:n-1})$ to $x_n$ in some situations (e.g. consider the Huffman coding for an i.i.d. Bernoulli($\theta$) process with $\theta\neq 1/2$. In that case. Huffman would assign 1 bit to each outcome, which is optimal, rather than $\lg 1/\theta$ and $\lg 1/(1-\theta)$ bits respectively).&lt;/p&gt;
&lt;h2 id=&#34;what-is-optimal-compression&#34;&gt;What is Optimal Compression?&lt;/h2&gt;
&lt;p&gt;Something that might be a bit surprising is that Shannon&amp;rsquo;s optimality result is only true in fairly limited circumstances (limited class of DGPs and limited class of encoders/decoders). This begs the question, can one actually perform even better compression in more general circumstances? Given that English text (and almost any real-world time-series data sources) are not Markov, can they be compressed better than Shannon prescribes?&lt;/p&gt;
&lt;p&gt;Obviously yes, in some trivial sense. We could compress any data into a message of 1 bit, if we know what the data is before hand (the sender and receiver both know what the data is).&lt;/p&gt;
&lt;p&gt;Clearly, the problem needs to be made well posed before we can solve it. We will see how this can be done in the AIT treatment below. By defining &lt;em&gt;universal&lt;/em&gt; compression (programs as compression), we get around this problem. Then the question is, using universal compression, can be do better than Shannon prescribes? Turns out we can generalize the noiseless coding theorem to the case where the data is drawn from a (semi)computable (semi)measure, and where we consider all monotone Turing machine encoders/decoders.&lt;/p&gt;
&lt;h2 id=&#34;restatement-under-ait&#34;&gt;Restatement Under AIT&lt;/h2&gt;
&lt;p&gt;It may be helpful in comparing to AIT to recast Shannon&amp;rsquo;s noiseless coding theorem from the perpsective of AIT - that is to say, make it a statement about an individual data sequence rather than a probabilistic statement (even if its a statement with probability 1). How can this be done? By using Martin-Lof randomness.&lt;/p&gt;
&lt;p&gt;The pointwise noiseless coding theorem:&lt;/p&gt;
&lt;p&gt;For an ergodic Markov measure $\mu$ on $\X^\infty$,&lt;br&gt;
if $\o\in\X^\infty$ is $\mu$-typical (Martin-Lof $\mu$-random), then&lt;br&gt;
$$\lim_{n\to\infty}\frac{1}{n}\ell(E(\o_{1:n})) = \hat{R}_E$$&lt;/p&gt;
&lt;p&gt;This directly follows from the fact that $\o$ is $\mu$-typical iff all constructive statements that are true with $\mu$-probability 1 are true of $\o$. I formulated the noiseless coding theorem above as a statement with probability 1, and so it holds for a particular $\mu$-typical sequence $\o$.&lt;/p&gt;
&lt;h1 id=&#34;optimal-compression&#34;&gt;Optimal compression&lt;/h1&gt;
&lt;p&gt;The general case is that we are receiving a stream of data, which we can model as infinite.&lt;/p&gt;
&lt;h2 id=&#34;infinite-vs-finite-data&#34;&gt;Infinite vs Finite Data&lt;/h2&gt;
&lt;p&gt;Why infinite sequences? We could model the data as an element of $\B^*$, the finite binary sequences. But then we need to also encode the sequence length once it ends. This becomes a different sort of problem than compression of infinite sequences.&lt;/p&gt;
&lt;p&gt;In practice, we are compressing finite chunks of data, so this fits. But then this is no longer online/monotonic compression. If you compress a finite sequence of length $n$, and then receive a continuation bringing it to length $n&#39;$, the part of the compression which stores the length needs to be retroactively changed to bring in the new data.&lt;/p&gt;
&lt;p&gt;If that&amp;rsquo;s not a big deal in practice, then I would argue that is because we are actually treating it more like the case of infinite sequences, but where we have a mutable length value off to the side to store the length at intermediate points in time.&lt;/p&gt;
&lt;p&gt;In this sense, the problem of online/monotonic compression implies compression of infinite sequences. Shannon implicitly invokes infinite data sequences by virtue of minimizing average, a.k.a. asymptotic, compression length.&lt;/p&gt;
&lt;h2 id=&#34;greedy-compression&#34;&gt;Greedy compression&lt;/h2&gt;
&lt;p&gt;Despite the name, monotone Kolmogorov complexity, denoted by the function $\Km$, is actually performing greedy compression on some finite sequence.&lt;/p&gt;
&lt;p&gt;A quick review of differences between prefix Kolmogorov complexity $K$, and monotone Kolmogorov complexity $\Km$.   Both use universal Turing machines with 1 work tape, 1 input tape, and 1 output tape. But $K$ uses prefix machines (the machine must halt and the output is not valid and read until it halts) and $\Km$ uses monotone machines (output is a streamed infinite sequence, where all prefixes are valid and can be read in the middle of computation).&lt;/p&gt;
&lt;p&gt;Let $x\in\B^*$.&lt;/p&gt;
&lt;p&gt;For $K$, the universal machine $U$ is a prefix machine which takes a finite binary sequence as input. (its inputs form a prefix-free code, defined by whatever causes $U$ to halt)&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
K_U(x) = \min\set{\ell(p) \mid x = U(p),\ p \in \B^*}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;For $\Km$, the universal machine $U$ is a monotone machine takes as input an infinite binary sequence. If $p\in\B^*$ is finite, then let $U(p)$ be the output prefix (whatever is written on the output tape up to the write head at some computation step) of maximum length s.t. the read head has not gone past the input prefix $p$.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\Km_U(x) = \min\set{\ell(p) \mid x \sqsubseteq U(p),\ p \in \B^*}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;As we see, with $\Km$ it is sufficient to produce an input prefix $p$ which generates at least $x$ as a prefix at the output. In this way, $p$ need not encode any length information about $x$.&lt;/p&gt;
&lt;p&gt;An intuition pump for $\Km_M$ is to imagine using a monotone machine $M$ that performs arithmetic decoding according to some measure $\mu$. (note that the $U$ argument for $K$ or $\Km$ need not be universal). Then $\Km_M(x)$ is the length of the shortest arithmetic encoding that is $M$-decoded to a sequence starting with $x$. Let $p_x$ be that shortest encoding (program prefix). As we append to $x$, $p_x$ need not monotonically change. See &lt;a href=&#34;#appendix---greedy-vs-monotonic&#34;&gt;#Appendix - Greedy vs Monotonic&lt;/a&gt; for a visual example of this.&lt;/p&gt;
&lt;h3 id=&#34;optimality&#34;&gt;Optimality&lt;/h3&gt;
&lt;p&gt;L&amp;amp;V Corollary 4.5.3:&lt;/p&gt;
&lt;p&gt;An infinite sequence $\o$ is $\mu$-typical (Martin-Lof $\mu$-random) iff&lt;br&gt;
$$\sup_{n\in\N}\set{\lg 1/\mu(\o_{1:n}) - \Km_U(\o_{1:n})}$$&lt;/p&gt;
&lt;p&gt;exists.&lt;/p&gt;
&lt;p&gt;This is saying that there exists some finite constant $c&amp;lt;\infty$ s.t.  $\g(\o_{1:n}) = \lg 1/\mu(\o_{1:n}) - \Km_U(\o_{1:n}) &amp;lt; c$ for all $n\in\N$. That is to say, this difference doesn&amp;rsquo;t get arbitrarily large. (Note that $\g(\o_{1:n})$ is an explicit universal Martin-Lof $\mu$-test for randomness, since there&amp;rsquo;s an &amp;ldquo;iff&amp;rdquo; relationship here. It&amp;rsquo;s very cool that we can explicitly define universal tests for randomness, even if they are not computable.)&lt;/p&gt;
&lt;p&gt;If we think of the self-information $\lg 1/\mu(\o_{1:n})$ as the compression length of $\o_{1:n}$ under $\mu$, then the theorem is saying that the prefixes of a $\mu$-typical are optimally compressed by a $\mu$-encoder. But let&amp;rsquo;s make this notion more precise.&lt;/p&gt;
&lt;p&gt;Let $E$ be a monotone Turing machine s.t. $\l = \mu_E$ (call $E$ a $\mu$-encoder), and let $M$ be a monotone Turing machine s.t. $\mu  = \l_M$ (call $M$ a $\mu$-decoder). Then $\z_{1:m} = E(\o_{1:n})$ is the prefix of the encoding $\z$ of $\o$, and $\o_{1:n&#39;} = M(\z_{1:m&#39;})$ is a prefix of the decoding $\o$.  (note that the round-trip $M(E(\o_{1:n})) \sqsubseteq \o_{1:n}$ and can be less than length $n$)&lt;/p&gt;
&lt;p&gt;In general, the decoder $M$ can map multiple encodings to the same sequence $\o$. As we&amp;rsquo;ve seen in the first section, if $\mu$ is a proper measure then an encoder and decoder always exist. But if $\mu$ is a semimeasure we can only guarantee that the decoder $M$ exists (the universal machine $U$ has no corresponding encoder, as finding programs which emit sequences is not computable).&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d like to be able to say that any $\mu$-decoder $M$ can realize the optimal code length  $\lg 1/\mu(\o_{1:n})$, i.e. that there exists an encoding prefix $\z_{1:m}$ s.t. $\o_{1:n} \sqsubseteq M(\z_{1:m})$ and that $m \approx \lg 1/\mu(\o_{1:n})$. We can reuse $\Km$ with $M$ to obtain the best length $m$, i.e. $\Km_M(\o_{1:n})$ is the length of the shortest input to $M$ resulting in the prefix $\o_{1:n}$.&lt;/p&gt;
&lt;p&gt;For any $\o$, $\mu$, and $\mu$-decoder $M$, we must have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\Km_M(\o_{1:n}) \geq \lg 1/\mu(\o_{1:n})&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;(this is true by the definitions of these objects)&lt;/p&gt;
&lt;p&gt;I will now (here and below) give aspirational theorems that I don&amp;rsquo;t have proofs for. These seem to me like they should be true. These are possibly straightforward to prove given the theorems in L&amp;amp;V, or these are difficult open problems. Something for me to figure out&amp;hellip;&lt;/p&gt;
&lt;p&gt;Conjecture 1:&lt;br&gt;
$\o$ is $\mu$-typical iff&lt;br&gt;
$$\sup_{n\in\N}\set{\Km_M(\o_{1:n}) - \Km_U(\o_{1:n})}$$&lt;/p&gt;
&lt;p&gt;exists.&lt;/p&gt;
&lt;p&gt;This is more directly comparing two compression schemes, the $\mu$-decoder vs the Solomonoff (universal) decoder. If they are within a constant as $n\to\infty$, then we can say the $M$-decoder is as good as the universal decoder. This directly implies that $\Km_M(\o_{1:n}) - \lg 1/\mu(\o_{1:n})$ is upper bounded, and so we can say that $M$ realizes the $\mu$-compression  length $\lg 1/\mu(\o_{1:n})$ (at least if we find the right encoding $\z_{1:m}$ s.t. $m=\Km_M(\o_{1:n})$ and $\o_{1:n}\sqsubseteq M(\z_{1:m})$ ).&lt;/p&gt;
&lt;p&gt;We do have another related statement that is proved in L&amp;amp;V (also part of Corollary 4.5.3):&lt;br&gt;
$\o$ is $\mu$-typical iff&lt;br&gt;
$$&lt;br&gt;
\sup_{n\in\N}\set{ \lg 1 / \mu(\o_{1:n}) - \lg 1 / \xi(\o_{1:n}) }&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;exists.&lt;/p&gt;
&lt;h3 id=&#34;arithmetic-coding&#34;&gt;Arithmetic Coding&lt;/h3&gt;
&lt;p&gt;Whenever $\mu$ is a computable measure, we can construct an arithmetic encoder and decoder pair. Let $M$ be an arithmetic decoder for $\mu$. Then we have a useful result:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\Km_M(\o_{1:n}) - \lg 1/\mu(\o_{1:n}) &amp;lt; 2&lt;br&gt;
$$&lt;br&gt;
for all $n\in\N$. (this theorem is stated in C&amp;amp;T section 13.3 Arithmetic Coding and is easy to prove). Then this immediately gives us conjecture 1 for the special case of an arithmetic decoder.&lt;/p&gt;
&lt;p&gt;In practice, we will only be dealing with computable measures, and so it is sufficient to think of a distribution over infinite sequences as simultaneously implying a compression scheme over infinite sequences via arithmetic coding. Conjecture 1 says that $\mu$-arithmetic coding will be optimal compression (in the Kolmogorov sense) iff $\o$ is $\mu$-typical.&lt;/p&gt;
&lt;h2 id=&#34;monotone-compression&#34;&gt;Monotone Compression&lt;/h2&gt;
&lt;p&gt;The Kolmogorov sense of compression, captured by $\Km$, is performing greedy compression. While we are indeed using monotone decoders to decompress, if we were to stream in data for (de)compression, we might have to keep retroactively changing the previously determined shortest encoding w.r.t. the monotonic decoder. (This is not hard to show for arithmetic decoding. Try constructing an instance of this yourself! See &lt;a href=&#34;#appendix---greedy-vs-monotonic&#34;&gt;#Appendix - Greedy vs Monotonic&lt;/a&gt; for a solution.)&lt;/p&gt;
&lt;p&gt;This is due to the definition of $\Km$, which does a minimization over prefixes given a particular $\o_{1:n}$ (what we observe while performing the compression), but the continuation $\o$ is not taken into account.&lt;/p&gt;
&lt;p&gt;To be equivalent to Shannon&amp;rsquo;s setup (and to generalize the noiseless coding theorem to arbitrary measures $\mu$), we need to perform true monotonic compression, where generated compressed prefixes are never retroactively altered. For this, we will need a different notion of optimality.&lt;/p&gt;
&lt;p&gt;As before, let $M$ be a $\mu$-decoder and $\o\in\B^\infty$ be an infinite sequence.&lt;br&gt;
Suppose $\o = M(\z)$ for some $\z\in\B^\infty$.&lt;/p&gt;
&lt;p&gt;Note that&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
m \geq \lg 1/\mu(\o_{1:n})&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;whenever $\o_{1:n}\sqsubseteq M(\z_{1:m})$  (this is a trivial consequence of our definitions)&lt;/p&gt;
&lt;p&gt;If we wanted to evaluate whether the particular infinite encoding $\z$ is an optimal compression of $\o$ (rather than the jumping around to different prefixes in $\B^*$ that $\Km$ implies), we once again have the problem of a gap between $m$ and $1/\mu(\o_{1:n})$.&lt;/p&gt;
&lt;p&gt;Conjecture 2:&lt;br&gt;
If $\z\in\B^\infty$ is algorithmically random and $M$ is a $\mu$-decoder, then there exists a finite constant $c&amp;lt;\infty$ s.t.&lt;br&gt;
$$&lt;br&gt;
\rho(\z_{1:m}) = \ell(M(\z_{1:m})) - \lg 1/\mu(M(\z_{1:m})) &amp;lt; c&lt;br&gt;
$$&lt;br&gt;
for infinitely many $m\in\N$. But it will also be the case that for every $L&amp;gt;0$ there exists an $m\in\N$ s.t. $\rho(\z_{1:m}) &amp;gt; L$. This all amounts to saying that $\rho(\z_{1:m})$ oscillates infinitely often, with the trough of the oscillations falling below $c$, and the peaks being arbitrarily high (for every $L$ there exists an oscillation which goes above that $L$).&lt;/p&gt;
&lt;p&gt;When $M$ is an arithmetic decoder, we again have that $c = 2$ (for binary sequences).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;br&gt;
Is $\z$ algorithmically random iff $\o=M(\z)$ is $\mu$-random?&lt;br&gt;
When the encoder $E$ exists, is $\o$ $\mu$-random iff $\z=E(\o)$ is algorithmically random?&lt;/p&gt;
&lt;p&gt;If the answer to both questions is yes, then it is sufficient to define an encoding $\z$ of $\o$ as an optimal compression iff $\z$ is algorithmically random, and then we have that $E(\o)$ is an optimal compression of $\o$ iff $E$ is a $\mu$-encoder.&lt;/p&gt;
&lt;h3 id=&#34;generalizing-the-noiseless-coding-theorem&#34;&gt;Generalizing the Noiseless Coding Theorem&lt;/h3&gt;
&lt;p&gt;When the $\mu$-encoder $E$ exists, we have a result that looks like a generalization of the noiseless coding theorem.&lt;/p&gt;
&lt;p&gt;If $\o$ is $\mu$-random (equivalent to saying it is drawn from $\mu$), then&lt;/p&gt;
&lt;p&gt;$$\ell(E(\o_{1:n})) \geq \lg 1/\mu(\o_{1:n})$$&lt;/p&gt;
&lt;p&gt;is something we get by definition, but it looks analogous to Shannon&amp;rsquo;s result that expected compression length is lower bounded by entropy. Except here we replace entropy with self-information of the particular sequence.&lt;/p&gt;
&lt;p&gt;We then have that this self-information lower bound is optimal using Corollary 4.5.3 (from earlier). We finally show that we can get arbitrarily close to this lower bound in practice on the particular $\o$ in question using conjecture 2 (arbitrarily close here means gets within $c$ infinitely often).&lt;/p&gt;
&lt;p&gt;Basically, this result tells us that when data $\o$ is $\mu$-typical, we can use a $\mu$-encoder to get an optimal compression on that data. This is also an asymptotic result, because prefixes of $\o$ may be very atypical for $\mu$ while the whole $\o$ is typical. Furthermore, the data prefix needs to be larger enough so that the gap between $\lg 1/\mu(\o_{1:n})$ and $\Km_U(\o_{1:n})$ is negligible.&lt;/p&gt;
&lt;h1 id=&#34;making-this-practical&#34;&gt;Making This Practical&lt;/h1&gt;
&lt;p&gt;Above I generalized the noiseless coding theorem. The generalized theorem agrees with Shannon in the case where the data is generated by an ergodic Markov process. Otherwise, instead of entropy of the process being the optimal compression rate, we use self-information as the optimal total compressed length of the particular data we have.  This promotes self-information to a central role as defining a quantity of information (Does self-information/entropy actually define information? What is information? Shannon was only interested in communication. What about semantic information? All good questions for another time.).&lt;/p&gt;
&lt;p&gt;So how do we use the generalized noiseless coding theorem in practice?&lt;/p&gt;
&lt;p&gt;Remember that Shannon walks us through how we should perform compression. First decide on a model class (e.g. Markov processes with context size $k$). Then estimate the model parameters using a corpus of data (e.g. a corpus of English text). Shannon suggests manually counting all the $k$ grams. Then share that model with the receiver and use it to compress future data that is streamed in.&lt;/p&gt;
&lt;p&gt;This model fitting step generalizes to performing maximum likelihood. If our model is $p_\theta(X_n \mid x_{&amp;lt;n})$, parametrized by $\theta$, then we can fit our model to a dataset $x_{1:N}$, by performing the maximization&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\max_\theta \prod_{i=1}^N p_\theta(x_i \mid x_{&amp;lt;i})&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This works for Markov models, where the parameters are just the transition probabilities. The maximization will be equivalent to manually counting $k$-gram frequencies in the data.&lt;/p&gt;
&lt;p&gt;The intuition is that the better the model captures the statistics of the data (whatever that means), the better the compression. With Martin-Lof randomness, we can define exactly what it means to &amp;ldquo;capture the statistics of data&amp;rdquo; (is the data typical for your model?), and this definition of typicality turns out to be equivalent to optimal compression under your model anyway (data is typical for a distribution if it optimally compresses under that distribution).&lt;/p&gt;
&lt;p&gt;In both Shannon&amp;rsquo;s setting and ours, we may not capture the &amp;ldquo;true&amp;rdquo; distribution of the data from a corpus. The way to deal with this is called &lt;a href=&#34;https://en.wikipedia.org/wiki/Adaptive_coding&#34;target=&#34;_blank&#34;&gt;adaptive coding&lt;/a&gt;, where as more data comes in, the estimated model parameters are improved. Those improvements can be retransmitted to the receiver every so often (this incentivizes us to choose the model class so that the parameter description length is short).&lt;/p&gt;
&lt;p&gt;To recap, Shannon&amp;rsquo;s framework assumes the statistics of the data are known for all time and then gives an optimality result. This is not useful in practice. Instead, one estimates the statistics of data on hand, and then improves the estimate as more data comes in. A bad estimate early on accounts for only a fixed amount of inefficiency. As the estimate improves, you converge in the long run to the optimal compression rate, as the constant amount of inefficiency becomes insignificant.&lt;/p&gt;
&lt;p&gt;The same happens in our generalized setup. We are estimating $\mu$ from finite data (this is equivalent to the problem of prediction). If we are wrong about $\mu$ on future data, we can update to $\mu&#39;$ and change our compression going forward. If we eventually succeed in capturing the statistics of the data for all time, then arithmetic coding under our best $\mu$ will be Kolmogorov optimal compression. But having succeeded implies that the statistics of the data will never change again (in a way we have not already captured in $\mu$). This is unlikely to happen in practice.&lt;/p&gt;
&lt;p&gt;You can see how this problem of finding the $\mu$ for which $\o$ is typical, given only the finite data $\o_{1:n}$, is equivalent the problem of prediction. Since $\mu$ is merely computable, $\mu$ can include fancy time-dependent patterns. The time-dependent patterns that we picked up on in $\o_{1:n}$ (and which continue) will be predicted for the rest of $\o$, but patterns which only begin after $\o_{1:n}$ will not be predicted by $\mu$.&lt;/p&gt;
&lt;h1 id=&#34;appendix---greedy-vs-monotonic&#34;&gt;Appendix - Greedy vs Monotonic&lt;/h1&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/arithmetic_coding_example.jpg&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Horizontally we have the real unit interval (isomorphic to the infinite binary sequences). The regions between the blue lines contain all the encoded binary sequences starting with some prefix (under the uniform measure $\l$). The regions between the green lines contain all the decoded binary sequences starting with some prefix (under some measure $\mu$). If we observe that data sequence $\o$ starts with $\o_{1:4}=0100$, then thats equivalent to saying that $\o$ falls within the grey region. An encoding for $\o_{1:4}$ is a region between blue lines that falls entirely within the grey region (because our encoding should uniquely identify $\o_{1:4}$). We can see that the purple region towards the top, identified by the prefix $1000$, is the largest blue region which falls entirely within the grey region (and so has the smallest prefix length). But when we observe more of $\o$, we might discover that $\o$ lies in the green region towards the top, identified by the prefix $01111$. So we would be forced to retroactively switch our prefix from $1000$ to $01111$ as more of $\o$ streams in. A monotonic encoder would not emit anything until enough of $\o$ was read in to uniquely determine the output prefix.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variational Inference</title>
      <link>https://danabo.github.io/blog/posts/variational-inference/</link>
      <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/variational-inference/</guid>
      <description>&lt;p&gt;This is a primer on variational inference in machine learning, based on sections of &lt;a href=&#34;https://link.springer.com/content/pdf/10.1023%2FA%3A1007665907178.pdf&#34;target=&#34;_blank&#34;&gt;Jordan et al.&lt;/a&gt; (&lt;em&gt;An Introduction to Variational Methods for Graphical Models&lt;/em&gt;; 1999). I go over the mathematical forms of variational inference, and I include a discussion on what it means for something to be &amp;ldquo;variational.&amp;rdquo; I hope this conveys a bit of the generating ideas that give rise to the various forms of variational inference.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\es}{\emptyset}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Phi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\pr}{\times}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\N}{\mb{N}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\tup}{\par}&lt;br&gt;
\newcommand{\brak}[1]{\left[#1\right]}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\kl}[2]{D_{\text{KL}}\left(#1\ \| \ #2\right)}&lt;br&gt;
\DeclareMathOperator*{\argmin}{argmin}&lt;br&gt;
\DeclareMathOperator*{\argmax}{argmax}&lt;br&gt;
\newcommand{\d}{\mathrm{d}}&lt;br&gt;
\newcommand{\L}{\mc{L}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\Er}{\mc{E}}&lt;br&gt;
\newcommand{\ht}{\hat{\t}}&lt;br&gt;
\newcommand{\hp}{\hat{\p}}&lt;br&gt;
\newcommand{\D}{\mc{D}}&lt;br&gt;
\newcommand{\H}{\mc{H}}&lt;br&gt;
\newcommand{\softmax}{\text{softmax}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
$$&lt;/p&gt;
&lt;h1 id=&#34;terminology&#34;&gt;Terminology&lt;/h1&gt;
&lt;p&gt;My experience with other sources that explain variational inference is that they leave me confused about what the word &amp;ldquo;variational&amp;rdquo; is pointing at. E.g. &lt;a href=&#34;https://probml.github.io/pml-book/book0.html&#34;target=&#34;_blank&#34;&gt;Murphy 2012&lt;/a&gt; (&lt;em&gt;Machine Learning: a Probabilistic Perspective&lt;/em&gt;), &lt;a href=&#34;https://link.springer.com/book/9780387310732&#34;target=&#34;_blank&#34;&gt;Bishop 2006&lt;/a&gt; (&lt;em&gt;Pattern Recognition and Machine Learning&lt;/em&gt;), &lt;a href=&#34;https://arxiv.org/abs/1601.00670&#34;target=&#34;_blank&#34;&gt;Blei 2016&lt;/a&gt; (&lt;em&gt;Variational Inference: A Review for Statisticians&lt;/em&gt;), and even Wikipedia: &lt;a href=&#34;https://en.wikipedia.org/wiki/Variational_Bayesian_methods&#34;target=&#34;_blank&#34;&gt;Variational Bayesian methods&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Aside from being irksome, I find the lack of clarity around this word inhibits by ability to grok what authors are thinking when they write about variational inference. Specifically, I want to understand the underlying generating idea for this class of methodologies. As an ML practitioner and researcher, aim to build off of existing ideas. To do that I need to know how previous authors thought about their ideas. Having the generator of an idea, I can play with it and potentially generate something not previously considered.&lt;/p&gt;
&lt;p&gt;To that end, this section is a detour to explore the meaning of the word &amp;ldquo;variational&amp;rdquo; in the context of variational inference. If this does not interest you, feel free to skip to the next section, &lt;a href=&#34;#variational-inference&#34;&gt;#Variational Inference&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;what-is-variational-&#34;&gt;What is Variational ?&lt;/h2&gt;
&lt;p&gt;In the context of variational inference, the word &amp;ldquo;variational&amp;rdquo; refers to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Calculus_of_variations&#34;target=&#34;_blank&#34;&gt;calculus of variations&lt;/a&gt; (CoV). Some variational inference texts mention the CoV connection explicitly, e.g. &lt;a href=&#34;https://link.springer.com/book/9780387310732&#34;target=&#34;_blank&#34;&gt;Bishop 2006&lt;/a&gt;, section 10.1.&lt;/p&gt;
&lt;p&gt;According to Wikipedia,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;strong&gt;calculus of variations&lt;/strong&gt; is a field of &lt;a href=&#34;https://en.wikipedia.org/wiki/Mathematical_analysis&#34;title=&#34;Mathematical analysis&#34;target=&#34;_blank&#34;&gt;mathematical analysis&lt;/a&gt; that uses variations, which are small changes in &lt;a href=&#34;https://en.wikipedia.org/wiki/Function_%28mathematics%29&#34;title=&#34;Function (mathematics)&#34;target=&#34;_blank&#34;&gt;functions&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Functional_%28mathematics%29&#34;title=&#34;Functional (mathematics)&#34;target=&#34;_blank&#34;&gt;functionals&lt;/a&gt;, to find maxima and minima of functionals: &lt;a href=&#34;https://en.wikipedia.org/wiki/Map_%28mathematics%29&#34;title=&#34;Map (mathematics)&#34;target=&#34;_blank&#34;&gt;mappings&lt;/a&gt; from a set of &lt;a href=&#34;https://en.wikipedia.org/wiki/Function_%28mathematics%29&#34;title=&#34;Function (mathematics)&#34;target=&#34;_blank&#34;&gt;functions&lt;/a&gt; to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Real_number&#34;title=&#34;Real number&#34;target=&#34;_blank&#34;&gt;real numbers&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://mathworld.wolfram.com/CalculusofVariations.html&#34;target=&#34;_blank&#34;&gt;wolfram.com&lt;/a&gt; gives a slightly different description for the CoV,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A branch of mathematics that is a sort of generalization of &lt;a href=&#34;https://mathworld.wolfram.com/Calculus.html&#34;target=&#34;_blank&#34;&gt;calculus&lt;/a&gt;. Calculus of variations seeks to find the path, curve, surface, etc., for which a given &lt;a href=&#34;https://mathworld.wolfram.com/Function.html&#34;target=&#34;_blank&#34;&gt;function&lt;/a&gt; has a &lt;a href=&#34;https://mathworld.wolfram.com/StationaryValue.html&#34;target=&#34;_blank&#34;&gt;stationary value&lt;/a&gt; (which, in physical problems, is usually a &lt;a href=&#34;https://mathworld.wolfram.com/Minimum.html&#34;target=&#34;_blank&#34;&gt;minimum&lt;/a&gt; or &lt;a href=&#34;https://mathworld.wolfram.com/Maximum.html&#34;target=&#34;_blank&#34;&gt;maximum&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &amp;ldquo;stationary value&amp;rdquo; here refers to the min or max of a functional, and a functional is a mapping from functions to real numbers. Both accounts of the CoV make reference optimizing a functional.&lt;/p&gt;
&lt;p&gt;If we want to use &amp;ldquo;variational&amp;rdquo; as an adjective, e.g. in &amp;ldquo;variational method&amp;rdquo; (&lt;a href=&#34;https://en.wikipedia.org/wiki/History_of_variational_principles_in_physics&#34;target=&#34;_blank&#34;&gt;examples&lt;/a&gt;), how could we reasonably extrapolate from the above description of the CoV?&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s my proposal: something is &amp;ldquo;variational&amp;rdquo; if it involves an optimization problem with uncountably many degrees of freedom. Equivalently, we are searching over a function space to find an optimal function (according to our optimization criteria). Then a &amp;ldquo;variation&amp;rdquo; is some &amp;ldquo;perturbation&amp;rdquo; to a candidate function that moves us infinitesimally in some direction in function space.&lt;/p&gt;
&lt;p&gt;So with this notion, regular calculus is not variational, despite involving searching over function spaces, because a given function is either the solution or not, and is not scored. I suppose that if we recast integration and differentiation as functionals whose optima are the derivatives and integrals of the desired function, we would be reposing calculus as a variational problem (similar to the kind of problem-reposing we will see in the next section).&lt;/p&gt;
&lt;h2 id=&#34;variational-methods-in-ml&#34;&gt;Variational methods in ML&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://link.springer.com/content/pdf/10.1023%2FA%3A1007665907178.pdf&#34;target=&#34;_blank&#34;&gt;Jordan et al.&lt;/a&gt;, section 4, &lt;em&gt;Basics of variational methodology&lt;/em&gt;, gives us a primer on variational methods.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let us begin by considering a simple example. In particular, let us express the logarithm function variationally:&lt;br&gt;
$$\ln(x) = \min_\l\set{\l x-\ln\l-1}\,.$$&lt;br&gt;
In this expression $\l$ is the variational parameter, and we are required to perform the minimization for each value of $x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://link.springer.com/content/pdf/10.1023%2FA%3A1007665907178.pdf&#34;target=&#34;_blank&#34;&gt;Jordan et al.&lt;/a&gt; is implicitly defining what &amp;ldquo;variational&amp;rdquo; means here. We are given an optimization criteria, namely $\fa x,\ \min_{\l_x}\set{\l_x x-\ln\l_x-1}$, which has infinitely many optimization parameters, $\l_x$ for every $x$. We could view this optimization as being over the space of (continuous) functions, $\R\to\R$, and searching for some $\l:\R\to\R$ s.t. $\l(x) x-\ln\l(x)-1$ is minimal for every $x\in(0,\infty)$. In this case, $\l(x)=1/x$ is the solution.&lt;/p&gt;
&lt;p&gt;Note that in this problem we don&amp;rsquo;t have a functional. Nevertheless  &lt;a href=&#34;https://link.springer.com/content/pdf/10.1023%2FA%3A1007665907178.pdf&#34;target=&#34;_blank&#34;&gt;Jordan et al.&lt;/a&gt; is taking this to be a variational method. If we are to broaden our notion of variational from the previous section, &lt;a href=&#34;#what-is-variational-?&#34;&gt;#What is Variational ?&lt;/a&gt;, to accommodate this usage, we could say that a variational problem is an optimization problem over an infinite dimensional function space, or equivalently with infinitely many optimization parameters. In most cases the optimization problem can be specified with a functional, but in this case it is not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: Is $\min_{\l:\R\to\R}\set{\int_\X\l(x) x-\ln\l(x)-1\ \d x}$ an equivalent problem? Then $\mc{F}[\l]=\int_\X\l(x) x-\ln\l(x)-1\ \d x$ is our functional.&lt;/p&gt;
&lt;p&gt;To generalize this variational method, suppose we have a function $f(x)$ which is intractable to calculate directly. Luckily, we know of some other tractable function, $g(x,\l)$, s.t. $g(x,\l) \geq f(x)$ for all $\l\in\R$ and $x\in\X$. Furthermore, $g(x,\l) = f(x)$ for some $\l\in\R$, for all $x\in\X$ - i.e. $g$ is a tight upper bound of $f$. Let $\l(x) = \argmin_{\tilde{\l}}g(x,\tilde{\l})$. Then it is the case that $f(x) = g(x,\l(x))$ for all $x\in\X$. So for any $f$, a suitable choice of $g$ gives us a variational form of $f(x)$.&lt;/p&gt;
&lt;p&gt;If we are not able to perform the exact minimization $\argmin_{\l}g(x,\l)$ symbolically, but we instead find an approximate minimum $\hat{\l}_x$ numerically (e.g. with gradient descent). Then $g(x,\hat{\l}_x)$ is an approximation of $f(x)$, and $g(x,\hat{\l}_x) \geq f(x)$ is guaranteed. This is now useful as a way to numerically approximate $f(x)$ by converting it into an optimization problem which we know how to numerically approximate.&lt;/p&gt;
&lt;h1 id=&#34;setup&#34;&gt;Setup&lt;/h1&gt;
&lt;p&gt;Let $x = (x_1,x_2,x_3,\dots) \in \X = \X_1\pr\X_2\pr\X_3\pr\dots$ and $z = (z_1,z_2,z_3,\dots) \in \Z = \Z_1\pr\Z_2\pr\Z_3\pr\dots$ be finite or infinite length tuples, where $x$ is the &lt;em&gt;data variable&lt;/em&gt; and $\X$ is the &lt;em&gt;data space&lt;/em&gt;, and $z$ is the &lt;em&gt;hidden&lt;/em&gt; or &lt;em&gt;latent variable&lt;/em&gt; and $\Z$ is the &lt;em&gt;latent space&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;When we visualize $(x,z)$ as a graph, we call each $x_i$ or $z_j$ a &lt;em&gt;node&lt;/em&gt; (in the context of the graphical model literature). However, in this post I will call them &lt;em&gt;dimensions&lt;/em&gt;. Each $\X_i$ and $\Z_j$ can be a finite or countable set, or an uncountable set with a metric space (typically the Euclidean metric on $\R$).&lt;/p&gt;
&lt;p&gt;Suppose we have a family of joint distributions $\set{p_\t\mid\t\in\T}$, i.e. $p_\t(x,z)$ is the joint probability of $x$ and $z$ for some choice of $\t\in\T$. We say that $p_\t$ is &lt;em&gt;parametrized&lt;/em&gt; by $\t$, where $\T$ is a finite dimensional metric space, e.g. $\T\subseteq\R^r$ with $r\in\N$.&lt;/p&gt;
&lt;p&gt;We assume that $p_\t(x,z)$ and $p_\t(z)$ are tractable (i.e. fast) quantities to calculate with a computer (to sufficient precision) for all $(x,z)\in\X\pr\Z$, but that $p_\t(x)$ and $p_\t(z\mid x)$ are intractable (too slow to be useful) while also being quantities of interest.&lt;/p&gt;
&lt;p&gt;Variational inference is a method for tractably approximating $p_\t(x)$ and $p_\t(z\mid x)$ for all $(x,z)\in\X\pr\Z$.&lt;/p&gt;
&lt;h2 id=&#34;note-about-probability-notation&#34;&gt;Note about probability notation&lt;/h2&gt;
&lt;p&gt;When I write $p_\t(x,z)$, I am treating $p_\t$ as a function of $x$ and $z$ that outputs a probability mass or density.&lt;/p&gt;
&lt;p&gt;However, I will overload $p_\t$, by argument name, to represent a number of related functions. For instance, the marginal probabilities $p_\t(x) = \int_\Z p_\t(x,z)\ \d z$ and $p_\t(z) = \int_\X p_\t(x,z)\ \d x$ (replacing integrals with sums when variables are discrete) are different quantities depending on whether the argument to $p_\t$ is $x$ or $z$. Additionally I might consider marginal probabilities of specific dimensions, e.g. $p_\t(x_{i_1},x_{i_2},\dots,z_{j_1},z_{j_2},\dots)$ is the integral of $p_\t(x,z)$ over all dimensions not included as arguments.&lt;/p&gt;
&lt;p&gt;Furthermore, we have conditional probabilities, e.g. $p_\t(x\mid z)=p_\t(x,z)/p_\t(z)$ or $p_\t(x_{i_1},z_{j_1}\mid x_{i_2},z_{j_2})=p_\t(x_{i_1},x_{i_2},z_{j_1},z_{j_2})/p_\t(x_{i_2},z_{j_2})$.&lt;/p&gt;
&lt;p&gt;This covers the various functions that $p_\t$ can represent, and you can see how the form of the arguments to $p_\t$ determines which function we are considering.&lt;/p&gt;
&lt;h1 id=&#34;variational-inference&#34;&gt;Variational Inference&lt;/h1&gt;
&lt;p&gt;See &lt;a href=&#34;https://link.springer.com/content/pdf/10.1023%2FA%3A1007665907178.pdf&#34;target=&#34;_blank&#34;&gt;Jordan et al.&lt;/a&gt;, section 6, &lt;em&gt;The block approach&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Continuing from the &lt;a href=&#34;#setup&#34;&gt;#Setup&lt;/a&gt; above, $p_\t(z\mid x)$ is an intractable quantity (for most $z$ and $x$). So instead we introduce the family of tractable distributions $q_\p$ on $z$ parametrized by $\p\in\P$, where $q_\p(z)$ is intended to be our approximation of $p_\t(z\mid x)$ for a particular $x$, and $\p$ will be our variational parameter(s) w.r.t. $x$ (i.e. the optimization solution will be a function from $\X$ to $\P$).&lt;/p&gt;
&lt;p&gt;Define the following quantities,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\M(\t;\ x) &amp;amp;\df \log\par{1/p_\t(x)}\,, \\&lt;br&gt;
\Er(\t,\p;\ x) &amp;amp;\df \kl{q_\p(z)}{p_\t(z\mid x)} \\&amp;amp;= \int_\Z q_\p(z)\log\par{\frac{q_\p(z)}{p_\t(z\mid x)}}\ \d z\,, \\&lt;br&gt;
\L(\t,\p;\ x) &amp;amp;\df \M(\t;\ x) + \Er(\t,\p;\ x)\,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;($\kl{\cdot}{\cdot}$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Kullback%e2%80%93Leibler_divergence&#34;target=&#34;_blank&#34;&gt;KL-divergence&lt;/a&gt;, which is &lt;a href=&#34;https://en.wikipedia.org/wiki/Kullback%e2%80%93Leibler_divergence#Properties&#34;target=&#34;_blank&#34;&gt;always non-negative&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;All three quantities are non-negative for all $\t,\p,x$. Then we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\L(\t,\p;\ x) \geq \M(\t;\ x)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;with equality when $\Er(\t,\p;\ x)=0$, i.e. when $q_\p(z)=p_\t(z\mid x)$ for all $z$.&lt;/p&gt;
&lt;p&gt;Let $\t$ be constant. When $p_\t(\cdot \mid x) \in\set{q_\p(\cdot)\mid \p\in\P}$ for all $x$, then we can write $\M(\t;\ x) = \min_\p\L(\t,\p;\ x)$, with the solution being some function $\p^*:\X\to\P$. Hence we have a variational optimization problem whose solution is a function.&lt;/p&gt;
&lt;p&gt;Otherwise if $p_\t(\cdot \mid x) \notin\set{q_\p(\cdot)\mid \p\in\P}$, then $\L(\t,\p;\ x)$ is forever an upper bound of $\M(\t;\ x)$, but its minimum may still be close enough to $\M(\t;\ x)$ for it to be a reasonable substitute. So the variational minimization problem, $\fa x,\ \min_{\p_x}\L(\t,\p_x;\ x)$, is of interest in either case.&lt;/p&gt;
&lt;p&gt;A nice property of this particular setup is that the minimization objective, $\fa x,\ \min_{\p_x}\L(\t,\p_x;\ x)$, will also achieve $\fa x,\ \min_{\p_x}\Er(\t,\p_x;\ x)$ because $\M(\t;\ x)$ remains constant w.r.t. $\p_x$ (i.e. the gap $\Er(\t,\p_x;\ x)=\L(\t,\p;\ x) - \M(\t;\ x)$ is minimized). Minimal $\Er(\t,\p_x;\ x)$ in turn implies that $q_\p(z)$ is the best approximation of $p_\t(z\mid x)$ for all $z,x$. So from our single variational objective, we get two approximations of intractable quantities: $\L(\t,\p_x;\ x)$ for $\M(\t;\ x)$ which gives us $p_\t(x)$, and $q_{\p_x}(z)$ for $p_\t(z\mid x)$.&lt;/p&gt;
&lt;p&gt;Note that a tractable approximation of $\M(\t;\ x)$ gives us a tractable  approximation of $p_\t(z\mid x)$, and vice versa, since we can easily get one from the other using $\M(\t;\ x)=\log\par{p_\t(z\mid x)/p_\t(x,z)}$. Indeed, replacing $p_\t(z\mid x)$ with $q_{\p_x}(z)$ does get us $\L(\t,\p_x;\ x)$,&lt;br&gt;
$$\begin{aligned}\M(\t;\ x)&amp;amp;=\E_{z\sim q_\p(z)}\brak{\log\par{p_\t(z\mid x)/p_\t(x,z)}} \\&amp;amp;\leq \E_{z\sim q_\p(z)}\brak{\log\par{q_{\p_x}(z)/q_\t(x,z)}}\\&amp;amp;=\L(\t,\p_x;\ x)\,.\end{aligned}$$&lt;/p&gt;
&lt;h2 id=&#34;tractability&#34;&gt;Tractability&lt;/h2&gt;
&lt;p&gt;Suppose $\M(\t;\ x)$ is intractable. Then we can subsitute it for our variational approximation, $\L(\t,\p;\ x)$. Now the question is whether we can tractably calculate $\L(\t,\p;\ x)$ for all $\t,\p,x$.&lt;/p&gt;
&lt;p&gt;We have&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\L(\t,\p;\ x)=\kl{q_\p(z)}{p_\t(z)} + \E_{z\sim q_\p(z)}\left[\log\par{1/p_\t(x \mid z)}\right]\,,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;where $\kl{q_\p(z)}{p_\t(z)}=\int_\Z q_\p(z)\log\par{\frac{q_\p(z)}{p_\t(z)}}\ \d z$&lt;br&gt;
and $\E_{z\sim q_\p(z)}\left[\log\par{1/p_\t(x \mid z)}\right]=\int_\Z q_\p(z)\log\par{1/p_\t(x \mid z)}\ \d z$.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;#appendix&#34;&gt;#Appendix&lt;/a&gt; for derivation.&lt;/p&gt;
&lt;p&gt;This form is promising because it involves only quantities that we know are tractable, $q_\p(z)$, $p_\t(z)$ and $p_\t(x \mid z)$. However, calculating expectations w.r.t. $q_\p(z)$, and optimizing $\p$ through those expectations (e.g. with gradient descent) may still be tricky to perform tractably, and further approximations are likely needed. E.g. &lt;a href=&#34;https://arxiv.org/pdf/1312.6114.pdf&#34;target=&#34;_blank&#34;&gt;Kingma et al.&lt;/a&gt; discusses this problem and ways to get around it.&lt;/p&gt;
&lt;h2 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h2&gt;
&lt;p&gt;See &lt;a href=&#34;https://link.springer.com/content/pdf/10.1023%2FA%3A1007665907178.pdf&#34;target=&#34;_blank&#34;&gt;Jordan et al.&lt;/a&gt;, section 6.2, &lt;em&gt;Parameter estimation via variational methods&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Often, we are interested in solving&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\t^* = \argmin_{\t} \M(\t;\ x)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is maximum likelihood estimation, which is one way to &lt;em&gt;fit&lt;/em&gt; a &lt;em&gt;model&lt;/em&gt; (i.e. the family of distributions $p_\t$ for $\t\in\T$) to a dataset. Here, $\M(\t;\ x)$ is called a loss function. Often, we cannot solve this optimization exactly, so we use numerical optimization, such as gradient descent w.r.t. $\t$.&lt;/p&gt;
&lt;p&gt;When the loss $\M(\t;\ x)$ is intractable to calculate, we can replace it with the upper bound $\L(\t,\p;\ x)$ and then perform the joint minimization&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
(\t^*,\p^*)=\argmin_{\t,\p} \L(\t,\p;\ x)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which also gives us the approximation $q_{\p^*}(z)\approx p_{\t^*}(z\mid x)$.&lt;/p&gt;
&lt;h3 id=&#34;iid-datasets&#34;&gt;i.i.d. datasets&lt;/h3&gt;
&lt;p&gt;Usually, we are performing maximum likelihood estimation on a dataset that we are modeling as i.i.d. Specifically, our dataset is a list, $X=(x\up{1},\dots,x\up{n})$, of $n$ instances of the visible dimensions, where each $x\up{k} \in \X$. As before we have the parametrized distribution $p_\t(x,z)$ on $\X\pr\Z$. Since we assume the dataset is i.i.d., then we have&lt;/p&gt;
&lt;p&gt;$$p_\t(X) = p_\t(x\up{1},\dots,x\up{n}) = \prod_{k=1}^n p_\t(x\up{k})\,,$$&lt;/p&gt;
&lt;p&gt;and when $Z \in \Z^n$,&lt;/p&gt;
&lt;p&gt;$$p_\t(X,Z) = \prod_{k=1}^n p_\t(x\up{k},z\up{k})\,.$$&lt;/p&gt;
&lt;p&gt;Then we have&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}\M(\t;\ X) &amp;amp;= \log\par{1/p_\t(X)} \\&amp;amp;= \sum_{k=1}^n \log\par{1/p_\t(x\up{k})} \\&amp;amp;= \sum_{k=1}^n\M(\t;\ x\up{k})\,.\end{aligned}$$&lt;/p&gt;
&lt;p&gt;We want to approximate $p_\t(z\mid x)$ like before, but unlike before $x$ is not held fixed. In addition to having $n$ instances of $x$ in our dataset, we also want to efficiently calculate $p_\t(z\mid x)$ on any arbitrary $x\in\X$ outside our dataset.&lt;/p&gt;
&lt;p&gt;Following the example of &lt;a href=&#34;#variational-methods-in-ml&#34;&gt;#Variational methods in ML&lt;/a&gt;, let&amp;rsquo;s perform a separate minimization for every $x\in\X$. That is to say, let $q_\p(z)$ be a distribution over $\Z$ like before, with $\Er(\t,\p;\ x)=\kl{q_\p(z)}{p_\t(z\mid x)}$ and $\L(\t,\p;\ x) = \M(\t;\ x) + \Er(\t,\p;\ x)$.&lt;/p&gt;
&lt;p&gt;Then for each $x\up{k} \in X$ we have separate parameters $\p_{x\up{k}}$ so that $q_{\p_{x\up{k}}}(z)$ is our working approximation for $p_\t(z\mid x\up{k})$. Then our approximation of $p_\t(Z\mid X)$ is $q_{\p_{x\up{1}},\dots,\p_{x\up{n}}}(Z)=\prod_{k=1}q_{\p_{x\up{k}}}(z\up{k})$, and so&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
&amp;amp;\Er(\t,\p;\ X) \\&lt;br&gt;
&amp;amp;= \int_{\Z^n} q_{\p_{x\up{1}},\dots,\p_{x\up{n}}}(Z)\log\par{\frac{q_{\p_{x\up{1}},\dots,\p_{x\up{n}}}(Z)}{p_\t(Z\mid X)}}\ \d Z \\&lt;br&gt;
&amp;amp;= \int_{\Z^n} \brak{\prod_{k=1}^n q_{\p_{x\up{k}}}(z\up{k})}\brak{\sum_{k=1}^n\log\par{\frac{q_{\p_{x\up{k}}}(z\up{k})}{p_\t(z\up{k}\mid x\up{k})}}}\ \d z\up{1}\dots\d z\up{n} \\&lt;br&gt;
&amp;amp;= \sum_{k=1}^n \int_{\Z} q_{\p_{x\up{k}}}(z)\log\par{\frac{q_{\p_{x\up{k}}}(z)}{p_\t(z\mid x\up{k})}}\ \d z \\&lt;br&gt;
&amp;amp;= \sum_{i=1}^n \Er(\t,\p_{x\up{k}};\ x\up{k})\,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Then our dataset loss is $\sum_{k=1}^n\L(\t,\p_{x\up{k}};\ x\up{k})$ with $\L(\t,\p_{x\up{k}};\ x\up{k}) = \M(\t;\ x\up{k}) + \Er(\t,\p_{x\up{k}};\ x\up{k})$.&lt;/p&gt;
&lt;p&gt;To fit the model to $X=(x\up{1},\dots,x\up{n})$ w.r.t. $\t$, we perform the joint minimization,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
(\ht,\hp_{x\up{1}},\dots,\hp_{x\up{n}}) = \argmin_{\t,\p_{x\up{1}},\dots,\p_{x\up{n}}} \sum_{k=1}^n\L(\t,\p_{x\up{k}};\ x\up{k})\,,&lt;br&gt;
$$&lt;br&gt;
to obtain $\ht$ (we can discard the $\set{\hp_{x\up{k}}}$).&lt;/p&gt;
&lt;p&gt;Then for any $x\in\X$ of interest, we perform the minimization&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\hp_x = \argmin_{\p} \L(\ht,\p;\ x)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;giving us $q_{\hp_x}(z) \approx p_\ht(z\mid x)$.&lt;/p&gt;
&lt;h4 id=&#34;another-approach&#34;&gt;Another Approach&lt;/h4&gt;
&lt;p&gt;An alternative way to approximate $p_\t(z\mid x)$ for arbitrary $x\in\X$ is to have our approximate distribution $q_\p$ be a function of both $z$ and $x$ (for a single choice of parameters $\p$). This is typically notated as $q_\p(z\mid x)$, though this is technically an abuse of notation since we do not define the joint $q_\p(x,z)$. We could instead write $q_{\p,x}(z)$ or $q_{\p}(z;\ x)$, both of which make it clear that the probability distribution is only over $\Z$-space. However, I will continue with $q_\p(z\mid x)$ since it visually mimics $p_\t(z\mid x)$ and I find that easier to think about.&lt;/p&gt;
&lt;p&gt;Then we have $q_\p(Z\mid X)=\prod_{k=1}q_\p(z\up{k}\mid x\up{k})$, and so&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
&amp;amp;\Er(\t,\p;\ X) \\&lt;br&gt;
&amp;amp;= \int_{\Z^n} q_\p(Z\mid X)\log\par{\frac{q_\p(Z\mid X)}{p_\t(Z\mid X)}}\ \d Z \\&lt;br&gt;
&amp;amp;= \int_{\Z^n} \brak{\prod_{k=1}^n q_\p(z\up{k}\mid x\up{k})}\brak{\sum_{k=1}^n\log\par{\frac{q_\p(z\up{k}\mid x\up{k})}{p_\t(z\up{k}\mid x\up{k})}}}\ \d z\up{1}\dots\d z\up{n} \\&lt;br&gt;
&amp;amp;= \sum_{k=1}^n \int_{\Z} q_\p(z\mid x\up{k})\log\par{\frac{q_\p(z\mid x\up{k})}{p_\t(z\mid x\up{k})}}\ \d z \\&lt;br&gt;
&amp;amp;= \sum_{i=1}^n \Er(\t,\p;\ x\up{k})\,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Then our dataset loss is $\L(\t,\p;\ X) = \sum_{k=1}^n\L(\t,\p;\ x\up{k})$ with $\L(\t,\p;\ x\up{k}) = \M(\t;\ x\up{k}) + \Er(\t,\p;\ x\up{k})$.&lt;/p&gt;
&lt;p&gt;Our optimization problem becomes&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
(\ht,\hp) = \argmin_{\t,\p} \sum_{k=1}^n\L(\t,\p;\ x\up{k})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then $q_\hp(z\mid x) \approx p_\ht(z\mid x)$ for any $x\in\X$ of interest.&lt;/p&gt;
&lt;p&gt;This approach may let us approximate $p_\t(z\mid x)$ faster since we don&amp;rsquo;t rerun our optimization process for every $x$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: What other reasons are there to prefer one of these two approaches to i.i.d. MLE. What are the pros and cons of each?&lt;/p&gt;
&lt;h1 id=&#34;variational-bayes&#34;&gt;Variational Bayes&lt;/h1&gt;
&lt;p&gt;See &lt;a href=&#34;https://link.springer.com/content/pdf/10.1023%2FA%3A1007665907178.pdf&#34;target=&#34;_blank&#34;&gt;Jordan et al.&lt;/a&gt;, section 7.1.3, &lt;em&gt;Bayesian methods&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Variational inference (VI) applied to Bayesian inference is sometimes called &amp;ldquo;variational Bayes&amp;rdquo; (VB) (short for &amp;ldquo;variational Bayesian inference&amp;rdquo;). Mathematically VI and VB are the same. Whether we are doing Bayesian inference or just inference is a difference about the meaning we ascribe to that math, which determines how we apply it.&lt;/p&gt;
&lt;p&gt;Bayesian inference operates in a paradigm which I would call &amp;ldquo;Bayesian epistemology.&amp;rdquo; Informally, in this paradigm we observe pieces of data over a period of time, we have a set of hypotheses which remain unobserved, and each hypothesis gives us a prediction about what we will observe in the future given the present. To each hypothesis we assign a weight representing our belief about the plausibility of that hypothesis. Then we take the weighted average of all the hypotheses&#39; predictions to get our final prediction which we use, e.g. for decision making. See &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/#defining-bayesian-inference&#34;&gt;Deconstructing Bayesian Inference#defining-bayesian-inference&lt;/a&gt; for a more in-depth account of Bayesian epistemology.&lt;/p&gt;
&lt;p&gt;Formally, suppose we have a joint distribution $P(x,z)$ as before, except that $P$ is not parametrized. We take $\Z$ to be our hypothesis space and $\X$ to be the data space. We also suppose that we have partial data, e.g. we observe $x_{\leq n} = (x_1,\dots,x_n) \in \X_1\pr\dots\pr\X_n$, which is a subset of all the dimensions of the data space $\X$. I didn&amp;rsquo;t specify that we observe each dimension in the order of its indexing, but for convenience let&amp;rsquo;s re-index so that that is the case.&lt;/p&gt;
&lt;p&gt;Let $P(z)$ be the belief weight we assign for each $z\in\Z$. Call $P(z)$ our prior about $z$. Upon observing $x_{\leq n}$, our beliefs change, where $P(z\mid x_{\leq n})$ is now the weight we assign to $z$, called the posterior. We are interested in our average prediction distribution over the unobserved data, $x_{&amp;gt;n} = (x_{n+1},x_{n+2},\dots) \in \X_{n+1}\pr\X_{n+2}\pr\dots$, given the observed data $x_{\leq n}$,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
P(x_{&amp;gt;n}\mid x_{\leq n}) &amp;amp;= \int_{\Z}P(x_{&amp;gt;n}\mid x_{\leq n},z)P(z\mid x_{\leq n})\ \d z \\&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;for all $x_{&amp;gt;n}\in\X_{&amp;gt;n}$. We can see that the posterior $P(z\mid x_{\leq n})$ is involved in this calculation.&lt;/p&gt;
&lt;p&gt;When the posterior is intractable, we can perform VI (now restyled as VB), by defining an approximate posterior $q_\p(z)$ parametrized by $\p$. Like before, let&lt;/p&gt;
&lt;p&gt;$$\M(x_{\leq n}) = \log\par{1/P(x_{\leq n})}$$&lt;/p&gt;
&lt;p&gt;$$\Er(\p;\ x_{\leq n}) = \kl{q_\p(z)}{P(z\mid x_{\leq n})}$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\L(\p;\ x_{\leq n}) = \M(x_{\leq n}) + \Er(\p;\ x_{\leq n})$$&lt;/p&gt;
&lt;p&gt;Then we want to find the infinite vector $(\p^*_{x_{\leq n}})_{x_{\leq n} \in \X_{\leq n}}$ (equivalently represented as the function $\p^*:\X_{\leq n} \to \P$) which solves the minimization problem, $\fa x_{\leq n},\ \min_{\p_{x_{\leq n}}}\L(\p_{x_{\leq n}};\ x_{\leq n})$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: Supposing our prediction distribution $P(x_{&amp;gt;n}\mid x_{\leq n})$ is intractable, can we obtain a variational approximation of this probability?&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In machine learning, model parameters are hypotheses. Bayesian machine learning often seeks to convert a parametrized model like $p_\t(x,z)$ into a Bayesian model by putting a prior on $\T$.&lt;/p&gt;
&lt;p&gt;Let $P(x,z,\t) = p_\t(x,z)P(\t)$ where $P(\t)$ is the prior probability of $\t$. Instead of finding a single $\t^*$ which maximizes $p_\t(x)$, we are interested in the posterior $P(\t \mid x)$ and our average prediction of the latent variable, $P(z\mid x)$.&lt;/p&gt;
&lt;p&gt;This is the same as what I outlined above, replacing $x_{\leq n} \mapsto x$, $z\mapsto\t$ and $x_{&amp;gt;n}\mapsto z$.&lt;/p&gt;
&lt;h1 id=&#34;appendix&#34;&gt;Appendix&lt;/h1&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
&amp;amp; \L(\t,\p;\ x) \\&lt;br&gt;
=\ &amp;amp; \Er(\t,\p;\ x) + \M(\t;\ x) \\&lt;br&gt;
=\ &amp;amp; \kl{q_\p(z)}{p_\t(z\mid x)} + \log \par{1/p_\t(x)} \\&lt;br&gt;
=\ &amp;amp; \int_\Z q_\p(z) \log\par{\frac{q_\p(z)}{p_\t(z\mid x)}}\ \d z + \int_\Z q_\p(z)\log \par{\frac{1}{p_\t(x)}}\ \d z  \\&lt;br&gt;
=\ &amp;amp; \int_\Z q_\p(z) \log\par{\frac{q_\p(z)}{p_\t(x\mid z)p_\t(z)/p_\t(x)}\cdot\frac{1}{p_\t(x)}}\ \d z \\&lt;br&gt;
=\ &amp;amp; \int_\Z q_\p(z) \log\par{\frac{q_\p(z)}{p_\t(z)}\cdot\frac{1}{p_\t(x\mid z)}}\ \d z \\&lt;br&gt;
=\ &amp;amp; \int_\Z q_\p(z) \log\par{\frac{q_\p(z)}{p_\t(z)}}\ \d z + \int_\Z q_\p(z) \log\par{\frac{1}{p_\t(x\mid z)}}\ \d z  \\&lt;br&gt;
=\ &amp;amp;  \kl{q_\p(z)}{p_\t(z)} + \E_{z\sim q_\p(z)}\left[\log\par{1/p_\t(x \mid z)}\right] \,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Ideal Gas Entropy Derivation</title>
      <link>https://danabo.github.io/blog/posts/ideal-gas-entropy-derivation/</link>
      <pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/ideal-gas-entropy-derivation/</guid>
      <description>&lt;p&gt;Derivation of the change in entropy formula for an ideal gas (used in the &lt;a href=&#34;https://danabo.github.io/blog/posts/carnot-cycle/&#34;&gt;Carnot Cycle&lt;/a&gt; post) from state space volumes. Discussion about connections between the observer&amp;rsquo;s information about the gas and how that relates to the reversibility of transformations applied to the gas.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\s}{\sigma}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\th}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\d}{\delta}&lt;br&gt;
\newcommand{\dd}{\text{d}}&lt;br&gt;
\newcommand{\pd}{\partial}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\G}{\Gamma}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\N}{\mb{N}}&lt;br&gt;
\newcommand{\H}{\mc{H}}&lt;br&gt;
\newcommand{\L}{\Lambda}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\tup}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\par}{\tup}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\btup}[1]{\left[#1\right]}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[2]{#1_{\mid #2}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\restr}[1]{_{\mid{#1}}}&lt;br&gt;
\newcommand{\dt}{{\D t}}&lt;br&gt;
\newcommand{\Dt}{{\D t}}&lt;br&gt;
\newcommand{\ddT}{{\delta T}}&lt;br&gt;
\newcommand{\Mid}{\,\middle|\,}&lt;br&gt;
\newcommand{\qed}{\ \ \blacksquare}&lt;br&gt;
\newcommand{\p}{\vec{p}}&lt;br&gt;
\newcommand{\q}{\vec{q}}&lt;br&gt;
$$&lt;/p&gt;
&lt;h1 id=&#34;observer-information-and-reversibility&#34;&gt;Observer information and reversibility&lt;/h1&gt;
&lt;p&gt;In this post I will interpret entropy as measuring modeling uncertainty about some system, as in &lt;a href=&#34;https://danabo.github.io/blog/posts/the-reversibility-problem/&#34;&gt;The Reversibility Problem&lt;/a&gt;, &lt;a href=&#34;https://danabo.github.io/blog/posts/why-doesnt-uncopying-defeat-the-2nd-law/&#34;&gt;Why Doesn&amp;#39;t Uncopying Defeat The 2nd Law&lt;/a&gt; and &lt;a href=&#34;https://danabo.github.io/blog/posts/reversible-szilard-cycle-problem/&#34;&gt;Reversible Szilard Cycle Problem&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To recap, the state of some system we are modeling is described by a real-valued tuple $\o  = (\o_1,\o_2,\dots,\o_{2n}) = (q_1, q_n, p_1, p_n) \in \O \subseteq \R^{2n}$, where $\O$ is the state space of that system. The system&amp;rsquo;s time-evolution is defined by a family of functions, $\set{\t_\Dt\mid\Dt\in\R}$, called propagators, where each $\t_\Dt:\O\to\O$ maps states at time $t$ to time $t+\Dt$ (for all $t\in\R$, making these propagators time-independent.)&lt;/p&gt;
&lt;p&gt;Suppose we know that the system is initialized in some state region $\L_0 \subseteq \O$, i.e. we know that $\o_0 \in \L_0$ where $\o_0 \in \O$ is the initial state of the system. We know this either by performing a measurement on the system which produces only partial information, or by assumption. Then at any time $t$, we know that the state of the system is $\o_t \in \L_t = \t_t(\L_0)$, assuming the propagator $\t_t$ is the correct time-evolution of the system.&lt;/p&gt;
&lt;p&gt;Let $\mu$ be a uniform measure on $\O$ (need not be normalized), i.e. $\mu$ is defined by some constant density function on $\O$ (see &lt;a href=&#34;https://danabo.github.io/blog/posts/liouvilles-theorem/#uniform-measure&#34;&gt;Liouville&amp;#39;s Theorem#uniform-measure&lt;/a&gt;). Then by Liouville $\mu$ is uniform under any canonical change of coordinates. See &lt;a href=&#34;https://danabo.github.io/blog/posts/liouvilles-theorem/#measure-preservation&#34;&gt;Liouville&amp;#39;s Theorem#measure-preservation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Given two state regions, $\L_A\subseteq\O$ and $\L_B\subseteq\O$, we can quantify their relative size difference with the fraction $\mu(\L_A)/\mu(\L_B)$. Or expressed in the unit of &lt;em&gt;bits&lt;/em&gt;,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\D h = \lg\tup{\frac{\mu(\L_A)}{\mu(\L_B)}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;(where $\lg = \log_2$ is log base 2), which is positive when $\mu(\L_A) &amp;gt; \mu(\L_B)$ and $0$ when $\mu(\L_A) = \mu(\L_B)$. Then $\D h$ is the number of halvings to go from $\L_A$ to $\L_B$. See &lt;a href=&#34;https://danabo.github.io/blog/posts/bayesian-information-theory/&#34;&gt;Bayesian information theory&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Why do we care about the quantity $\D h$ in the context of thermodynamics? Because of the connection to (ir)reversibility of the process in question. But what is that connection? Suppose our goal is to have a system starting in state region $\L_0$ at time $0$ to reliably end up in the state region $\L_\text{final}$ at time $t$ (i.e. we want $\t_t(\L_0) \subseteq \L_\text{final}$), then by Liouville we must have $\mu(\L_0)=\mu(\t_t(\L_0)) \leq \mu(\L_\text{final})$ (if the system is isolated during this process). This is a necessary but not sufficient condition for what I&amp;rsquo;ll call reliability.&lt;/p&gt;
&lt;p&gt;The formal problem statement is to choose a propagator family $\set{\t_\Dt\mid\Dt\in\R}$ (equivalently a Hamiltonian) that minimizes $\mu(\t_t(\L_0) - \L_\text{final})$ (minimize the quantity of $\t_t(\L_0)$ outside of $\L_\text{final}$). We are restricted to physically valid propagators which also preserve the system&amp;rsquo;s internal dynamics. E.g. the particles of a gas collide in the way the usually do, but we have freedom to chose how the container morphs over time (the container interacts with the gas particles via an external potential). See &lt;a href=&#34;https://danabo.github.io/blog/posts/the-reversibility-problem/#naive-formulation&#34;&gt;The Reversibility Problem#naive-formulation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since $\D h = \lg\tup{\mu(\L_\text{final})/\mu(\L_0)}=\lg\tup{\mu(\L_\text{final})/\mu(\t_t(\L_0))}$, if $\D h = \lg\tup{\mu(\L_\text{final})/\mu(\L_0)} &amp;lt; 0$, then we know that there is no physically valid time-evolution function (obeying some Hamiltonian) which reliably maps $\L_0$ into a subset of $\L_\text{final}$ without external interaction. No matter what propagators we choose, some of the trajectories starting in $\L_0$ must end up outside of $\L_\text{final}$.&lt;/p&gt;
&lt;p&gt;On the other hand, if $\D h \geq 0$, then a perfectly reliable process from $\L_0$ into $\L_\text{final}$ is not ruled out, but that is still not enough to conclude that perfect reliability is achievable. See &lt;a href=&#34;https://danabo.github.io/blog/posts/why-doesnt-uncopying-defeat-the-2nd-law/&#34;&gt;Why Doesn&amp;#39;t Uncopying Defeat The 2nd Law&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Take a moment to compare the statement, &amp;ldquo;$\D h &amp;lt; 0$ implies that perfect reliability of the $\L_0$-to-$\L_\text{final}$ transition is impossible,&amp;rdquo; with the &lt;a href=&#34;https://en.wikipedia.org/wiki/The_2nd_Law&#34;target=&#34;_blank&#34;&gt;2nd law of thermodynamics&lt;/a&gt; which roughly states, &amp;ldquo;entropy of an isolated system cannot decrease.&amp;rdquo;&lt;/p&gt;
&lt;h1 id=&#34;observer-information-as-entropy&#34;&gt;Observer information as entropy?&lt;/h1&gt;
&lt;p&gt;Hopefully the above discussion elucidated the connection between an experimenter&amp;rsquo;s state of information about a system and the reversibility of that system (from the experimenter&amp;rsquo;s perspective). This motivates our interest in the quantity $\lg(\mu(\L_\text{final}) / \mu(\L_\text{initial}))$ when our system is an $N$-particle gas which we want reliably transition from $\L_\text{initial}$ to $\L_\text{final}$ for the purposes of extracting work from heat energy, as illustrated in the &lt;a href=&#34;https://danabo.github.io/blog/posts/carnot-cycle/&#34;&gt;Carnot Cycle&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the Carnot cycle, we have a gas described by the macroscopic quantities of (spatial volume) $V$, temperature $T$, and number of particles $N$. This gas is operated on by a sequence of processes (phases), each of which have an initial gas state $(V_i, T_i, N_i)$ and final gas state $(V_f, T_f, N_f)$, where $N_i=N_f=N$ for a closed gas. For the remainder of this post, assume that $N$ is held fixed.&lt;/p&gt;
&lt;p&gt;For the purposes of determining the thermodynamic efficiency of the Carnot cycle, we used the quantity&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
nR\log\par{\frac{V_f}{V_i}} + nC_V\lg\par{\frac{T_f}{T_i}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is defined as the thermodynamic change in entropy of the gas. Here, $C_V$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Molar_heat_capacity&#34;target=&#34;_blank&#34;&gt;molar heat capacity at a constant volume&lt;/a&gt; (a constant that depends on the type of gas being considered), $R$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gas_constant&#34;target=&#34;_blank&#34;&gt;gas constant&lt;/a&gt;, and $n$ is the number of moles of gas, i.e. $n = N/A$ where $A$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Avogadro_number&#34;target=&#34;_blank&#34;&gt;Avogadro number&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;References for change in entropy formula:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Entropy#Cooling_and_heating&#34;&gt;https://en.wikipedia.org/wiki/Entropy#Cooling_and_heating&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.grc.nasa.gov/WWW/K-12/airplane/entropy.html&#34;&gt;https://www.grc.nasa.gov/WWW/K-12/airplane/entropy.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The next section of this post will derive this change in entropy formula. We suppose that the macroscopic state, or macrostate, $(V, T, N)$ identifies the gas&amp;rsquo;s microscopic state, or microstate, $\o \in\O$ as being in the state region comprised of all microstates which have volume $V$, temperature $T$, and number of particles $N$. In other words, macrostate is a state region (i.e. a set of states). Let $\L(V, T, N)\subseteq\O$ be the macrostate (set of states) for $(V, T, N)$.&lt;/p&gt;
&lt;p&gt;With $\mu$ being a uniform measure on $\O$, we do indeed find that&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lg\par{\frac{\mu(\L(V_f, T_f, N_f)}{\mu(\L(V_i, T_i, N_i)}} \propto nR\log\par{\frac{V_f}{V_i}} + nC_V\lg\par{\frac{T_f}{T_i}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;(This relationship ends up being approximate.)&lt;/p&gt;
&lt;h1 id=&#34;derivation-of-ideal-gas-entropy&#34;&gt;Derivation of ideal gas entropy&lt;/h1&gt;
&lt;p&gt;This derivation is similar to the one in &amp;ldquo;Statistical Physics of Particles&amp;rdquo; by Kardar, section 4.4 &amp;ldquo;The Ideal Gas&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Let $\O$ be the state space of all $N$ particle gasses, where $(\q,\p)=(\q_1,\dots,\q_N,\p_1,\dots,\p_N)\in\O$ with $\q_i$ and $\p_i$ being scalars, pairs, or 3-tuples, depending on whether we are working in 1D, 2D or 3D space. Let $D$ be the dimensionality of space so that $\O=\R^{DN}$. Each $\o\in\O$ is a microstate.&lt;/p&gt;
&lt;p&gt;We have an ideal gas of $N$ particles at total kinetic energy $E$ and confined to a container with spatial volume $V$. This defines a macrostate $\L(E,V,N)\subseteq\O$, which is the set of all microstates satisfying these criteria, specifically&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\L(E,V,N) = \Bigg\{(\q,\p)\in\O \,\,\Bigg\vert\,  &amp;amp;\Big(\fa i\in\set{1,\dots,N}:\q_i \in \text{container}\Big)\\&lt;br&gt;
&amp;amp; \text{and}\ \par{\frac{1}{2m}\sum_{i=1}^N {\p_i}^2 \in [E,E+\e]}\Bigg\}&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;where $\e&amp;gt;0$ is our uncertainty about the total energy (our measurement precision) and $[E,E+\e]$ is the closed interval from $E$ to $E+\e$.&lt;/p&gt;
&lt;p&gt;Let $\mu$ be the uniform unit measure on $\O$ (defined by constant density of 1 everywhere) w.r.t. our chosen unit of spatial length, so that the unit-hypercube has a $\mu$-volume of 1. We want to find the volume of our macrostate $\mu(\L(E,V,N))$. We can do this by noticing that, (1) each particle position $q_i$ independently occupies any point in the container with volume $V$, and (2) the momenta are constrained to lie on a spherical shell containing the surface of the $DN$-ball with radius $r = \sqrt{2mE}$. This shell has thickness $\D r = \sqrt{2m(E+\e)} - \sqrt{2mE}$.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/N-sphere#Volume_and_surface_area&#34;target=&#34;_blank&#34;&gt;hypervolume&lt;/a&gt; of the unit $n$-ball with radius $r$ (interior volume of sphere in $n$ dimensions) is&lt;br&gt;
$$&lt;br&gt;
B_n(r) = \frac{\pi^{\frac{n}{2}}}{\G\tup{\frac{n}{2}+1}}r^{n}\,,&lt;br&gt;
$$&lt;br&gt;
where $\G(x)$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gamma_function&#34;target=&#34;_blank&#34;&gt;gamma function&lt;/a&gt; which is the continuous extension of factorial where $\G(n) = (n-1)!$ for $n\in\N$ (excluding 0). Note that $\G(n+\frac{1}{2})=\frac{(2n)!}{4^n n!}\sqrt{\pi}$ for $n\in\N$ (&lt;a href=&#34;https://en.wikipedia.org/wiki/Particular_values_of_the_gamma_function&#34;target=&#34;_blank&#34;&gt;source&lt;/a&gt;). We will only be concerned with these integer and half-integer values of the gamma function.&lt;/p&gt;
&lt;p&gt;For our purposes, $n=DN$, the total number of dimensions in state space, and not the number of moles of gas.&lt;/p&gt;
&lt;p&gt;The macrostate hypervolume is&lt;br&gt;
$$&lt;br&gt;
\mu(\L(E,V,N)) = V^N \cdot\left[B_{DN}\par{\sqrt{2m(E+\e)}} - B_{DN}\par{\sqrt{2mE}}\right]\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Expanding out the term in square brackets, we get&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
&amp;amp; B_{n}\par{\sqrt{2m(E+\e)}} - B_{n}\par{\sqrt{2mE}} \\&lt;br&gt;
=\ &amp;amp; \frac{\pi^{\frac{n}{2}}}{\G\tup{\frac{n}{2}+1}}\par{2m}^{n/2}\left[\par{E+\e}^{n/2} - E^{n/2}\right]\,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;We can make a convenient approximation when $\e$ is small. Using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_theorem#Newton%27s_generalized_binomial_theorem&#34;target=&#34;_blank&#34;&gt;generalized binomial theorem&lt;/a&gt;, we have&lt;br&gt;
$$\begin{aligned}&lt;br&gt;
&amp;amp; (E+\e)^{n/2} - E^{n/2} \\&lt;br&gt;
=\ &amp;amp; \left[\sum_{k=0}^{\infty} {n/2\choose k}E^{n/2-k}{\e}^k\right] - E^{n/2} \\&lt;br&gt;
=\ &amp;amp; \par{E^{n/2} - E^{n/2}} + n/2\par{E^{n/2-1}}\e + {n/2\choose 2}E^{n/2-2}{\e}^2 + \dots \\&lt;br&gt;
\approx\ &amp;amp; n/2\par{E^{n/2-1}}\e&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;where $\e^k\approx 0$ for $k\geq2$ if we assume that $\e$ is small enough so that higher powers of $\e$ are negligible (this is a 1st order approximation).&lt;/p&gt;
&lt;p&gt;(The generalized binomial theorem reduces to the standard binomial theorem for integer powers, i.e. when $n/2$ is an integer we have ${n/2\choose k}=0$ for all $k&amp;gt;n/2$.)&lt;/p&gt;
&lt;p&gt;Plugging in our approximation (with $n=DN$), we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(\L(E,V,N)) \approx V^N\frac{\pi^{\frac{DN}{2}}}{\G\tup{\frac{DN}{2}+1}}\par{2m}^{DN/2}\frac{DN}{2}E^{DN/2-1}\e&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Given two macrostates, $\L(E_i,V_i,N)$ and $\L(E_f,V_f,N)$, the log-ratio between their $\mu$-sizes is approximately&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\D h = \lg\par{\frac{\mu(\L(E_f,V_f,N))}{\mu(\L(E_i,V_i,N))}} \approx N\lg\par{\frac{V_f}{V_i}} + (DN/2-1)\lg\par{\frac{E_f}{E_i}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We more or less have the expression we want. It is now a matter of transforming some constants and scaling.&lt;/p&gt;
&lt;p&gt;As stated above, $\D h$ is a quantity with the unit &lt;em&gt;bits&lt;/em&gt; (log base 2). Boltzmann defined &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula&#34;target=&#34;_blank&#34;&gt;thermodynamic entropy&lt;/a&gt; as $S = k_B\ln W$ where $W$ is the normalized phase volume (i.e. probability) of some state region (i.e. macrostate) and $k_B$ is the  &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann_constant&#34;target=&#34;_blank&#34;&gt;Boltzmann constant&lt;/a&gt;. Then $\D S$ and $\D h$ differ by a scaling factor, specifically $\D S = \frac{k_B}{\lg e}\D h$. Thermodynamic entropy $S$ has the units $J/K$ (Joules per Kelvin).&lt;/p&gt;
&lt;p&gt;A few additional transformations gives us the final entropy equation. Let&amp;rsquo;s let $D=3$ (the gas is in 3D space), since the remaining transformations rely on this. We are using total energy $E$ instead of temperature to define our macrostates, but we can convert total energy to temperature using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Temperature#Kinetic_theory_approach&#34;target=&#34;_blank&#34;&gt;kinetic definition of temperature&lt;/a&gt;, which says $T \propto E/3N$, i.e. that temperature is proportional to the total kinetic energy averaged across all of the degrees of freedom of the system. Our gas has $3N$ degrees of freedom. The scaling factor determines the temperature units. In this case, we have  $T=\frac{2}{3Nk_B}E$. Then $T_f/T_i = E_f/E_i$ when $N_f=N_i$.&lt;/p&gt;
&lt;p&gt;Finally, we make use of the definitions of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gas_constant&#34;target=&#34;_blank&#34;&gt;gas constant&lt;/a&gt;, $R = Ak_B=\frac{N}{n}k_B$, and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Molar_heat_capacity#Monatomic_gases&#34;target=&#34;_blank&#34;&gt;monatomic constant volume molar heat capacity&lt;/a&gt;, $C_V = \frac{3}{2}R = \frac{3N}{2n}k_B$.&lt;/p&gt;
&lt;p&gt;Then&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\D S = k_B\frac{\D h}{\lg(e)} &amp;amp;\approx k_BN\ln\par{\frac{V_f}{V_i}} + k_B(3N/2-1)\ln\par{\frac{E_f}{E_i}} \\&lt;br&gt;
&amp;amp;= k_BN\lg\par{\frac{V_f}{V_i}} + k_B(3N/2-1)\lg\par{\frac{T_f}{T_i}} \\&lt;br&gt;
&amp;amp;\approx k_BN\ln\par{\frac{V_f}{V_i}} + k_B(3N/2)\ln\par{\frac{T_f}{T_i}} \\&lt;br&gt;
&amp;amp;= nR\ln\par{\frac{V_f}{V_i}} + nC_V\ln\par{\frac{T_f}{T_i}}\,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;The second approximation holds when $N$ is large so that $3N/2-1 \approx 3N/2$.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;As an amusing aside, we can view thermodynamic entropy as just having a different log base than $\D h$, where&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
S=k_B\frac{\lg W}{\lg(e)} = \frac{\lg W}{\frac{1}{k_B}\lg(e)} = \frac{\lg W}{\lg(e^{1/k_B})} = \log_{e^{1/k_B}} W\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We can see that thermodynamic entropy uses log base $e^{1/k_B} \approx \exp(7.24297 \times 10^{22})$, which is an insanely large number with $\log_{10}(\exp(7.24297 \times 10^{22})) \approx 3.1456\times 10^{22}$ digits in base 10. Then $\D S$ measures the number of times we divide $\mu$-size by $e^{1/k_B}$ when we transform between two phase regions, instead of the number of halvings, i.e. number of times we divide by 2.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://en.wikipedia.org/wiki/Natural_units&#34;target=&#34;_blank&#34;&gt;natural units&lt;/a&gt;, $k_B=1$.&lt;br&gt;
See &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann_constant#Natural_units&#34;&gt;https://en.wikipedia.org/wiki/Boltzmann_constant#Natural_units&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;temperature-and-uncertainty&#34;&gt;Temperature and uncertainty&lt;/h2&gt;
&lt;p&gt;It may not be intuitively obvious why state region size should depend on temperature.&lt;/p&gt;
&lt;p&gt;Hopefully it is intuitive that state region size depends on volume. If the spatial coordinate $q$ is confined to an interval, then the marginal size of that region is proportional to the length of the interval. I.e. the larger the container the more possible states the gas can be in.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../change_in_volume.jpg&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../change_in_volume.jpg&#34; width=&#34;400&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
On the other hand, the temperature is proportional to the kinetic energy of each degree of freedom which is proportional to momentum squared. In other words, moving our state region up and down along a $p$ axis changes the temperature of the gas accordingly. But we are only translating the state region, rather than stretching or shrinking it, so how does the size of the state region change?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../change_in_momentum.jpg&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../change_in_momentum.jpg&#34; width=&#34;400&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
Indeed, for a gas with one particle in 1D space, i.e. $DN = 1$, the state region size does not depend on temperature.&lt;/p&gt;
&lt;p&gt;When $DN &amp;gt; 1$, changing the temperature (and thus total kinetic energy) changes the radius of a hyperspherical shell in $DN$-dimensional momentum space, which changes the hyper-surfacearea of that shell (which is proportional to the hypervolume of the shell when it has small thickness).&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/momentum_sphere.jpg&#34; alt=&#34;&#34;&gt;
&lt;/p&gt;
&lt;h1 id=&#34;absolute-entropy&#34;&gt;Absolute entropy&lt;/h1&gt;
&lt;p&gt;I want to point out that entropy, $S$, and change in entropy, $\D S$, are very different beasts.&lt;/p&gt;
&lt;p&gt;Going back to our information-theoretic perspective, we have $\D h = \lg\tup{\mu(\L_A)/\mu(\L_B)}$ is the change in information (i.e. information gain) by going from state region $\L_A$ to state region $\L_B$. This implies we are viewing $h_A=\lg\par{1/\mu(\L_A)}$ and $h_B=\lg\par{1/\mu(\L_B)}$ as absolute quantities of information that $\L_A$ and $\L_B$ are respectively worth. This would be a valid perspective if the measure $\mu$ were a probability measure where $\mu(\O)=1$. However, if the domain $\O$ is, say $\R^n$, then there does not exist a uniform probability measure. If we tried to normalized a given uniform measure $\mu$, we&amp;rsquo;d find that $\lg\par{\mu(\O)/\mu(\L_A)}=\infty$, i.e. $\L_A$ is worth an infinite amount of information.&lt;/p&gt;
&lt;p&gt;One reason to prefer to only consider changes in information $\D h$ rather than absolute information $h$ is so that we are free to put a uniform measure on $\R^n$, have it be unnormalized, and don&amp;rsquo;t need to care about the arbitrary choice of which uniform measure to use (each defined by a different choice of constant density). With Liouville&amp;rsquo;s theorem we are assured that uniform measures are uniform in all canonical coordinate systems. Then $\D h$ is a uniquely determined quantity just by stating that we are using a uniform measure on state space (but $h$ depends on our particular choice of uniform measure).&lt;/p&gt;
&lt;p&gt;Importantly, even when physicists are calculating an entropy $S$ for a system, this is not absolute in the sense that I mean. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula&#34;target=&#34;_blank&#34;&gt;Boltzmann entropy formula&lt;/a&gt; is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
S = k_B\ln W&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $W$ is the &lt;em&gt;probability&lt;/em&gt; of the corresponding macrostate we wish to calculate the entropy for. Then $W$, and thus $S$, depends on the set of allowed macrostates of the system, because that determines the normalizing factor which makes our measure over states a probability measure. However, the change in entropy $\D S = k_B\ln \par{W_2/W_1}$  between two macrostates does not depend on the set of allowed macrostates, since the normalizing factor gets canceled out in the fraction $W_2/W_1$. See &lt;a href=&#34;https://danabo.github.io/blog/posts/liouvilles-theorem/#entropy-and-the-bertrand-paradox&#34;&gt;Liouville&amp;#39;s Theorem#entropy-and-the-bertrand-paradox&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Going back to the example of a gas in a container, if we are only considering gasses with temperature and volume within some bounded ranges, we can put a uniform probability distribution over all such gasses. The bounds we place on allowed temperatures and volumes will determine the normalizing factor on our measure, and thus scale $W$ for each macrostate accordingly.&lt;/p&gt;
&lt;p&gt;If we wanted to place no bounds, then we cannot have the measure be uniform and be a probability measure. We would either be stuck with the arbitrary choice of some non-uniform probability measure out of an infinite space of possible measures, or with the arbitrary choice of unnormalized uniform measure. The latter case is equivalent to defining absolute entropy as $S = k_B\ln W + \text{constant}$, where the scaling factor on the uniform measure determines the constant offset. We can then see that this constant offset is quite irrelevant. Indeed, in the case of an ideal gas, the efficiency analysis of the &lt;a href=&#34;https://danabo.github.io/blog/posts/carnot-cycle/&#34;&gt;Carnot Cycle&lt;/a&gt; depends only on $\D S$.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Liouville Supplemental: Bertrand Paradox</title>
      <link>https://danabo.github.io/blog/posts/liouville-supplemental-bertrand-paradox/</link>
      <pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/liouville-supplemental-bertrand-paradox/</guid>
      <description>&lt;p&gt;I reframe the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bertrand_paradox_%28probability%29&#34;target=&#34;_blank&#34;&gt;Bertrand paradox&lt;/a&gt; as the statement that uniformity of measure is relative to choice of coordinate system.&lt;/p&gt;
&lt;p&gt;The objective-Bayesian approach to the  &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayesian_epistemology#Problem_of_priors&#34;target=&#34;_blank&#34;&gt;problem of priors&lt;/a&gt; is to assign a maximally uninformative prior to the given possibility space. What is considered maximally uninformative can be derived with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Principle_of_maximum_entropy&#34;target=&#34;_blank&#34;&gt;maximum entropy principle&lt;/a&gt; - a generalization of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Principle_of_indifference#Application_to_continuous_variables&#34;target=&#34;_blank&#34;&gt;principle of indifference&lt;/a&gt;. In many cases this ends up being a uniform prior. However, we run into a problem since uniformity is relative to choice of coordinates. This is relevant to physics since there is no preferred coordinate system to work in.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\b}{\beta}&lt;br&gt;
\newcommand{\s}{\sigma}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\th}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\d}{\delta}&lt;br&gt;
\newcommand{\dd}{\text{d}}&lt;br&gt;
\newcommand{\pd}{\partial}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\r}{\rho}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\G}{\Gamma}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\H}{\mc{H}}&lt;br&gt;
\newcommand{\L}{\mc{L}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\tup}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[2]{#1_{\mid #2}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\restr}[1]{_{\mid{#1}}}&lt;br&gt;
\newcommand{\dt}{{\D t}}&lt;br&gt;
\newcommand{\Dt}{{\D t}}&lt;br&gt;
\newcommand{\ddT}{{\delta T}}&lt;br&gt;
\newcommand{\Mid}{\,\middle|\,}&lt;br&gt;
\newcommand{\qed}{\ \ \blacksquare}&lt;br&gt;
\newcommand{\diff}[2]{\frac{\dd #1}{\dd #2}}&lt;br&gt;
\newcommand{\diffop}[1]{\frac{\dd}{\dd #1}}&lt;br&gt;
\newcommand{\pdiff}[2]{\frac{\pd #1}{\pd #2}}&lt;br&gt;
\newcommand{\pdiffop}[1]{\frac{\pd}{\pd #1}}&lt;br&gt;
\newcommand{\evalat}[1]{\left. #1 \right|}&lt;br&gt;
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A measure is &lt;strong&gt;uniform&lt;/strong&gt; if its corresponding density function is constant. We need to specify which coordinate system the density is constant with respect to.&lt;/p&gt;
&lt;p&gt;In the Bertrand paradox, we are asked to randomly draw chords (a line connecting two points on the circumference of a circle) from the unit circle. The paradox is that depending on how we construct our chords, the sampling statistics of the chords will be different. Is it implied that the phrase &amp;ldquo;random draw&amp;rdquo; implies a uniform sampling distribution.&lt;/p&gt;
&lt;p&gt;We are given three different sampling methods as a demonstration.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pick two points on the circumfrence and draw a chord&lt;/li&gt;
&lt;li&gt;Pick a radius (angle of the radius) and a point on the radius and draw a chord perpendicular to the radius&lt;/li&gt;
&lt;li&gt;Pick a point in the circle and make that the midpoint of the chord&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can reframe these sampling methods as different coordinate systems over the same space of chords:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$(\a, \b) \in \set{(\a&#39;,\b&#39;)\in[0,2\pi)^2 \mid \b&#39; \geq \a&#39;}$&lt;/li&gt;
&lt;li&gt;$r \in (0,1],\ \th \in [0,2\pi)$&lt;/li&gt;
&lt;li&gt;$(x, y) \in \overline{B}_1(0,0) = \set{(x&#39;,y&#39;) \in \R^2 \mid x&#39;^2+y&#39;^2 \leq 1}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s calculate the Jacobians for some of the transformations between these coordinates.&lt;/p&gt;
&lt;p&gt;$(1 \longrightarrow 2)$&lt;br&gt;
$\th(\a,\b)=\frac{\a+\b}{2}$&lt;br&gt;
$r(\a,\b)=\sqrt{\frac{1}{2}+\frac{1}{2}\cos(\b-\a)}$&lt;br&gt;
(&lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_cosines&#34;target=&#34;_blank&#34;&gt;Law of cosines&lt;/a&gt;,  where $c$ is the chord length and $r^2 = 1-(c/2)^2$)&lt;br&gt;
$$\begin{aligned}&lt;br&gt;
\pdiff{(\th,r)}{(\a,\b)}&amp;amp;=\det\pmatrix{\pdiff{\th}{\a}&amp;amp;\pdiff{\th}{\b}\\\pdiff{r}{\a}&amp;amp;\pdiff{r}{\b}}\\&amp;amp;=\det\pmatrix{\frac{1}{2}&amp;amp;\frac{1}{2}\\ -\frac{\sin (\alpha -\beta )}{2^{3/2} \sqrt{\cos (\alpha -\beta )+1}} &amp;amp; \frac{\sin (\alpha -\beta )}{2^{3/2} \sqrt{\cos (\alpha -\beta )+1}}}\\&amp;amp;=\frac{\sin (\alpha -\beta )}{2^{3/2} \sqrt{\cos (\alpha -\beta )+1}}&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$(2 \longrightarrow 3)$&lt;br&gt;
$x(r,\th) = r\cos\th$&lt;br&gt;
$y(r,\th) = r\sin\th$&lt;br&gt;
$$\begin{aligned}&lt;br&gt;
\pdiff{(x,y)}{(\th,r)}&amp;amp;=\pdiff{(x,y)}{(r,\th)}\\&amp;amp;=\det\pmatrix{\pdiff{x}{r}&amp;amp;\pdiff{x}{\th}\\\pdiff{y}{r}&amp;amp;\pdiff{y}{\th}}\\&amp;amp;=\det\pmatrix{\cos\th &amp;amp; -r\sin\th \\ \sin\th &amp;amp; r\cos\th} \\&amp;amp;= r\cos^2\th + r\sin^2\th \\&amp;amp;= r&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$(1 \longrightarrow 3)$&lt;br&gt;
$x(\a,\b)=r\cos\th=\sqrt{\frac{1}{2}+\frac{1}{2}\cos(\b-\a)}\cdot \cos\tup{\frac{\a+\b}{2}}$&lt;br&gt;
$y(\a,\b)=r\sin\th=\sqrt{\frac{1}{2}+\frac{1}{2}\cos(\b-\a)}\cdot \sin\tup{\frac{\a+\b}{2}}$&lt;br&gt;
$$\begin{aligned}&lt;br&gt;
\pdiff{(x,y)}{(\a,\b)}&amp;amp;=\pdiff{(\th,r)}{(\a,\b)}\cdot\pdiff{(x,y)}{(\th,r)}\\&amp;amp;=\frac{\sin (\alpha -\beta )}{2^{3/2} \sqrt{\cos (\alpha -\beta )+1}}\cdot r\\&amp;amp;=\frac{\sin (\alpha -\beta )}{2^{3/2} \sqrt{\cos (\alpha -\beta )+1}} \cdot \sqrt{\frac{1}{2}+\frac{1}{2}\cos(\b-\a)} \\&amp;amp;= \frac{1}{4} \sin (\alpha -\beta )&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Inverse Jacobians are just reciprocals, so $\pdiff{(\a,\b)}{(x,y)}=1/\pdiff{(x,y)}{(\a,\b)}$ and $\pdiff{(r,\th)}{(x,y)}=1/\pdiff{(x,y)}{(r,\th)}$.&lt;br&gt;
Swapping variables negates the Jacobian, so $\pdiff{(r,\th)}{(\a,\b)}=-\pdiff{(\th,r)}{(\a,\b)}$.&lt;/p&gt;
&lt;p&gt;Useful coordinate relationships:&lt;br&gt;
$r = \sqrt{x^2+y^2}$&lt;br&gt;
$\frac{1}{4} \sin (\alpha -\beta ) = \frac{1}{2} \sqrt{x^2+y^2}\sqrt{1-x^2-y^2}$&lt;/p&gt;
&lt;p&gt;Let $\l_1$ be a uniform measure in coordinate #1 with constant density $\r_1$.&lt;br&gt;
Let $\l_2$ be a uniform measure in coordinate #2 with constant density $\r_2$.&lt;br&gt;
Let $\l_3$ be a uniform measure in coordinate #3 with constant density $\r_3$.&lt;/p&gt;
&lt;p&gt;If $R$ is a set of points in Cartesian coordinates (#3), let $\vtup{r,\th}(R) = \set{(r(x,y),\th(x,y)) \mid (x,y) \in R}$ and $\vtup{\a,\b}(R) = \set{(\a(x,y),\b(x,y)) \mid (x,y) \in R}$ be the transformed set into the other coordinate systems.&lt;/p&gt;
&lt;p&gt;The measure of a set $R$ in Cartesian coordinates (#3), according to each measure $\l_1,\l_2$ and $\l_3$, is&lt;br&gt;
$$&lt;br&gt;
\l_3(R) = \int_R \r_3\ \dd x\ \dd y&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\l_2(R) &amp;amp;= \int_{\vtup{r,\th}(R)} \r_2\ \dd r\ \dd \th \\&amp;amp;= \int_R \r_2\abs{\pdiff{(r,\th)}{(x,y)}}\ \dd x\ \dd y \\&amp;amp;= \int_R \frac{\r_2}{r}\ \dd x\ \dd y \\&amp;amp;=\int_R \frac{\r_2}{\sqrt{x^2+y^2}}\ \dd x\ \dd y&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\l_1(R) &amp;amp;= \int_{\vtup{\a,\b}(R)} \r_1\ \dd \a\ \dd \b \\&amp;amp;= \int_R \r_1\abs{\pdiff{(\a,\b)}{(x,y)}}\ \dd x\ \dd y \\&amp;amp;= \int_R \frac{4\r_1}{\sin (\alpha -\beta )}\ \dd x\ \dd y \\&amp;amp;=\int_R \frac{2\r_1}{\sqrt{x^2+y^2}\sqrt{1-x^2-y^2}}\ \dd x\ \dd y&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Thus the density function for $\l_1$ in Cartesian is $\tilde{\r}_1(x,y)=\frac{2\r_1}{\sqrt{x^2+y^2}\sqrt{1-x^2-y^2}}$ and the density function for $\l_2$ in Cartesian is $\tilde{\r}_2(x,y)=\frac{\r_2}{\sqrt{x^2+y^2}}$. The measures $\l_1$ and $\l_2$ have non-constant density functions in Cartesian coordinates, and so are non-uniform in Cartesian coordinates. Clearly what is considered a random draw depends on our choice of coordinates.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Liouville Supplemental: Coordinate Transformations</title>
      <link>https://danabo.github.io/blog/posts/liouville-supplemental-coordinate-transformations/</link>
      <pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/liouville-supplemental-coordinate-transformations/</guid>
      <description>&lt;p&gt;This is supplemental material for &lt;a href=&#34;https://danabo.github.io/blog/posts/liouvilles-theorem/&#34;&gt;Liouville&amp;#39;s Theorem&lt;/a&gt;. Specifically I go through a few examples of phase space transformations, canonical and non-canonical. I also show that we can turn arbitrary configuration space transformations into canonical phase space transformations, a result that will be useful for my discussion about the Bertrand paradox (&lt;a href=&#34;https://danabo.github.io/blog/posts/liouvilles-theorem/#the-bertrand-paradox&#34;&gt;Liouville&amp;#39;s Theorem#the-bertrand-paradox&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\b}{\beta}&lt;br&gt;
\newcommand{\s}{\sigma}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\th}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\d}{\delta}&lt;br&gt;
\newcommand{\dd}{\text{d}}&lt;br&gt;
\newcommand{\pd}{\partial}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\r}{\rho}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\G}{\Gamma}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\H}{\mc{H}}&lt;br&gt;
\newcommand{\L}{\mc{L}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\tup}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[2]{#1_{\mid #2}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\restr}[1]{_{\mid{#1}}}&lt;br&gt;
\newcommand{\dt}{{\D t}}&lt;br&gt;
\newcommand{\Dt}{{\D t}}&lt;br&gt;
\newcommand{\ddT}{{\delta T}}&lt;br&gt;
\newcommand{\Mid}{\,\middle|\,}&lt;br&gt;
\newcommand{\qed}{\ \ \blacksquare}&lt;br&gt;
\newcommand{\diff}[2]{\frac{\dd #1}{\dd #2}}&lt;br&gt;
\newcommand{\diffop}[1]{\frac{\dd}{\dd #1}}&lt;br&gt;
\newcommand{\pdiff}[2]{\frac{\pd #1}{\pd #2}}&lt;br&gt;
\newcommand{\pdiffop}[1]{\frac{\pd}{\pd #1}}&lt;br&gt;
\newcommand{\evalat}[1]{\left. #1 \right|}&lt;br&gt;
$$&lt;/p&gt;
&lt;h1 id=&#34;example-scaling&#34;&gt;Example: Scaling&lt;/h1&gt;
&lt;p&gt;A simple example to build intuition.&lt;/p&gt;
&lt;p&gt;First let&amp;rsquo;s look at a simple transformation which is not canonical. Define a scaling transformation $Q(q,p)=2q$ and $P(q,p)=2p$. Let&amp;rsquo;s also consider a specific Hamiltonian, $H(q,p)=\frac{1}{2}p^2$.   Then $q(Q,P) = \frac{1}{2}Q$ and $p(Q,P) = \frac{1}{2}P$, and $\tilde{H}(Q,P) = H(q(Q,P),p(Q,P))=\frac{1}{2}\tup{\frac{1}{2}P}^2 = \frac{1}{8}P^2$.&lt;/p&gt;
&lt;p&gt;Using the multivariate chain rule we have&lt;br&gt;
$$\begin{aligned}&lt;br&gt;
\diff{Q}{t}&amp;amp;=\pdiff{Q}{q}\diff{q}{t} + \pdiff{Q}{p}\diff{p}{t} \\&amp;amp;= \pdiff{Q}{q}\pdiff{H}{p} - \pdiff{Q}{p}\pdiff{H}{q} \\&amp;amp;= 2p = P&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;But $\pdiff{\tilde{H}}{P} = \frac{1}{4}P \neq P = \diff{Q}{t}$, and so we cannot write $\diff{Q}{t} = \pdiff{\tilde{H}}{P}$ in this coordinate system, at least if we want to keep the dynamics of the system the same.&lt;/p&gt;
&lt;p&gt;On the other hand, we can make the $Q$-space transformation $Q(q,p)=2q$ canonical by choosing a different $P$-space transformation, specifically $P(q,p)=p/2$. Then $q(Q,P) = \frac{1}{2}Q$ and $p(Q,P) = 2P$, and $\tilde{H}(Q,P) = H(q(Q,P),p(Q,P))=\frac{1}{2}\tup{2P}^2 = 2P^2$.&lt;/p&gt;
&lt;p&gt;Then we have&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\diff{Q}{t}&amp;amp;=\pdiff{Q}{q}\diff{q}{t} + \pdiff{Q}{p}\diff{p}{t} \\&amp;amp;= \pdiff{Q}{q}\pdiff{H}{p} - \pdiff{Q}{p}\pdiff{H}{q} \\&amp;amp;= 2p = 4P&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Thus $\pdiff{\tilde{H}}{P} = 4P = \diff{Q}{t}$, and so Hamilton&amp;rsquo;s equation is satisfied (clearly $\pdiff{\tilde{H}}{Q}=0=-\diff{P}{t}$).&lt;/p&gt;
&lt;p&gt;(This transformation is canonical for all Hamiltonians on 2D phase space)&lt;/p&gt;
&lt;h1 id=&#34;hamiltonian-specific-canonical&#34;&gt;Hamiltonian Specific Canonical&lt;/h1&gt;
&lt;p&gt;It is important to clarify the difference between a transformation being canonical for a particular Hamiltonian vs canonical for all Hamiltonians. The derivation in &lt;a href=&#34;https://danabo.github.io/blog/posts/liouvilles-theorem/#measure-preservation&#34;&gt;Liouville&amp;#39;s Theorem#measure-preservation&lt;/a&gt; is not specific to any Hamiltonian, and gives us conditions for canonical transformations that are Hamiltonian agnostic.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at a transformation that is canonical for some but not all Hamiltonians. The following system is a 1D harmonic oscillator (2D phase space). We will perform a Cartesian to polar transformation on phase space. Recall from calculus that Cartesian to polar coordinates has a non-unit Jacobian: $\pdiff{(Q,P)}{(q,p)}=P$ and so $\tilde{\r}(Q,P,t)=P\r(q(Q,P),p(Q,P),t)$ for any density $\r$. Yet in this particular example, we will find that Hamilton&amp;rsquo;s equations hold in the new coordinate system.&lt;/p&gt;
&lt;p&gt;Consider the Hamiltonian $H = \frac{1}{2}\tup{p^2+q^2}$. This is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Harmonic_oscillator&#34;target=&#34;_blank&#34;&gt;harmonic oscillator&lt;/a&gt; with spring mass $m=1$ and spring constant $k=1$.&lt;/p&gt;
&lt;p&gt;To solve the system, use Hamilton&amp;rsquo;s equations,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\diff{q}{t} = \pdiff{H}{p} = p\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\diff{p}{t} = -\pdiff{H}{q} = -q\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Solving the PDE system, we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
q(t) = q_0 \cos(t)+p_0 \sin(t)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(t) = -q_0 \sin(t)+p_0 \cos(t)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $(q(0),p(0))=(q_0,p_0)$. Basically, the trajectories of this system are circles in phase space centered at the origin. For a given initial condition $(q_0,p_0)$, the trajectory of the system is the circle passing through that point.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s transform to polar coordinates: $Q(q,p)=\tan^{-1}\tup{\frac{p}{q}}$ and  $P(q,p)=\sqrt{q^2+p^2}$, giving us the inverse transformation $q(Q,P)=P\cos Q$ and $p(Q,P)=P\sin Q$.&lt;/p&gt;
&lt;p&gt;In the transformed coordinate system, $\tilde{H}=\frac{1}{2}\tup{\tup{P\cos Q}^2+\tup{P\sin Q}^2}=\frac{1}{2}P^2$. If we blindly apply Hamilton&amp;rsquo;s equations in the transformed coordinates we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\diff{Q}{t} = \pdiff{\tilde{H}}{P} = P\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\diff{P}{t} = -\pdiff{\tilde{H}}{Q} = 0\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This Hamiltonian corresponds to a free particle at constant momentum. Solutions are $P(t)=P_0$ and $Q(t)=Q_0+P_0t$. These are circular paths in the original coordinate system, and so we have the same trajectories transformed.&lt;/p&gt;
&lt;p&gt;As mentioned above transforming 2D phase space to polar coordinates is not measure preserving and in general not a canonical transformation for all Hamiltonians. But for this particular Hamiltonian, our trajectories do not change their distance from the origin, i.e. radius (they trace out circles). The Cartesian to polar transformation is not measure preserving as the radius coordinate changes, but is measure preserving as the angle changes. This transformation preserves rotational symmetry, i.e. density is preserved along circular paths, and our Hamiltonian produces circular paths.&lt;/p&gt;
&lt;p&gt;An interesting question to consider: Do all the symmetries of a given Hamiltonian generate transformations canonical to that Hamiltonian? Then it would make sense that if one Hamiltonian obeys a symmetry and another doesn&amp;rsquo;t, then they wouldn&amp;rsquo;t share the same canonical transformations. Furthermore, we can recast Liouville&amp;rsquo;s theorem as stating that for Hamiltonians obeying time-translation symmetry (which are the Hamiltonians conserving energy), time-translations are canonical transformations.&lt;/p&gt;
&lt;h1 id=&#34;arbitrary-configuration-space-transformations&#34;&gt;Arbitrary Configuration-Space Transformations&lt;/h1&gt;
&lt;p&gt;A surprising result is that for all configuration space transformations $\vec{Q}(\vec{q})$, there is a momentum space transformation such that the combined phase space transformation is canonical. My derivation below follows  &lt;a href=&#34;https://nononsensebooks.com/cm/&#34;target=&#34;_blank&#34;&gt;No-Nonsense Classical Mechanics&lt;/a&gt; section 7.3.2.&lt;/p&gt;
&lt;p&gt;Suppose we are given some configuration space transformation $\vec{Q}(\vec{q})$ which does not depend on the conjugate momenta $\vec{p}$. We can derive the $\vec{P}(\vec{q}, \vec{p})$ needed to make $\vtup{\vec{Q},\vec{P}}(\vec{q}, \vec{p})$ a canonical transformation.&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;https://en.wikipedia.org/wiki/Lagrangian_mechanics&#34;target=&#34;_blank&#34;&gt;Lagrangian formulation of classical mechanics&lt;/a&gt;, momentum is defined as $p_i \df \pdiff{\L(\vec{q},\dot{\vec{q}})}{\dot{q_i}}$ where $\dot{q_i} = \diff{q_i}{t}$. The Lagrangian is valid (&lt;a href=&#34;https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation&#34;target=&#34;_blank&#34;&gt;Euler-Lagrangian equations&lt;/a&gt; hold) for all choice of configuration coordinates. Then $P_i \df \pdiff{\L(\vec{q}(\vec{Q}),\dot{\vec{q}}(\vec{Q},\dot{\vec{Q}}))}{\dot{Q_i}}$ where $\vec{q}(\vec{Q})$ is the inverse coordinate transformation and $\dot{q}_i(\vec{Q},\dot{\vec{Q}})=\diffop{t}\left[q_i(\vec{Q})\right]=\sum_{j=1}^n\pdiff{q_i}{Q_j}\diff{Q_j}{t}=\sum_{j=1}^n\pdiff{q_i}{Q_j}\ \dot{Q}_j$.&lt;/p&gt;
&lt;p&gt;Then&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
P_i &amp;amp;= \pdiff{\L(\vec{q}(\vec{Q}),\dot{\vec{q}}(\vec{Q},\dot{\vec{Q}}))}{\dot{Q}_i} \\&lt;br&gt;
&amp;amp;= \sum_{j=1}^n \pdiff{\L(\vec{q},\dot{q}_j)}{\dot{q}_j} \pdiff{\dot{q}_j}{\dot{Q}_i} \\&lt;br&gt;
&amp;amp;= \sum_{j=1}^n p_j \pdiff{q_j}{Q_i}\,,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;since $p_j = \pdiff{\L(\vec{q},\dot{q}_j)}{\dot{q}_j}$ and $\pdiff{\dot{q}_j}{\dot{Q}_i}=\pdiffop{\dot{Q}_i}\left[\dot{q}_i(\vec{Q},\dot{\vec{Q}})\right]=\pdiffop{\dot{Q}_i}\left[\sum_{k=1}^n \pdiff{q_j}{Q_k}\dot{Q}_k\right]=\pdiff{q_j}{Q_i}$. Note that $\pdiffop{\dot{Q}_i}\dot{Q}_k = 1$ when $i=k$ and $0$ otherwise, since we take $\dot{Q}_1,\dots,\dot{Q}_n$ to be free variables in the Lagrangian formulation.&lt;/p&gt;
&lt;p&gt;In general, the point transformation $\vec{Q}(\vec{q})$ need not be (uniform) measure preserving in configuration space. However, when we consider the combined phase space, the transformation $\vec{Q}(\vec{q})$ and $\vec{P}(\vec{q},\vec{p})=\evalat{\pdiff{\vec{q}}{\vec{Q}}}_{\vec{q}}\cdot\vec{p}$ (where $\pdiff{\vec{q}}{\vec{Q}}$ is the Jacobian matrix evaluated at $\vec{q}$, i.e. $P_i(\vec{q},\vec{p})=\vec{p}\cdot \evalat{\pdiff{\vec{q}}{Q_i}}_{\vec{q}}$ is a dot product) produces a (uniform) measure invariant transformation in phase space.&lt;/p&gt;
&lt;p&gt;We can prove this (thus proving Liouville&amp;rsquo;s theorem) by showing that the Jacobian $\pdiff{(\vec{q},\vec{p})}{(\vec{Q},\vec{P})}=1$ at all locations in phase space.&lt;/p&gt;
&lt;h2 id=&#34;example-2d-phase-space&#34;&gt;Example: 2D phase space&lt;/h2&gt;
&lt;p&gt;Working in 2D phase space is convenient because we can visualize it in full.&lt;/p&gt;
&lt;p&gt;Consider the transformation $Q(q)=q^2$.&lt;br&gt;
Inverse: $q(Q) = \sqrt{Q}$.&lt;br&gt;
Then we determine $P(q,p)$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
P(q,p) = p\pdiff{q}{Q} = \frac{p}{2\sqrt{Q}} = \frac{p}{2\sqrt{q^2}} = \frac{p}{2q}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If we want to smoothly interpolate between the old and new coordinates, we can generalize our transformation to $Q(q)=a q^2 + b q$ where $a=\gamma$ and $b=1-\gamma$, where $\gamma$ is our interpolating parameter.&lt;br&gt;
Inverse: $q(Q) = \frac{-b+\sqrt{4 a Q+b^2}}{2 a}$.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
P(q,p) = p\pdiff{q}{Q} = \frac{p}{\sqrt{4 a Q+b^2}} = \frac{p}{2 a q + b} = \frac{p}{2 \gamma q + 1-\gamma}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Here we have the Cartesian grid in $(q,p)$ space transformed into $(Q,P)$ space, varying $\gamma$ from 0 towards 1.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020220405121020.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020220405121020.png&#34; width=&#34;400&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020220405121031.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020220405121031.png&#34; width=&#34;400&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020220405121036.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020220405121036.png&#34; width=&#34;400&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020220405121102.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020220405121102.png&#34; width=&#34;400&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-cartesian-to-polar-configuration&#34;&gt;Example: Cartesian To Polar Configuration&lt;/h2&gt;
&lt;p&gt;The result that all configuration space transformations can be made canonical with the right momentum space transformation is surprising because so many configuration space transformations are not measure preserving by themselves. Just above we took the Cartesian to polar transformation as an example of a non-measure preserving transformation. I will go through the derivation of the canonical transformation which does a Cartesian to polar configuration transformation. The reason why this works is that configuration space is an infinitely thin slice of phase space, so all configuration space regions have 0 measure in phase space. Thus there is trivially no need to conserve measure in configuration space alone. The momentum transformation must be such that whatever distortions of measure occur in configuration space are &amp;ldquo;balanced out&amp;rdquo; in momentum space.&lt;/p&gt;
&lt;p&gt;Consider a system in 4D Cartesian phase space $(x,y,p_x,p_y)$, with $\vec{q}=(x,y)$ and $\vec{p}=(p_x,p_y)$. Let&amp;rsquo;s give the system the following Hamiltonian, $H=\frac{1}{2m}\tup{p_x^2+p_y^2} + U(x,y)$ where $U$ is some time-independent (constant through time) potential function and $m$ is the mass of the system.&lt;/p&gt;
&lt;p&gt;Then by Hamilton&amp;rsquo;s equations we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\dot{x}=\pdiff{H}{p_x}=\frac{p_x}{m}\,,\qquad \dot{y}=\pdiff{H}{p_y}=\frac{p_y}{m}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\dot{p_x}=-\pdiff{H}{x}=-\pdiff{U(x,y)}{x}\,,\qquad \dot{p_y}=-\pdiff{H}{y}=-\pdiff{U(x,y)}{y}&lt;br&gt;
\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;These are the standard equations that a Newtonian point particle obeys in the presence of a potential field $U$.&lt;/p&gt;
&lt;p&gt;Now transform to polar spatial coordinates: $R(x,y)=\sqrt{x^2+y^2}$ and $\T(x,y)=\tan^{-1}\tup{\frac{y}{x}}$, giving us the inverse transformation $x(R,\T)=R\cos\T$ and $y(R,\T)=R\sin\T$.&lt;/p&gt;
&lt;p&gt;We can independently derive the momenta in polar coordinates &lt;a href=&#34;https://en.wikipedia.org/wiki/Angular_momentum#Orbital_angular_momentum_in_two_dimensions&#34;target=&#34;_blank&#34;&gt;from Newtonian mechanics&lt;/a&gt;: $P_R=m\dot{R}$ and $P_\T=mR^2\dot{\T}$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what momentum transformation we derive using from the constraint that we need a canonical transformation (using the equations above).&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
P_R(x,y,p_x,p_y) &amp;amp;= p_x \pdiff{x}{R}+p_y \pdiff{y}{R} \\&lt;br&gt;
&amp;amp;= p_x \cos\T+p_y \sin\T&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
P_\T(x,y,p_x,p_y)&amp;amp;=p_x \pdiff{x}{\T}+p_y \pdiff{y}{\T} \\&lt;br&gt;
&amp;amp;= -p_x R\sin\T+p_y R\cos\T&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Solving this system of equations for $p_x,p_y$ we get the inverse transformation for momentum:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
p_x(R,\T,P_R,P_\T) &amp;amp;= P_R \cos (\T)-\frac{P_\T \sin(\T)}{R} \\&lt;br&gt;
p_y(R,\T,P_R,P_\T) &amp;amp;= P_R \sin (\T)+\frac{P_\T \cos(\T)}{R}&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Plugging in $x(R,\T),y(R,\T),p_x(R,\T,P_R,P_\T),p_y(R,\T,P_R,P_\T)$ to $H(x,y,p_x,p_y)$, we get&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
&amp;amp; H(R,\T,P_R,P_\T) \\&lt;br&gt;
&amp;amp;= \frac{1}{2m}\tup{p_x(R,\T,P_R,P_\T)^2+p_y(R,\T,P_R,P_\T)^2} \\ &amp;amp;\quad\ +\ U(x(R,\T),y(R,\T)) \\&lt;br&gt;
&amp;amp;= \frac{1}{2m}\tup{\tup{P_R \cos (\T)-\frac{P_\T \sin(\T)}{R}}^2+\tup{P_R \sin (\T)+\frac{P_\T \cos(\T)}{R}}^2} \\ &amp;amp;\quad\ +\ U(R\cos\T,R\sin\T)) \\&lt;br&gt;
&amp;amp;= \frac{1}{2m}\tup{P_R^2 + \frac{1}{R^2}P_\T^2} + U(R\cos\T,R\sin\T)&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Then Hamilton&amp;rsquo;s equations give us&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\dot{R}=\pdiff{H}{P_R}=\frac{P_R}{m}\,,\qquad \dot{\T}=\pdiff{H}{P_\T}=\frac{P_\T}{mR^2}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;(matching what we expect)&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\dot{P}_R&amp;amp;=-\pdiff{H}{R}\\&amp;amp;=-\pdiff{U(x,y)}{x}\pdiff{x}{R}-\pdiff{U(x,y)}{y}\pdiff{y}{R}\\&amp;amp;=-\pdiff{U(x,y)}{x}\cos\T-\pdiff{U(x,y)}{y}\sin\T\,,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\dot{P}_\T&amp;amp;=-\pdiff{H}{\T}\\&amp;amp;=-\pdiff{U(x,y)}{x}\pdiff{x}{\T}-\pdiff{U(x,y)}{y}\pdiff{y}{\T} \\&amp;amp;= \pdiff{U(x,y)}{x}R\sin\T-\pdiff{U(x,y)}{y}R\cos\T\,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Liouville&#39;s Theorem</title>
      <link>https://danabo.github.io/blog/posts/liouvilles-theorem/</link>
      <pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/liouvilles-theorem/</guid>
      <description>&lt;p&gt;Liouville&amp;rsquo;s Theorem states that the size of a state region of any closed system remains constant as the system evolves through time. This has consequences for connections between information and physics.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\s}{\sigma}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\th}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\d}{\delta}&lt;br&gt;
\newcommand{\dd}{\text{d}}&lt;br&gt;
\newcommand{\pd}{\partial}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\G}{\Gamma}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\H}{\mc{H}}&lt;br&gt;
\newcommand{\L}{\mc{L}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\tup}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\btup}[1]{\left[#1\right]}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[2]{#1_{\mid #2}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\restr}[1]{_{\mid{#1}}}&lt;br&gt;
\newcommand{\dt}{{\D t}}&lt;br&gt;
\newcommand{\Dt}{{\D t}}&lt;br&gt;
\newcommand{\ddT}{{\delta T}}&lt;br&gt;
\newcommand{\Mid}{\,\middle|\,}&lt;br&gt;
\newcommand{\qed}{\ \ \blacksquare}&lt;br&gt;
\newcommand{\diff}[2]{\frac{\dd #1}{\dd #2}}&lt;br&gt;
\newcommand{\diffop}[1]{\frac{\dd}{\dd #1}}&lt;br&gt;
\newcommand{\pdiff}[2]{\frac{\pd #1}{\pd #2}}&lt;br&gt;
\newcommand{\pdiffop}[1]{\frac{\pd}{\pd #1}}&lt;br&gt;
\newcommand{\evalat}[1]{\left. #1 \right|}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\newcommand{\r}{\rho}$&lt;/p&gt;
&lt;h1 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h1&gt;
&lt;h2 id=&#34;hamiltonian-mechanics&#34;&gt;Hamiltonian Mechanics&lt;/h2&gt;
&lt;p&gt;(I also briefly described Hamiltonian mechanics in &lt;a href=&#34;https://danabo.github.io/blog/posts/the-reversibility-problem/#setup&#34;&gt;The Reversibility Problem#setup&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Let the state of some system be described by a real-valued tuple $\o  = (\o_1,\o_2,\dots,\o_{2n}) = (q_1, q_n, p_1, p_n) \in \O$, where $\O$ is the system&amp;rsquo;s state space (set of all valid states) which is typically called its &lt;a href=&#34;https://en.wikipedia.org/wiki/Phase_space&#34;target=&#34;_blank&#34;&gt;phase space&lt;/a&gt;. We distinguish between the $q_i$ and $p_i$ coordinates. The subspace consisting of the tuples $(q_1,\dots,q_n)$ called &lt;a href=&#34;https://en.wikipedia.org/wiki/Configuration_space_%28physics%29&#34;target=&#34;_blank&#34;&gt;configuration space&lt;/a&gt;. For each $q_i$ there is a corresponding &lt;a href=&#34;https://en.wikipedia.org/wiki/Phase_space#Conjugate_momenta&#34;target=&#34;_blank&#34;&gt;conjugate momentum&lt;/a&gt; $p_i$. The $q_i$ are not called positions, but rather, configuration or &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalized_coordinates&#34;target=&#34;_blank&#34;&gt;generalized coordinates&lt;/a&gt;, since they can represent arbitrary internal degrees of freedom of a system, such as distance between parts, their orientations in space, etc. The relationship between the pairs $(q_i,p_i)$ is determined by the Hamiltonian of the system, which I&amp;rsquo;ll get to in a moment.&lt;/p&gt;
&lt;p&gt;In Cartesian coordinates where $q_i$ represents a position in space, we get the usual definition of momentum, $p_i = m_i\diffop{t}q_i$, but this need not be the case in general. For example, if $q_i$ is an angle of rotation then its conjugate momentum $p_i$ is an &lt;a href=&#34;https://en.wikipedia.org/wiki/Angular_momentum&#34;target=&#34;_blank&#34;&gt;angular momentum&lt;/a&gt; which has a different relationship with $q_i$.&lt;/p&gt;
&lt;p&gt;For our purposes, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Hamiltonian_mechanics#Phase_space_coordinates_%28p,q%29_and_Hamiltonian_H&#34;target=&#34;_blank&#34;&gt;Hamiltonian&lt;/a&gt; is a function $H : \O\times\R \to \R$ mapping phase space coordinates and time to a real number which we will take to be the total energy of the system in question. So $H(\vec{q},\vec{p},t)$ is the total energy of the system when it is in state $(\vec{q},\vec{p})$ at time $t$. If $H$ is constant through time, i.e. $H(\vec{q},\vec{p},t)=H(\vec{q},\vec{p},t&#39;)$ for all $(\vec{q},\vec{p})\in\O$ and $t,t&#39; \in \R$, then we call $H$ time-independent and drop the time input, i.e. we can just write $H(\vec{q},\vec{p})$. Otherwise, we call $H$ time-dependent. Time-independent systems satisfy &lt;a href=&#34;https://en.wikipedia.org/wiki/Time_translation_symmetry&#34;target=&#34;_blank&#34;&gt;time-translational invariance&lt;/a&gt;, i.e. time-shifting the system does not alter its dynamics.&lt;/p&gt;
&lt;p&gt;The Hamiltonian $H$ contains all the information we need to derive equations of motion of the system, which are explicit functions from time to phase state of the system $\s : \R \to \O$ which I call trajectories. So $\s(t)$ gives the state of an instance of the system at time $t$ following the trajectory $\s$.&lt;/p&gt;
&lt;p&gt;We can derive these trajectories directly from $H$ using &lt;a href=&#34;https://en.wikipedia.org/wiki/Hamiltonian_mechanics#From_Euler-Lagrange_equation_to_Hamilton%27s_equations&#34;target=&#34;_blank&#34;&gt;Hamilton&amp;rsquo;s equations&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\diff{q_i}{t}=\pdiff{H}{p_i},\qquad \diff{p_i}{t}=-\pdiff{H}{q_i}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;These two equations are the only way $q_i$ and $p_i$ are distinguished mathematically. The possible trajectories of the system are all the solutions to this system of partial differential equations.&lt;/p&gt;
&lt;p&gt;The above math conflates coordinates $\vec{q},\vec{p}$ as free variables with trajectories $\s : \R\to\O$ which are functions from time to phase space. That is to say, the DiffEQs above take $\vec{q}(t),\vec{p}(t)$ to be functions of time, but the same $\vec{q},\vec{p}$ are also used as free coordinate variables in other contexts. So in instances where it is helpful to be more precise, we can rewrite Hamilton&amp;rsquo;s equations in terms of trajectories.&lt;/p&gt;
&lt;p&gt;But first we need some notation. If $\s$ is a trajectory with $\s(t) = (\vec{q}\up{t},\vec{p}\up{t})=(q_1\up{t},\dots,q_n\up{t},p_1\up{t},\dots,p_n\up{t})$ being the state of the system along that trajectory at time $t$, then let $\s_\vec{q}(t) = \vec{q}\up{t}$ and $\s_\vec{p}(t) = \vec{p}\up{t}$ be the sub-dimensions of the output of $\s(t)$ corresponding to the configuration and conjugate momenta coordinates respectively. Furthermore, let $\s_{q_i}(t)=q_i\up{t}$ and $\s_{p_i}(t)=p_i\up{t}$ select specific configuration and momentum coordinates on the output of $\s(t)$.&lt;/p&gt;
&lt;p&gt;Let $\Sigma$ be the set of all trajectories $\s$ s.t.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\evalat{\diff{\s_{q_i}}{t}}_{t=\hat{t}} = \evalat{\pdiff{H}{p_i}}_{(\vec{q},\vec{p})=\s(t),\ t=\hat{t}} \quad\textrm{and}\quad  \evalat{\diff{\s_{p_i}}{t}}_{t=\hat{t}} = \evalat{-\pdiff{H}{q_i}}_{(\vec{q},\vec{p})=\s(t),\ t=\hat{t}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $i=1,\dots,n$ and for all $\hat{t}\in\R$. Then $\Sigma$ is the set of all solutions to Hamilton&amp;rsquo;s equations, which I call the set of valid trajectories w.r.t. $H$.&lt;/p&gt;
&lt;p&gt;It is useful to work explicitly in terms of functions that map states between times, called propagators. Let $\t_{t\to t&#39;} : \O\to\O$ be a propagator mapping states at time $t$ to states at time $t&#39;$ (if $t&#39;&amp;lt;t$ then we are propagating state backwards in time). We can define the behavior of the propagator in terms of the valid trajectories of $H$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\t_{t\to t&#39;}(\s(t)) = \s(t&#39;)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $\s \in \Sigma$ and all $t,t&#39; \in \R$.&lt;/p&gt;
&lt;p&gt;Note that propagators are bijections where $\t^{-1}_{t\to \t&#39;} = \t_{t&#39; \to t}$.&lt;/p&gt;
&lt;p&gt;If $H$ is time-independent, then we can just specify propagators in terms of their delta time, where $\t_\Dt(\s(t))=\s(t+\Dt)$ for all $t,\Dt \in\R$.&lt;/p&gt;
&lt;h2 id=&#34;measure&#34;&gt;Measure&lt;/h2&gt;
&lt;p&gt;Liouville&amp;rsquo;s theorem is a statement about measures on phase space which obey Hamilton&amp;rsquo;s equations. Let&amp;rsquo;s make all that precise.&lt;/p&gt;
&lt;p&gt;Briefly, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Measure_%28mathematics%29&#34;target=&#34;_blank&#34;&gt;measure&lt;/a&gt; $\mu$ is a function from subsets of $\O$ to non-negative real numbers which is only defined on certain so-called measurable subset (as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Partial_function&#34;target=&#34;_blank&#34;&gt;partial function&lt;/a&gt; $\mu$ has the type signature $\mu:2^\O \to \R_{\geq 0}$). We assume that $\O$ is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Measurable_space&#34;target=&#34;_blank&#34;&gt;measurable space&lt;/a&gt;. For a primer on measure theory, see my &lt;a href=&#34;http://zhat.io/articles/primer-probability-theory#primer-to-measure-theory&#34;target=&#34;_blank&#34;&gt;post on probability theory&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Throughout this post we suppose we are given a measure $\mu$ on phase space $\O$. Everything that follows does not assume any particular interpretation to the meaning of $\mu$. Normalized measures can typically represent an i.i.d. stochastic drawing of the state of the system (i.e. frequentist interpretation), or the modeler&amp;rsquo;s (you and I) state of belief about the system&amp;rsquo;s state (called the Bayesian interpretation). I tend to think of these measures as simply providing a way to quantify the sizes (e.g. areas, volumes) of phase space regions (subsets of phase space). See &lt;a href=&#34;https://danabo.github.io/blog/posts/bayesian-information-theory/&#34;&gt;Bayesian information theory&lt;/a&gt; and &lt;a href=&#34;https://danabo.github.io/blog/posts/physical-information/&#34;&gt;Physical Information&lt;/a&gt; for details on this interpretation. That means that I don&amp;rsquo;t assume $\mu$ is normalized (which is required for $\mu$ to be a probability measure), i.e. $\mu(\O)$ need not be $1$ and need not even be defined ($\mu$ need not be not normalizable).&lt;/p&gt;
&lt;p&gt;To talk about time-evolving a measure, we need to assign a measure $\mu_t$ to every moment of time $t\in\R$. Let $R \subseteq \O$ be any measurable subset of phase space $\O$, which I will henceforth simply refer to as a region, or phase region.&lt;/p&gt;
&lt;p&gt;We require that&lt;br&gt;
$$&lt;br&gt;
\mu_{t}(R) = \mu_{t&#39;}(\t_{t \to t&#39;}(R))\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all regions $R$ and all $t,t&#39; \in \R$.&lt;/p&gt;
&lt;p&gt;Note that everything described here is deterministic time-evolution. That is to say, if the measure $\mu_t$ at time $t$ represents uncertainty (Bayesian) or randomness (frequentist) due to the choice of the state of the system at time $t$, then every state before and after is uniquely determined by that choice. This is in contrast to stochastic time-evolution, which destroys information that states of the system share about each other across time, and involves a different sort of time-evolution of measure.&lt;/p&gt;
&lt;p&gt;Note that putting a measure on state space at some time $t$, and then deterministically time-evolving that measure according to the propagators $\t_{t\to t&#39;}$, is equivalent to putting a measure on the valid trajectories in $\Sigma$. So an equivalent formulation would be to define $\Sigma$ as a measurable space and put a measure $M$ on $\Sigma$. This is equivalent to time-evolving measures on $\O$ as described. See &lt;a href=&#34;https://danabo.github.io/blog/posts/causality-for-physics/#incorporating-probability&#34;&gt;Causality For Physics#incorporating-probability&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;Given that $\mu_t$ is time-evolved in this way, Liouville&amp;rsquo;s Theorem makes a statement about how density behaves over time, given that the system is closed and conserves energy (according to its given Hamiltonian). It is easier to state the theorem in terms of density functions rather than measures. Though density functions may not exist for any measure on any measurable space, we can assume they do for phase spaces that represent Hamiltonian systems. In that case we can define the conversion between measure and density.&lt;/p&gt;
&lt;p&gt;Supposing $\O$ is an $N$-dimensional parametrized measurable space (for our purposes $N=2n$) where differentiation and integration is well defined (basically always the case for physical systems) with a measure $\mu_t$. Then the corresponding density function $\r:\O\times\R \to \R$ satisfies&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu_t(R) = \int_R \r(\vec{\o},t)\ \dd{\o_1}\dots\dd{\o_{N}}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all regions $R$ and times $t\in\R$. We can &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_density_function#Densities_associated_with_multiple_variables&#34;target=&#34;_blank&#34;&gt;explicitly derive the density&lt;/a&gt; in terms of measure $\mu_t$ as&lt;/p&gt;
&lt;p&gt;$$\r(\o, t) = \frac{\dd^{N}}{\dd \o_1\dots \dd \o_{N}}\mu_t\big(\G(\o_1,\dots,\o_{N})\big)\,,$$&lt;/p&gt;
&lt;p&gt;where $\G(\o_1,\dots,\o_{N}) = \prod_{i=1}^{N}(-\infty,\o_i]$ is a hyper-quadrant with origin at $(\o_1,\dots,\o_{N})$.&lt;/p&gt;
&lt;h1 id=&#34;liouvilles-theorem&#34;&gt;Liouville&amp;rsquo;s Theorem&lt;/h1&gt;
&lt;p&gt;As outlined above, suppose we have a system described by &lt;a href=&#34;https://en.wikipedia.org/wiki/Canonical_coordinates&#34;target=&#34;_blank&#34;&gt;canonical coordinates&lt;/a&gt; $\vec{\o}=(\vec{q},\vec{p})=(q_1,\dots,q_n,p_1,\dots,p_n)$ and a &lt;a href=&#34;https://en.wikipedia.org/wiki/Hamiltonian_mechanics&#34;target=&#34;_blank&#34;&gt;Hamiltonian&lt;/a&gt; $H$. Also suppose we are given some density function $\r(\vec{q},\vec{p},t)$, a function of coordinates and time, which obeys the time-evolution induced by $H$ (as explained above in &lt;a href=&#34;#measure&#34;&gt;#Measure&lt;/a&gt;). That is to say, the density must satisfy $\int_R \r(\vec{\o}, t)\ \dd^{2n}{\vec{\o}}=\int_{R&#39;} \r(\vec{\o}, t&#39;)\ \dd^{2n}{\vec{\o}}$ for all regions $R$ and times $t,t&#39;$, where $R&#39;={\t_{t\to t&#39;}(R)}$ and $\t_{t\to t&#39;}$ is the propagator induced by $H$ mapping time $t$ to time $t&#39;$.&lt;/p&gt;
&lt;p&gt;If the system is isolated and has constant total energy (obeys conservation of energy), then &lt;a href=&#34;https://en.wikipedia.org/wiki/Liouville%27s_theorem_%28Hamiltonian%29#Liouville_equations&#34;target=&#34;_blank&#34;&gt;Liouville&amp;rsquo;s Theorem states&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\diff{\r}{t} = 0\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We have to be careful here.  It might look like we are stating that $\r$ is constant through time, but that&amp;rsquo;s actually not what that means. We must make the important distinction between the partial derivative $\pdiff{\r}{t}$ which gives the change in $\r$ through time at some fixed point in phase space, and the single-variable derivative $\diff{\r}{t}$, which treats $\r$ as a single-variable function - specifically, $\r$ along some parametric curve $(\vec{q}(t),\vec{p}(t),t)$, i.e. $\r(t) = \r(\vec{q}(t),\vec{p}(t),t)$. This allows us to expand out the single-variable derivative using the multi-variate chain rule, thus rewriting Liouville&amp;rsquo;s theorem like this:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\diffop{t}\r(\vec{q}(t),\vec{p}(t),t) = \sum_{i=1}^n \pdiff{\r}{q_i}\diff{q_i}{t} + \sum_{i=1}^n \pdiff{\r}{p_i}\diff{p_i}{t} + \pdiff{\r}{t} = 0\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where all derivatives are evaluated at the point $(\vec{q}(t),\vec{p}(t),t)$ for some chosen time $t$.&lt;/p&gt;
&lt;p&gt;We further assume this parametric curve $(\vec{q}(t),\vec{p}(t),t)$ satisfies Hamilton&amp;rsquo;s equations, i.e. $(\vec{q}(t),\vec{p}(t))=\sigma(t)$ for all $t$, for some valid trajectory $\s\in\Sigma$. Then we can plugin and rewrite Liouville&amp;rsquo;s theorem as&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\diff{\r}{t} = \pdiff{\r}{t} + \sum_{i=1}^n \tup{\pdiff{\r}{q_i}\pdiff{H}{p_i} - \pdiff{\r}{p_i}\pdiff{H}{q_i}} = 0\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;making implicit dependence on the valid trajectories of the system and explicit dependence on the given Hamiltonian $H$.&lt;/p&gt;
&lt;p&gt;We could also rewrite Liouville&amp;rsquo;s theorem using our trajectory notation from above:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\evalat{\diff{\r}{t}}_{(\vec{q},\vec{p})=\s(\hat{t}),t=\hat{t}} = \evalat{\btup{\sum_{i=1}^n \pdiff{\r}{q_i}\diff{\s_{q_i}}{t} + \sum_{i=1}^n \pdiff{\r}{p_i}\diff{\s_{p_i}}{t} + \pdiff{\r}{t}} }_{(\vec{q},\vec{p})=\s(\hat{t}),t=\hat{t}} = 0&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;holds for all valid trajectories $\s\in\Sigma$ and all times $\hat{t}$.&lt;/p&gt;
&lt;h3 id=&#34;intuition-pump&#34;&gt;Intuition Pump&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s look at a fairly typical class of Hamiltonians: $H = \sum_{i=1}^n \frac{1}{2m_i}p_i^2 + U(q)$. This is just the standard form of a time-independent Hamiltonian of a system in the presence of some constant potential field $U$, where $m_i$ is the mass of the $i$-th coordinate. Then Hamilton&amp;rsquo;s equations tell us&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\diff{q_i}{t} = \pdiff{H}{p_i} = \frac{p_i}{m_i}\,,\qquad \diff{p_i}{t} = -\pdiff{H}{q_i} = -\pdiff{U}{q_i}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where the first equation relates coordinates $q_i$ to conjugate momenta $p_i$ in the obvious way (i.e. $p_i =  \diffop{t}(m_iq_i)$ is taken as the definition of momentum in Newtonian physics), and the second equation states that the time derivative of momentum (in the $i$-th direction) is the negative coordinate derivative of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Force#Potential_energy&#34;target=&#34;_blank&#34;&gt;potential field&lt;/a&gt; $U$ which gives the force along the $i$-th direction, i.e. $\diff{p_i}{t} = -\pdiff{U}{q_i}$ is just a restatement of Newton&amp;rsquo;s second law, i.e. change in momentum is due to a force generated by a &lt;a href=&#34;http://hyperphysics.phy-astr.gsu.edu/hbase/pegrav.html&#34;target=&#34;_blank&#34;&gt;change in potential energy&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Plugging in, Liouville&amp;rsquo;s theorem then becomes&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\diff{\r}{t} = \pdiff{\r}{t} + \sum_{i=1}^n \tup{\pdiff{\r}{q_i}\frac{p_i}{m_i} - \pdiff{\r}{p_i}\pdiff{U}{q_i}} = 0\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;h2 id=&#34;uniform-measure&#34;&gt;Uniform Measure&lt;/h2&gt;
&lt;p&gt;It may at a glance look like Liouville is just restating our starting definition for how density gets time-evolved. That is to say, we took as given that $\int_R \r(\vec{\o}, t)\ \dd^{2n}{\vec{\o}}=\int_{R&#39;} \r(\vec{\o}, t&#39;)\ \dd^{2n}{\vec{\o}}$, or equivalently $\mu_t(R) = \mu_{t&#39;}(R&#39;)$, which is stating that the measure of region $R$ is preserved when it is transformed to $R&#39;= \t_{t\to t&#39;}(R)$, because we transform the measure/density along with it. However, this statement tells us nothing about how $\r$ will change as individual states are time-evolved, i.e. how $\r(\s(t),t)$ will differ from $\r(\s(t&#39;),t&#39;)$ along a given trajectory $\s$. The Liouville statement $\diff{\r}{t}=0$ is telling us that density is constant along valid trajectories, i.e. $\r(\s(t),t)=\r(\s(t&#39;),t&#39;)$ for all $t,t&#39; \in \R$, for all valid $\s\in\Sigma$ (hence we could instead just assign time-independent density to the trajectories themselves, i.e. make $\r$ a function on $\Sigma$).&lt;/p&gt;
&lt;p&gt;A common colloquial understanding is that Liouville&amp;rsquo;s theorem states that the size of phase regions remains visually constant through time, where a Euclidean, i.e. uniform, measure on phase space is implied. This is indeed a very useful consequence of Liouville.&lt;/p&gt;
&lt;p&gt;Formally, a uniform measure is derived from a constant density function, i.e. $\r(\vec{\o},t) = \r(\vec{\o}&#39;,t)$ for any two $\vec{\o},\vec{\o}&#39; \in \O$. Suppose $\r$ is uniform with $\r(\vec{\o},t_0)=\r_0$ at time $t_0$. Since by Liouville $\r(\s(t),t)=\r(\s(t&#39;),t&#39;)$ for all times $t,t&#39;\in\R$, then $\r(\s(t),t)=\r(\s(t_0),t_0)=\r_0$ holds for all valid trajectories $\s\in\Sigma$ for all times $t$. Thus $\r(\vec{\o},t)=\r_0$ is uniform at all times $t$, since there will be some trajectory passing through any $\vec{\o}\in\O$. We conclude that $\r = \r_0$ is constant in phase space and time when time-evolved.&lt;/p&gt;
&lt;p&gt;Preservation of phase volume under a measure/density which is constant through time is an interesting result of Liouville.&lt;/p&gt;
&lt;h1 id=&#34;coordinate-agnostic-ignorance&#34;&gt;Coordinate Agnostic Ignorance&lt;/h1&gt;
&lt;p&gt;Liouville&amp;rsquo;s theorem is actually a special case of a more general phenomenon (the generalized Liouville theorem), and one of the usual proofs given for Liouville is actually a proof for the more general case. Rather than just time-evolution preserving density, the stronger result states that a certain class of coordinate transformations preserves density. Viewing time-evolution as such a coordinate transformation from time $t$ to new coordinates describing time $t&#39;$, we see that Liouville becomes a special case.&lt;/p&gt;
&lt;p&gt;This more general form of Liouville is relevant to my interest in the relationship between information and thermodynamic entropy. Specifically, it allows us to bypass the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bertrand_paradox_%28probability%29&#34;target=&#34;_blank&#34;&gt;Bertrand paradox&lt;/a&gt; and have a unique maximally ignorance way to quantify information. The Boltzmann formulation of entropy utilizes this same quantification (?)&lt;/p&gt;
&lt;h2 id=&#34;canonical-coordinates&#34;&gt;Canonical Coordinates&lt;/h2&gt;
&lt;p&gt;Some references that I found useful for this topic:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Canonical_transformation&#34;target=&#34;_blank&#34;&gt;Wikipedia: Canonical transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.scielo.org.mx/pdf/rmfe/v57n2/v57n2a4.pdf&#34;target=&#34;_blank&#34;&gt;The generating function of a canonical transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://phys.libretexts.org/Bookshelves/Classical_Mechanics/Variational_Principles_in_Classical_Mechanics_%28Cline%29/15%3A_Advanced_Hamiltonian_Mechanics/15.03%3A_Canonical_Transformations_in_Hamiltonian_Mechanics&#34;target=&#34;_blank&#34;&gt;Canonical Transformations in Hamiltonian Mechanics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pearson.com/us/higher-education/program/Goldstein-Classical-Mechanics-3rd-Edition/PGM170105.html&#34;target=&#34;_blank&#34;&gt;Goldstein, Poole &amp;amp; Safko, Classical Mechanics 3rd ed.&lt;/a&gt;, section 9.4&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elsevier.com/books/mechanics/landau/978-0-08-050347-9&#34;target=&#34;_blank&#34;&gt;Landau &amp;amp; Lifshitz, Mechanics - Volume 1, 3rd ed.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;A good summary &lt;a href=&#34;https://galileoandeinstein.phys.virginia.edu/7010/CM_10_Canonical_Transformations.html&#34;target=&#34;_blank&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://galileoandeinstein.phys.virginia.edu/7010/CM_11_Introduction_to_Liouville.html&#34;target=&#34;_blank&#34;&gt;here&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nononsensebooks.com/cm/&#34;target=&#34;_blank&#34;&gt;No-Nonsense Classical Mechanics&lt;/a&gt;, section 7.3.2.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In particular this discussion takes inspiration from the derivations in section 7.3.1 of &lt;a href=&#34;https://nononsensebooks.com/cm/&#34;target=&#34;_blank&#34;&gt;No-Nonsense Classical Mechanics&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Consider a time-independent coordinate transformation specified by a set of bijective functions $Q_i(\vec{q},\vec{p})$ and $P_i(\vec{q},\vec{p})$ for $i=1,\dots,n$, where $\vec{q} = (q_1,\dots,q_n)$ and $\vec{p}=(p_1,\dots,p_n)$, written more compactly as vector-valued function $\vec{Q}(\vec{q},\vec{p})$ and $\vec{P}(\vec{q},\vec{p})$. Notice that we are allowing intermingling of position and momentum coordinates. For example, we could swap position and momentum where $Q_i = p_i$ and $P_i=-q_i$ for all $i$.&lt;/p&gt;
&lt;p&gt;Our Hamiltonian $H(\vec{q},\vec{p}, t)$ is a function of the original coordinates and time (a time-dependent Hamiltonian). We obtain the Hamiltonian in the new coordinate system by plugging in the inverse mappings: $\tilde{H}(\vec{Q},\vec{P},t) = H(\vec{q}(\vec{Q},\vec{P}),\vec{p}(\vec{Q},\vec{P}),t)$, where $\tilde{H}$ is a function of the new coordinates $(\vec{Q},\vec{P})$ and time.&lt;/p&gt;
&lt;p&gt;Note that I am only considering time-independent coordinate transformations. Canonical transformations are allowed to be time-dependent, where $\vec{Q}(\vec{q},\vec{p},t)$ and $\vec{P}(\vec{q},\vec{p},t)$ are functions of time as well. However this is more complex to deal with, especially since the Hamiltonian $\tilde{H}$ derived by naively plugging in the coordinate transformation may not be the valid Hamiltonian for the system in the transformed coordinates. &lt;a href=&#34;https://en.wikipedia.org/wiki/Generating_function_%28physics%29&#34;target=&#34;_blank&#34;&gt;Generating functions&lt;/a&gt; are needed to deal with time-dependent transformations which I won&amp;rsquo;t get into here. Many of the sources I listed above explain them in detail. So for this post assume time-independent coordinate transformations.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;canonical&lt;/strong&gt; coordinate transformation is defined as a transformation that preserves Hamilton&amp;rsquo;s equations, i.e.&lt;/p&gt;
&lt;p&gt;$$\diff{Q_i}{t}=\pdiff{\tilde{H}}{P_i}\,,\qquad \diff{P_i}{t}=-\pdiff{\tilde{H}}{Q_i}$$&lt;/p&gt;
&lt;p&gt;holds in the new coordinate system.  To be more precise, given the set of valid trajectories $\Sigma$ in coordinates $\vec{q},\vec{p}$, we want the transformed trajectories to also obey Hamilton&amp;rsquo;s equations. For $\s\in\Sigma$, let $\tilde{\s}(t) = \vtup{\vec{Q},\vec{P}}(\s(t))$, which is a shorthand for the pair of relationships: $\tilde{\s}_{\vec{Q}}(t)=\vec{Q}(\s(t))$ and $\tilde{\s}_{\vec{P}}(t)=\vec{P}(\s(t))$. Let $\tilde{\Sigma} = \vtup{\vec{Q},\vec{P}}(\Sigma)$ be the set of all transformed trajectories in $\Sigma$. Then the coordinate transform is canonical iff&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\evalat{\diff{\tilde{\s}_{Q_i}}{t}}_{t=\hat{t}} = \evalat{\pdiff{\tilde{H}}{P_i}}_{(\vec{Q},\vec{P})=\tilde{\s}(t),\ t=\hat{t}} \quad\textrm{and}\quad  \evalat{\diff{\tilde{\s}_{P_i}}{t}}_{t=\hat{t}} = \evalat{-\pdiff{\tilde{H}}{Q_i}}_{(\vec{Q},\vec{P})=\tilde{\s}(t),\ t=\hat{t}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $\tilde{\s} \in \tilde{\Sigma}$, all $i=1,\dots,n$ and all $\hat{t}\in\R$.&lt;/p&gt;
&lt;p&gt;A coordinate system is canonical if Hamilton&amp;rsquo;s equations hold in that coordinate system, and a canonical transformation applied to canonical coordinates produces canonical coordinates.&lt;/p&gt;
&lt;p&gt;For some intuition pumps, see &lt;a href=&#34;https://danabo.github.io/blog/posts/liouville-supplemental-coordinate-transformations/#example-2d-phase-space&#34;&gt;Liouville Supplemental - Coordinate Transformations#example-2d-phase-space&lt;/a&gt; and &lt;a href=&#34;https://danabo.github.io/blog/posts/liouville-supplemental-coordinate-transformations/#example-non-canonical&#34;&gt;Liouville Supplemental - Coordinate Transformations#example-non-canonical&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;measure-preservation&#34;&gt;Measure Preservation&lt;/h2&gt;
&lt;p&gt;Here we prove that coordinate transformations which preserve Hamilton&amp;rsquo;s equations (i.e. canonical transformations) have a constant Jacobian equal to 1, implying that density is preserved under these transformations.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pearson.com/us/higher-education/program/Goldstein-Classical-Mechanics-3rd-Edition/PGM170105.html&#34;target=&#34;_blank&#34;&gt;Goldstein, Poole &amp;amp; Safko, Classical Mechanics 3rd ed.&lt;/a&gt; section 9.4&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Canonical_transformation#Liouville&#39;s_theorem&#34;&gt;https://en.wikipedia.org/wiki/Canonical_transformation#Liouville&#39;s_theorem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Remember that $\vec{Q}$ is a function of $(\vec{q},\vec{p})$. Using chain rule, we have&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\diffop{t}Q_i&lt;br&gt;
&amp;amp;= \sum_{j=1}^n \btup{\pdiff{Q_i}{q_j}\diff{q_j}{t}+\pdiff{Q_i}{p_j}\diff{p_j}{t}} \\&lt;br&gt;
&amp;amp;= \sum_{j=1}^n \btup{\pdiff{Q_i}{q_j}\pdiff{H}{p_j}-\pdiff{Q_i}{p_j}\pdiff{H}{q_j}}\,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Using $\tilde{H}(\vec{Q},\vec{P})=H(\vec{q}(\vec{Q},\vec{P}),\vec{p}(\vec{Q},\vec{P}))$, we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\pdiff{\tilde{H}}{P_i} = \sum_{j=1}^n\btup{\pdiff{H}{q_j}\pdiff{q_j}{P_i} + \pdiff{H}{p_j}\pdiff{p_j}{P_i}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We want to satisfy&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\diff{Q_i}{t} &amp;amp;= \pdiff{\tilde{H}}{P_i}&lt;br&gt;
\\&lt;br&gt;
\implies \sum_{j=1}^n \btup{\pdiff{H}{q_j}\tup{-\pdiff{Q_i}{p_j}}+\pdiff{H}{p_j}\pdiff{Q_i}{q_j}} &amp;amp;= \sum_{j=1}^n\btup{\pdiff{H}{q_j}\pdiff{q_j}{P_i} + \pdiff{H}{p_j}\pdiff{p_j}{P_i}}\,,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;giving us a the constraints&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\pdiff{Q_i}{q_j} = \pdiff{p_j}{P_i}\,,\qquad \pdiff{Q_i}{p_j} = -\pdiff{q_j}{P_i}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $i=1,\dots,n$.&lt;/p&gt;
&lt;p&gt;The determinant &lt;a href=&#34;https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant&#34;target=&#34;_blank&#34;&gt;Jacobian&lt;/a&gt; of our transformation $\tup{\vec{Q}(\vec{q},\vec{p}),\vec{P}(\vec{q},\vec{p})}$ determines the relationship between our density $\r(\vec{q},\vec{p},t)$ and the transformed density $\tilde{\r}(\vec{Q},\vec{P},t)=\pdiff{(\vec{Q},\vec{P})}{(\vec{q},\vec{p})}\r\tup{\vec{q}(\vec{Q},\vec{P}),\vec{p}(\vec{Q},\vec{P}),t)}$, where&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\pdiff{(\vec{Q},\vec{P})}{(\vec{q},\vec{p})} \df \det\begin{bmatrix}\pdiff{Q_1}{q_1} &amp;amp; \dots &amp;amp; \pdiff{Q_1}{p_n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \pdiff{P_n}{q_1} &amp;amp; \dots &amp;amp; \pdiff{P_n}{p_n}\end{bmatrix}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Chain_rule#General_rule&#34;target=&#34;_blank&#34;&gt;multivariate chain rule&lt;/a&gt; tells us&lt;br&gt;
$$\begin{aligned}&lt;br&gt;
\pdiff{(\vec{Q},\vec{P})}{(\vec{q},\vec{p})} &amp;amp;= \pdiff{(\vec{Q},\vec{P})}{(\vec{q},\vec{P})} \pdiff{(\vec{q},\vec{P})}{(\vec{q},\vec{p})} \\&lt;br&gt;
&amp;amp;= \left. \pdiff{(\vec{Q},\vec{P})}{(\vec{q},\vec{P})}  \middle/ \pdiff{(\vec{q},\vec{p})}{(\vec{q},\vec{P})} \right. \\&lt;br&gt;
&amp;amp;= \left. \pdiff{(\vec{Q})}{(\vec{q})}  \middle/ \pdiff{(\vec{p})}{(\vec{P})} \right.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;The middle step uses the fact that the Jacobian of the inverse of a transformation is the inverse of the Jacobian matrix, whose determinant is the reciprocal of the original determinant. The last step makes use of the cancellation of same variables.&lt;/p&gt;
&lt;p&gt;Plugging in our constraints, we have&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\pdiff{(\vec{Q})}{(\vec{q})}&lt;br&gt;
&amp;amp;= \det\begin{bmatrix}\pdiff{Q_1}{q_1} &amp;amp; \dots &amp;amp; \pdiff{Q_1}{q_n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \pdiff{Q_n}{q_1} &amp;amp; \dots &amp;amp; \pdiff{Q_n}{q_n}\end{bmatrix} \\&lt;br&gt;
&amp;amp;= \det\begin{bmatrix}\pdiff{p_1}{P_1} &amp;amp; \dots &amp;amp; \pdiff{p_1}{P_n} \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ \pdiff{p_n}{P_1} &amp;amp; \dots &amp;amp; \pdiff{p_n}{P_n}\end{bmatrix} \\&lt;br&gt;
&amp;amp;= \pdiff{(\vec{p})}{(\vec{P})}&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;and so $\left. \pdiff{(\vec{Q})}{(\vec{q})}  \middle/ \pdiff{(\vec{p})}{(\vec{P})} \right. = 1$. Thus $\pdiff{(\vec{Q},\vec{P})}{(\vec{q},\vec{p})}= 1$.&lt;/p&gt;
&lt;p&gt;Let $T$ be a canonical coordinate transformation on $\O$. Then $\pdiff{(T(\vec{\o}))}{(\vec{\o})}=1$. By the change of variable rule,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\int_R \r(\vec{\o},t)\ \dd^{2n} \vec{\o} &amp;amp;= \int_{T(R)} \pdiff{(T(\vec{\o}))}{(\vec{\o})}\r\tup{T^{-1}\tup{\vec{z}},t}\ \dd^{2n} \vec{z} \\&amp;amp;= \int_{T(R)} \r\tup{T^{-1}\tup{\vec{z}},t}\ \dd^{2n} \vec{z}\,,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;and so naively applying the coordinate change to $\r$ s.t. $\tilde{\r}(\vec{z},t)=\r\tup{T^{-1}\tup{\vec{z}},t}$ gives us the correct density in the transformed coordinates. If $\r=\r_0$, then $\tilde{\r}=\r_0$, and so the uniform density is preserved under the transformation.&lt;/p&gt;
&lt;h3 id=&#34;liouville-proof-sketch&#34;&gt;Liouville proof sketch&lt;/h3&gt;
&lt;p&gt;Liouville&amp;rsquo;s theorem is a special case of the above result. Time-evolution ends up being a canonical transformation, so long as energy is conserved in the system (i.e. the Hamiltonian is constant along every trajectory). Thus density is preserved through time-evolution, i.e. density is constant along a given trajectory. That is Liouville&amp;rsquo;s theorem.&lt;/p&gt;
&lt;p&gt;There is a simple argument for why time-evolution is a canonical transformation. Since, for a valid physical model, we take as given that Hamilton&amp;rsquo;s equations are obeyed at all times and phase coordinates, then transforming the system through time preserves Hamilton&amp;rsquo;s equations, making that transformation canonical (by definition). This argument may seem a bit handwavy. There are formal proofs for this which go a bit out of scope of this post. However, I can outline a straightforward proof sketch.&lt;/p&gt;
&lt;p&gt;Note that, unlike the canonical transformations we considered above, time-evolution is a not universally canonical transformation (canonical w.r.t. all Hamiltonians). One Hamiltonian&amp;rsquo;s time-evolution generally does not produce transformations that are canonical w.r.t. all Hamiltonians. So these time-translation transformations are examples of Hamiltonian-specific canonical transformations. See &lt;a href=&#34;https://danabo.github.io/blog/posts/liouville-supplemental-coordinate-transformations/#example-hamiltonian-specific&#34;&gt;Liouville Supplemental - Coordinate Transformations#example-hamiltonian-specific&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Suppose we are given a time-independent Hamiltonian $H(\vec{q},\vec{p})$ which induces propagators $\t_{\Dt}$ for $\Dt\in\R$. As I mentioned earlier, dealing with time-dependent transformations is out of scope of this post, and time-evolution will only be time-independent if the Hamiltonian is.&lt;/p&gt;
&lt;p&gt;Fix a particular $\Dt\in\R$ and define the following coordinate transformation:&lt;/p&gt;
&lt;p&gt;$$\tup{\vec{Q}(\vec{q},\vec{p}),\vec{P}(\vec{q},\vec{p})}=\t_\Dt\tup{\vec{q},\vec{p}}\,,$$&lt;/p&gt;
&lt;p&gt;giving us the inverse transformation,&lt;br&gt;
$$\tup{\vec{q}(\vec{Q},\vec{P}),\vec{p}(\vec{Q},\vec{P})}=\t_{-\Dt}\tup{\vec{Q},\vec{P}}\,.$$&lt;/p&gt;
&lt;p&gt;Then the transformed Hamiltonian is $\tilde{H}(\vec{Q},\vec{P})=H(\vec{q}(\vec{Q},\vec{P}),\vec{p}(\vec{Q},\vec{P}))$.&lt;/p&gt;
&lt;p&gt;Hamilton&amp;rsquo;s equations in terms of trajectories $\s:\R\to\O$ are&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\evalat{\diff{\s_{q_i}}{t}}_{t=\hat{t}} = \evalat{\pdiff{H}{p_i}}_{(\vec{q},\vec{p})=\s(t),\ t=\hat{t}} \quad\textrm{and}\quad  \evalat{\diff{\s_{p_i}}{t}}_{t=\hat{t}} = \evalat{-\pdiff{H}{q_i}}_{(\vec{q},\vec{p})=\s(t),\ t=\hat{t}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The set of trajectories $\Sigma$ satisfying this system of partial differential equations constitute the valid trajectories of the system.&lt;/p&gt;
&lt;p&gt;We can write down what the transformed trajectories look like:&lt;/p&gt;
&lt;p&gt;$$\tilde{\s}(\hat{t}) = \vtup{\vec{Q},\vec{P}}(\s(\hat{t})) = \s(\hat{t}+\Dt)$$&lt;br&gt;
for all $\s \in \Sigma$ and all $\hat{t}\in\R$.&lt;/p&gt;
&lt;p&gt;Then we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\evalat{\diff{\tilde{\s}_{Q_i}}{t}}_{t=\hat{t}} = \evalat{\diff{\s_{q_i}}{t}}_{t=\hat{t}+\Dt} = \evalat{\pdiff{H}{p_i}}_{(\vec{q},\vec{p})=\s(\hat{t}+\Dt)}$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\evalat{\diff{\tilde{\s}_{P_i}}{t}}_{t=\hat{t}} = \evalat{\diff{\s_{p_i}}{t}}_{t=\hat{t}+\Dt} = -\evalat{\pdiff{H}{q_i}}_{(\vec{q},\vec{p})=\s(\hat{t}+\Dt)}$$&lt;/p&gt;
&lt;p&gt;The next step in this proof requires that the Hamiltonian be invariant to time-translation, i.e. $\tilde{H}(\vec{q},\vec{p})=H(\vec{q},\vec{p})$. This ends up being the case for all Hamiltonians that obey conservation of energy, i.e. the Hamiltonian is constant along its valid trajectories, i.e. $H(\s(t))=H(\s(t&#39;))$ for all $\s\in\Sigma$ and $t,t&#39;\in\R$. I won&amp;rsquo;t prove this here, but time-translation symmetry is a well known result in physics.&lt;/p&gt;
&lt;p&gt;Then we complete our proof with&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\evalat{\pdiff{H}{q_i}}_{(\vec{q},\vec{p})=\s(\hat{t}+\Dt)} = \evalat{\pdiff{\tilde{H}}{q_i}}_{(\vec{q},\vec{p})=\s(\hat{t}+\Dt)} = \evalat{\pdiff{\tilde{H}}{Q_i}}_{(\vec{Q},\vec{P})=\tilde{\s}(\hat{t})}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\evalat{\pdiff{H}{p_i}}_{(\vec{q},\vec{p})=\s(\hat{t}+\Dt)} = \evalat{\pdiff{\tilde{H}}{p_i}}_{(\vec{q},\vec{p})=\s(\hat{t}+\Dt)} = \evalat{\pdiff{\tilde{H}}{P_i}}_{(\vec{Q},\vec{P})=\tilde{\s}(\hat{t})}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;And so we have Hamilton&amp;rsquo;s equations in the transformed coordinates:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\evalat{\diff{\tilde{\s}_{Q_i}}{t}}_{t=\hat{t}} = \evalat{\pdiff{\tilde{H}}{P_i}}_{(\vec{Q},\vec{P})=\tilde{\s}(\hat{t})}\,,\qquad \evalat{\diff{\tilde{\s}_{P_i}}{t}}_{t=\hat{t}} = -\evalat{\pdiff{\tilde{H}}{Q_i}}_{(\vec{Q},\vec{P})=\tilde{\s}(\hat{t})}&lt;br&gt;
$$&lt;/p&gt;
&lt;h2 id=&#34;configuration-space-coordinate-transformations&#34;&gt;Configuration Space Coordinate Transformations&lt;/h2&gt;
&lt;p&gt;A surprising result is that phase space measure is preserved under arbitrary change of configuration space coordinates, so long as we choose the right corresponding momentum transformation so that the phase space transformation is canonical.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://danabo.github.io/blog/posts/liouville-supplemental-coordinate-transformations/#arbitrary-configuration-space-transformations&#34;&gt;Liouville Supplemental - Coordinate Transformations#arbitrary-configuration-space-transformations&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;hamiltonian-specific-canonical-transformations&#34;&gt;Hamiltonian specific canonical transformations&lt;/h2&gt;
&lt;p&gt;Previously we derived transformations that are canonical for all Hamiltonians. However, there are yet more canonical transformations, specifically those which are specific to a given Hamiltonian.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://danabo.github.io/blog/posts/liouville-supplemental-coordinate-transformations/#hamiltonian-specific-canonical&#34;&gt;Liouville Supplemental - Coordinate Transformations#hamiltonian-specific-canonical&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;entropy-and-the-bertrand-paradox&#34;&gt;Entropy And The Bertrand Paradox&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Entropy_%28statistical_thermodynamics%29#Boltzmann%27s_principle&#34;target=&#34;_blank&#34;&gt;Boltzmann entropy&lt;/a&gt; is proportional to the log of the size of a phase region. Entropy as such already appears to have an observer-arbitrary aspect to it, since the choice of phase region that describes a system depends on the observer&amp;rsquo;s state of knowledge (or their goals, see &lt;a href=&#34;https://danabo.github.io/blog/posts/the-reversibility-problem/#the-interpretation-of-state-regions&#34;&gt;The Reversibility Problem#the-interpretation-of-state-regions&lt;/a&gt;). The observer-arbitrariness of entropy is only compounded further if the choice of measure, for measuring sizes of phase regions, is also unconstrained and arbitrarily chosen. Luckily, canonical coordinates get us out of the latter conundrum.&lt;/p&gt;
&lt;p&gt;It would make sense to follow the &lt;a href=&#34;https://en.wikipedia.org/wiki/Principle_of_indifference#Application_to_continuous_variables&#34;target=&#34;_blank&#34;&gt;principle of indifference&lt;/a&gt; and always require a uniform measure on phase space. However, uniformity of measure depends on our chosen coordinate system. This problem can be viewed as a generalization of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bertrand_paradox_%28probability%29&#34;target=&#34;_blank&#34;&gt;Bertrand paradox&lt;/a&gt;. See &lt;a href=&#34;https://danabo.github.io/blog/posts/liouville-supplemental-bertrand-paradox/&#34;&gt;Liouville Supplemental - Bertrand Paradox&lt;/a&gt;. This poses a problem since physics is coordinate system agnostic - there is no preferred coordinate system.&lt;/p&gt;
&lt;p&gt;You might be tempted to propose that we express everything in Cartesian coordinates, but what is considered as &amp;ldquo;Cartesian&amp;rdquo; is not in general well defined. While we might be able to take as primitive the idea that space around us (without relativistic gravitational forces) is flat, and so placing a grid on 3D space gives us Cartesian coordinates. However, when dealing with the generalized coordinates of an arbitrary system, the relationship between those coordinates and physical 3D space becomes complex. For example, we could have coordinates representing the orientations and positions of movable rigid parts in a compound structure. Or, we might be modeling the configuration of molecular structures, with various intermolecular orientations and relationships. Expressing any of that in Cartesian spatial coordinates is difficult to say the least.&lt;/p&gt;
&lt;p&gt;Luckily, we don&amp;rsquo;t need to bother with any of that. The result above in &lt;a href=&#34;#measure-preservation&#34;&gt;#Measure Preservation&lt;/a&gt;, and moreover in &lt;a href=&#34;https://danabo.github.io/blog/posts/liouville-supplemental-coordinate-transformations/#arbitrary-configuration-space-transformations&#34;&gt;Liouville Supplemental - Coordinate Transformations#arbitrary-configuration-space-transformations&lt;/a&gt;, tells us that no matter what coordinate system we put on configuration space, uniform measure in &lt;em&gt;phase space&lt;/em&gt; will be preserved if we change configuration coordinates to anything else. Our only condition is that our phase space coordinates be canonical for the system in question, which is already taken as given in most circumstances (and if that is not true, there are many other complications to deal with).&lt;/p&gt;
&lt;p&gt;This neat property of classical mechanics also allows us to uniquely quantify information about physical systems. In &lt;a href=&#34;https://danabo.github.io/blog/posts/physical-information/&#34;&gt;Physical Information&lt;/a&gt; I define what it means to have information about a system. Basically, if we&amp;rsquo;ve narrowed down the state of the system to some subset of state space (phase space), then we have information. But if we want to quantify how much information we have (allowing us to invoke all of the machinery of Shannon&amp;rsquo;s information theory, see &lt;a href=&#34;https://danabo.github.io/blog/posts/physical-information/#shannon-quantities&#34;&gt;Physical Information#shannon-quantities&lt;/a&gt;), we need to choose a measure on state space. If there is nothing constraining our choice of measure, our quantities of information are not very &amp;hellip; informative.&lt;/p&gt;
&lt;p&gt;But with the results above, in classical mechanics, we can safely put a uniform measure/density on phase space, in our chosen configuration coordinate system (assuming our phase coordinates are canonical). That measure/density will remains uniform in any other configuration coordinate system (assuming still in canonical phase coordinates). For example, suppose we have a system described with phase space $\O$ and Hamiltonian $H$. Let $\r = \r_0$ be a constant density function on $\O$ and time. Let $\mu_{\r_0}$ be the corresponding uniform measure. The information gained by narrowing down the state of a system from phase region $R$ to region $R&#39;$ is&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
h(R&#39; \mid R) &amp;amp;= -\lg\frac{\mu_{\r_0}(R&#39;)}{\mu_{\r_0}(R)} \\&lt;br&gt;
&amp;amp;= -\lg\frac{\int_{R&#39;} \r_0\ \dd^{2n}\vec{\o}}{\int_{R} \r_0\ \dd^{2n}\vec{\o}}  \\&lt;br&gt;
&amp;amp;= -\lg\frac{\r_0\int_{R&#39;}\dd^{2n}\vec{\o}}{\r_0\int_{R}\dd^{2n}\vec{\o}}  \\&lt;br&gt;
&amp;amp;= -\lg\frac{\int_{R&#39;}\dd^{2n}\vec{\o}}{\int_{R}\dd^{2n}\vec{\o}} \,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;So our information gain (and similarly change in entropy - see &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;Connecting Entropy And Information&lt;/a&gt;) of the system does not depend on $\r_0$. We can go ahead and use the Euclidean measure $\l=\mu_1$ with $\r_0=1$. Note that even if the measure of the entire phase space is infinite, i.e. $\l(\O)=\infty$, we still get finite quantities of information gain so long as the starting region $R$ has finite measure. That is to say, going from knowing that a gas is in a box to knowing that the gas is in a smaller box gives us finitely many bits of information. On the other hand, going from knowing nothing to knowing there is a gas in a box gives us infinite information, since in that case $R=\O$. Since we are not supposing we are randomly drawing states of the system, we do not need to work with normalizable measures. The measure here simply measures sizes of regions.&lt;/p&gt;
&lt;p&gt;Note that statistical thermodynamics makes use of this coordinate-invariant measure result to justify using uniform measures on phase space. E.g. see &lt;a href=&#34;https://www.elsevier.com/books/statistical-mechanics/beale/978-0-12-382188-1&#34;target=&#34;_blank&#34;&gt;Statistical Mechanics 4th ed.&lt;/a&gt; (Pathria &amp;amp; Beale) section 2.2. This relieves us from being restricted to equilibrium processes (which are ergodic) where the choice of measure is determined by &lt;a href=&#34;https://en.wikipedia.org/wiki/Ergodicity#Measure-preserving_dynamical_systems&#34;target=&#34;_blank&#34;&gt;measure invariance through time&lt;/a&gt;. Here the measure actually represents a frequentist distribution of states over time, via &lt;a href=&#34;https://en.wikipedia.org/wiki/Ergodicity&#34;target=&#34;_blank&#34;&gt;ergodicity&lt;/a&gt;. However, we can freely put a uniform measure on the phase space of an arbitrary non-equilibrium process, where that measure represents sizes of phase regions and is used to quantify information gain. This doesn&amp;rsquo;t, however, resolve the open problems in statistical mechanics and statistical thermodynamics, which have more to do with course-graining complexity. See &lt;a href=&#34;https://en.wikipedia.org/wiki/Non-equilibrium_thermodynamics#Difference_between_equilibrium_and_non-equilibrium_thermodynamics&#34;&gt;https://en.wikipedia.org/wiki/Non-equilibrium_thermodynamics#Difference_between_equilibrium_and_non-equilibrium_thermodynamics&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Renormalization_group&#34;&gt;https://en.wikipedia.org/wiki/Renormalization_group&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Carnot Cycle</title>
      <link>https://danabo.github.io/blog/posts/carnot-cycle/</link>
      <pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/carnot-cycle/</guid>
      <description>&lt;p&gt;This a formal description of the Carnot cycle which I hope is a useful reference for anyone who wants to quickly ramp up on thermodynamics. The Carnot cycle is often used as a canonical introduction to classical thermodynamics (specifically the thermodynamics of ideal gasses) since it nicely illustrates the relationship between the entropy, temperature and volume of a gas.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Carnot_cycle&#34;target=&#34;_blank&#34;&gt;Carnot cycle&lt;/a&gt; is a toy thermodynamic process (read as &amp;ldquo;thought experiment&amp;rdquo;) devised by &lt;a href=&#34;https://en.wikipedia.org/wiki/Nicolas_L%C3%A9onard_Sadi_Carnot&#34;target=&#34;_blank&#34;&gt;Sadi Carnot&lt;/a&gt; in 1824 to derive the first theory of heat efficiency (essentially a proto-thermodynamics). Carnot proves that his hypothetical cycle achieves the theoretically optimal efficiency of any heat engine that converts heat into mechanical work on a piston. Carnot also shows that the optimal efficiency of any transformation is achieved by a &lt;a href=&#34;https://en.wikipedia.org/wiki/Reversible_process_%28thermodynamics%29&#34;target=&#34;_blank&#34;&gt;reversible process&lt;/a&gt; which caries it out, thus establishing the connection between thermodynamics and reversibility (the Carnot cycle is reversible). See &lt;a href=&#34;https://danabo.github.io/blog/posts/the-reversibility-problem/#reversibility-and-thermodynamics&#34;&gt;The Reversibility Problem#reversibility-and-thermodynamics&lt;/a&gt; for more discussion on that.&lt;/p&gt;
&lt;p&gt;Later, &lt;a href=&#34;https://en.wikipedia.org/wiki/Rudolf_Clausius&#34;target=&#34;_blank&#34;&gt;Rudolf Clausius&lt;/a&gt; reformulated Carnot&amp;rsquo;s theory and &lt;a href=&#34;https://en.wikipedia.org/wiki/History_of_entropy#1862_definition&#34;target=&#34;_blank&#34;&gt;introduced a new concept&lt;/a&gt;: entropy. This marks the inception of &lt;a href=&#34;https://en.wikipedia.org/wiki/Thermodynamics#Classical_thermodynamics&#34;target=&#34;_blank&#34;&gt;classical thermodynamics&lt;/a&gt;, where entropy is taken as a primitive property of processes involving heat, like energy and mass. Later, &lt;a href=&#34;https://en.wikipedia.org/wiki/Ludwig_Boltzmann&#34;target=&#34;_blank&#34;&gt;Ludwig Boltzmann&lt;/a&gt; attempted to derive entropy and classical thermodynamics from Newtonian mechanics, giving rise to &lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_thermodynamics&#34;target=&#34;_blank&#34;&gt;statistical thermodynamics&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\ms}{\mathscr}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\es}{\emptyset}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\L}{\Lambda}&lt;br&gt;
\newcommand{\G}{\Gamma}&lt;br&gt;
\newcommand{\g}{\gamma}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\N}{\mb{N}}&lt;br&gt;
\newcommand{\I}{\mb{I}}&lt;br&gt;
\newcommand{\H}{\mc{H}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\s}{\sigma}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\tup}{\par}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[1]{_{\mid #1}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\Dt}{{\Delta t}}&lt;br&gt;
\newcommand{\tr}{\rightarrowtail}&lt;br&gt;
\newcommand{\qed}{\ \ \blacksquare}&lt;br&gt;
\newcommand{\c}{\overline}&lt;br&gt;
\newcommand{\dg}{\dagger}&lt;br&gt;
\newcommand{\dd}{\mathrm{d}}&lt;br&gt;
\newcommand{\pd}{\partial}&lt;br&gt;
\newcommand{\diff}[2]{\frac{\dd{#2}}{\dd{#1}}}&lt;br&gt;
\newcommand{\Ue}{U_{\text{ext}}}&lt;br&gt;
\newcommand{\Ui}{U_{\text{int}}}&lt;br&gt;
$$&lt;/p&gt;
&lt;h1 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h1&gt;
&lt;p&gt;I will derive everything below from the laws of classical mechanics, plus three additional useful equations characterizing ideal gasses.&lt;/p&gt;
&lt;p&gt;For an &lt;a href=&#34;https://en.wikipedia.org/wiki/Ideal_gas&#34;target=&#34;_blank&#34;&gt;ideal gas&lt;/a&gt; at &lt;a href=&#34;https://en.wikipedia.org/wiki/Thermodynamic_equilibrium&#34;target=&#34;_blank&#34;&gt;equilibrium&lt;/a&gt; with pressure $P$, volume $V$, temperature $T$, &lt;a href=&#34;https://en.wikipedia.org/wiki/Gas_constant&#34;target=&#34;_blank&#34;&gt;gas constant&lt;/a&gt; $R$ (depends on the type of gas), and number of &lt;a href=&#34;https://en.wikipedia.org/wiki/Amount_of_substance&#34;target=&#34;_blank&#34;&gt;moles&lt;/a&gt; $n$ of gas (i.e. number of particles divided by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Avogadro_constant&#34;target=&#34;_blank&#34;&gt;Avogadro constant&lt;/a&gt;), the &lt;a href=&#34;https://en.wikipedia.org/wiki/Ideal_gas_law&#34;target=&#34;_blank&#34;&gt;ideal gas law&lt;/a&gt; states that&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
PV=nRT\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;All the equilibrium states of the gas are fully described by the triple $(P,V,T)$, with one of the three being redundant due to the ideal gas law. In this post, I will use $(T,V)$ to describe the state of the gas at equilibrium.&lt;/p&gt;
&lt;p&gt;For an ideal gas initially at equilibrium with volume and temperature $V_i,T_i$, and later at equilibrium again with final volume and temperature $V_f,T_f$, the change in entropy of this gas is (&lt;a href=&#34;https://web.mit.edu/16.unified/www/FALL/thermodynamics/notes/node40.html&#34;target=&#34;_blank&#34;&gt;source 1&lt;/a&gt;, &lt;a href=&#34;http://hyperphysics.phy-astr.gsu.edu/hbase/Therm/entropgas.html&#34;target=&#34;_blank&#34;&gt;source 2&lt;/a&gt;, &lt;a href=&#34;https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Physical_Chemistry_%28Fleming%29/05%3A_The_Second_Law/5.04%3A_Calculating_Entropy_Changes&#34;target=&#34;_blank&#34;&gt;source 3&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\D S = nC_V \ln\par{\frac{T_f}{T_i}} + nR \ln\par{\frac{V_f}{V_i}}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $C_V$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Molar_heat_capacity&#34;target=&#34;_blank&#34;&gt;molar heat capacity at a constant volume&lt;/a&gt; (another constant that depends on the type of gas) and $R$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gas_constant&#34;target=&#34;_blank&#34;&gt;gas constant&lt;/a&gt;. In the context of this post, this is the definition of entropy, which will be taken as a primitive quantity. This definition is agnostic to how we define the absolute entropy of a gas at a particular temperature and volume. Only changes in entropy will be considered in this post.&lt;/p&gt;
&lt;p&gt;Note that the gas only needs to be at equilibrium at the start and end of the transformation and need not be in equilibrium during the change. This is because entropy is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Entropy#State_variables_and_functions_of_state&#34;target=&#34;_blank&#34;&gt;state variable&lt;/a&gt;, which means that the entropy of a system only depends on its current state (if the entropy is well defined in its current state), and not the path the system took to get to its current state. This allows us to calculate changes in entropy for processes with undefined entropy during intermediate steps. In this post, the entropy of a gas is not defined outside of equilibrium.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://chemed.chem.purdue.edu/genchem/topicreview/bp/ch21/chemical.php#inter&#34;target=&#34;_blank&#34;&gt;internal energy&lt;/a&gt; of an ideal gas is the total kinetic energy (KE) of all the particles of the gas. When in equilibrium, the temperature $T$ of the gas is proportional to the average KE across all the degrees of freedom of the gas (e.g. the $x$, $y$ and $z$ axes for each particle). Then the total KE is proportional to $T$ and $n$ (proportional to the number of particles in the gas). We have the internal energy of the gas,&lt;/p&gt;
&lt;p&gt;$$E_\text{int} = nC_VT\,,$$&lt;/p&gt;
&lt;p&gt;where again, $n$ is the number of moles of the gas and $C_V$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Molar_heat_capacity&#34;target=&#34;_blank&#34;&gt;molar heat capacity at a constant volume&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that some texts use &lt;a href=&#34;https://en.wikipedia.org/wiki/Specific_heat_capacity&#34;target=&#34;_blank&#34;&gt;specific heat capacity at constant volume&lt;/a&gt;, $c_v$, which is written as lower-case, instead of the molar specific heat, $C_V$. They&amp;rsquo;re related by $c_v = nC_V$.&lt;/p&gt;
&lt;h1 id=&#34;phases-of-the-carnot-cycle&#34;&gt;Phases Of The Carnot Cycle&lt;/h1&gt;
&lt;p&gt;The Carnot cycle consists of four phases (or steps), involving a gas in a container (which I&amp;rsquo;ll refer to simply as &amp;ldquo;the gas&amp;rdquo;) with a movable wall called &amp;ldquo;the piston&amp;rdquo;. Those four phases are, &lt;a href=&#34;#1-isothermal-expansion&#34;&gt;#1 Isothermal expansion&lt;/a&gt;, &lt;a href=&#34;#2-isentropic-adiabatic-expansion&#34;&gt;#2 Isentropic adiabatic expansion&lt;/a&gt;, &lt;a href=&#34;#3-isothermal-compression&#34;&gt;#3 Isothermal compression&lt;/a&gt; and &lt;a href=&#34;#4-isentropic-adiabatic-compression&#34;&gt;#4 Isentropic adiabatic compression&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;During the cycle, the piston pushes against the gas with some force. The magnitude of that force determines whether the container is expanding or compressing (whether the force the piston exerts is less than or greater than the gas&amp;rsquo;s pressure against the container walls). The piston is what we use to do work, e.g. pushing a mass up against gravity or pushing the wheels of a train against friction. The resistance against the piston determines the force the piston applies against the gas. We can simplify this all as a potential energy field that the piston lies in as a function of the piston&amp;rsquo;s position.&lt;/p&gt;
&lt;p&gt;Illustration of a piston doing mechanical work - in this case rotating a wheel (for example, this could be applied to a locomotive):&lt;br&gt;


  &lt;figure&gt;
    
    &lt;img src=&#34;https://lawofthermodynamicsinfo.com/wp-content/uploads/2020/04/heat-engine-gif-7.gif&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Illustration of a piston. Source: https://lawofthermodynamicsinfo.com/what-is-carnot-cycle-in-thermodynamics/&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;At various phases in the cycle, the gas may or may not be in thermal contact with heat reservoirs (each phase defines which heat reservoirs are present). Heat reservoirs are idealized systems with infinitely many degrees of freedom (e.g. particle positions) which maintain their temperature no matter how much heat energy is added or subtracted. For example, a gas with infinitely many particles occupying infinite volume. A heat reservoir that provides heat energy output is called a heat source, and a heat reservoir that absorbs heat energy input is called a heat sink. Heat sources are often called &amp;ldquo;hot&amp;rdquo; and heat sinks are often called &amp;ldquo;cold&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://www.saburchill.com/physics/images_thermal_physics/Carnot_cycle_05.gif&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;This animation illustrates the four phases in action. To understand the details of what is happening, please refer to the formal definition of each phase in the following sections. Source: https://www.saburchill.com/physics/chapters/0124b.html&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://www.grc.nasa.gov/www/k-12/airplane/Images/carnot.gif&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;This diagram shows the four phases with variable amounts of mass applied to the piston. In the following sections I go in depth in calculating what those masses should be and how they should change throughout the cycle, since this can be a point of confusion. Source: https://www.grc.nasa.gov/www/k-12/airplane/carnot.html&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Also, this video is a good illustration of the carnot cycle in action (with particle level simulation of the gas): &lt;a href=&#34;https://www.youtube.com/watch?v=M6XQi8eYYNs&#34;&gt;https://www.youtube.com/watch?v=M6XQi8eYYNs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All phases of the Carnot cycle are &lt;a href=&#34;https://en.wikipedia.org/wiki/Quasistatic_process&#34;target=&#34;_blank&#34;&gt;quasistatic&lt;/a&gt;, which means that the process stays close to equilibrium during the changes that take place. What this means for us is that we can assume that the entropy, temperature, volume and pressure of the gas are always well defined through out each phase. The quasistatic assumption is needed to ensure that every phase of the cycle is reversible, which makes the cycle as a whole reversible.&lt;/p&gt;
&lt;p&gt;The net result of the Carnot cycle is that energy is extracted from a heat source to move a piston (do useful work), but not entirely so (see discussion on &lt;a href=&#34;#efficiency&#34;&gt;#Efficiency&lt;/a&gt;). Some of the energy is dumped into a heat sink as waste. However, that waste heat is recoverable. The Carnot cycle is reversible. When run in reverse, heat energy is extracted from a cold reservoir, and combined with work added by the piston, to produce heat energy dumped into the hot reservoir. The reverse Carnot cycle is a heat pump that moves heat from cold to hot, i.e. a refrigerator where the cold reservoir is the inside that we are keeping cold. If during the Carnot cycle and waste heat is not dumped into the heat sink but instead lost to the ambient environment, that is energy which cannot be recovered in the reverse cycle, making the forward cycle irreversible.&lt;/p&gt;
&lt;h2 id=&#34;1-isothermal-expansion&#34;&gt;1. Isothermal expansion&lt;/h2&gt;
&lt;p&gt;Reference: &lt;a href=&#34;https://en.wikipedia.org/wiki/Isothermal_process&#34;&gt;https://en.wikipedia.org/wiki/Isothermal_process&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://www.grc.nasa.gov/www/k-12/airplane/Animation/gaslab/Images/chprmt.gif&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;An animation of isothermal compression and its reversal, isothermal expansion. The temperature and mass of the gas (number of particles) is held fixed. Here mass is placed on top of the piston to provide a force. That force must be continuously adjusted over time for the piston to move. Source: https://www.grc.nasa.gov/www/k-12/airplane/boyle.html&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://opentextbc.ca/universityphysicsv2openstax/wp-content/uploads/sites/275/2019/07/CNX_UPhysics_20_04_Piston.jpg&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;This shows the heat reservoir for the isothermal process. Source: https://opentextbc.ca/universityphysicsv2openstax/chapter/thermodynamic-processes/&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Let the gas start this phase in equilibrium at temp $T_i$ and volume $V_i$ (confined to a container).&lt;br&gt;
As the name of this phase suggests, the container will expand to a new volume $V_f$, with the constraint that the temperature of the gas remains constant, so that $T_i=T_f=T_H$ ($T_H$ is the &amp;ldquo;hot&amp;rdquo; temperature). Hence the adjective &amp;ldquo;iso&amp;rdquo;-&amp;ldquo;thermal&amp;rdquo;. This is achieved by keeping the container (and thus the gas) in thermal contact with a heat reservoir which maintains a temperature $T_H$ during the expansion. This reservoir is a heat source since in this phase we will exact heat energy from it to do work.&lt;/p&gt;
&lt;p&gt;Using the change in entropy definition above, we have the change in entropy of the gas during phase 1:&lt;br&gt;
$$\begin{aligned}\D S\up{1}&amp;amp;= nC_V \ln\par{\frac{T_H}{T_H}} + nR \ln\par{\frac{V_f}{V_i}} \\ &amp;amp;= nR \ln\par{\frac{V_f}{V_i}}\,.\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s suppose the gas container has an &lt;a href=&#34;https://en.wikipedia.org/wiki/Extrusion&#34;target=&#34;_blank&#34;&gt;extruded shape&lt;/a&gt; (has a fixed &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross_section_%28geometry%29&#34;target=&#34;_blank&#34;&gt;cross-sectional&lt;/a&gt; profile), where one end of the extruded profile is capped off with a movable wall (i.e. piston) with area $A$. If $x$ is the length of the container (which is also the position of the piston), then the volume of the container is $V(x) = Ax$. Then the initial and final volumes of the gas are $V_i=Ax_i$ and $V_f=Ax_f$ respectively, so that we have $\D S\up{1} = nR \ln\par{\frac{x_f}{x_i}}$, which depends only on the piston&amp;rsquo;s position.&lt;/p&gt;
&lt;p&gt;Let $U(x)$ be the potential energy of the piston at position $x$ so that the piston pushes on the gas with force $F = -U&#39;(x) = -\frac{\dd}{\dd{x}}U(x)$, i.e. this force is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Potential_energy#Derivable_from_a_potential&#34;target=&#34;_blank&#34;&gt;negative spatial derivative&lt;/a&gt; of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Scalar_potential&#34;target=&#34;_blank&#34;&gt;scalar potential field&lt;/a&gt; $U$. For example, if the piston is pushing a mass $m$ &lt;a href=&#34;https://en.wikipedia.org/wiki/Gravitational_energy&#34;target=&#34;_blank&#34;&gt;against gravity&lt;/a&gt;, then $U(x)=mgx$ with gravitational constant $g$, and so $F = -U&#39;(x)  = -mg$.&lt;/p&gt;
&lt;p&gt;The net work done by the gas on the piston in phase 1 is $W\up{1} = U(x_f)-U(x_i)$, since the forces involved here are &lt;a href=&#34;https://en.wikipedia.org/wiki/Conservative_force&#34;target=&#34;_blank&#34;&gt;conservative&lt;/a&gt; by necessity, as they are derived from the derivative of a potential field. In this case we say that the work done is &lt;a href=&#34;https://en.wikipedia.org/wiki/Work_%28physics%29#Path_independence&#34;target=&#34;_blank&#34;&gt;path independent&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The work done by the gas on the piston equals heat energy $Q_H$ extracted from the heat source, so we have $Q_H = W\up{1}$. Since the temperature of the gas remains constant, the internal energy of the gas is also constant. Thus any heat energy converted into work on the piston must come from the heat source. So in phase 1, the source provides heat energy $Q_H$ as input to the Carnot cycle.&lt;/p&gt;
&lt;p&gt;This phase has no determined end. The user is free to choose $x_f &amp;gt; x_i$, which is the point at which the user switches the process to phase 2. The choice of $x_f$ determines how much work will be done on the piston during the cycle, but the efficiency of the cycle is independent of $x_f$. See &lt;a href=&#34;#choosing-the-end-of-phase-1&#34;&gt;#Choosing the end of phase 1&lt;/a&gt; for details.&lt;/p&gt;
&lt;h3 id=&#34;potential-calculation&#34;&gt;Potential calculation&lt;/h3&gt;
&lt;p&gt;The potential function $U(x)$ is actually fully determined by the given contraints, i.e. we must choose a particular $U(x)$ if we want phase 1 to be quasistatic (approx. at equilibrium at all times) and reversible. Let&amp;rsquo;s derive  $U(x)$.&lt;/p&gt;
&lt;p&gt;For this process to be quasistatic, the force $F(x)=-U&#39;(x)$ that the piston exerts on the gas needs to equal the force the gas exerts on the container at all times, i.e. $F(x)=-AP(x)$, and so $U&#39;(x)=AP(x)$ for all $x$, where $A$ is area and the pressure $P(x)$ is force per area.&lt;/p&gt;
&lt;p&gt;Technically, if the piston&amp;rsquo;s force and gas pressure are exactly balanced then the piston won&amp;rsquo;t move. Assuming the gas was already at equilibrium, then it continues to be at equilibrium since nothing happens. If we suppose that the piston pushes against the gas with slightly less force than the gas exerts on the piston, then the piston is pushed out (the container expands). So we have $F(x)+\e=-AP(x)$ where $\e &amp;gt; 0$ is small. If this difference in forces $\e$ is small enough, we can fudge things and suppose the gas remains close to equilibrium during the change (the meaning of quasistatic), and that $\e$ is negligable and doesn&amp;rsquo;t need to be modeled, i.e. let $\e=0$ (we could call $\e$ infinitessimal). If you don&amp;rsquo;t find this explanation satisfying, perhaps an alternative conceptualization in &lt;a href=&#34;#a-note-about-quasistatic-processes&#34;&gt;#A note about quasistatic processes&lt;/a&gt; would be more convincing.&lt;/p&gt;
&lt;p&gt;Rearranging the ideal gas law, we have $P(x) = nR\frac{T(x)}{V(x)}$, where $T(x)=T_H$ and $V(x)=Ax$. Plugging this into $U&#39;(x)=AP(x)$, we get $U&#39;(x)=AnR\frac{T_H}{Ax}=nR\frac{T_H}{x}$. Integrating, we get $$U(x)=\int_{x_i}^{x}  nR\frac{T_H}{x} \dd{x}=nRT_H\ln\par{\frac{x}{x_i}}\,.$$&lt;/p&gt;
&lt;p&gt;Note that $U(0)=0$, i.e. we are setting the zero potential at the initial position $x_i$ (potential functions are invariant up to an additive constant, i.e. absolute potential energies are arbitrary, the physically meaningful quantities are changes in potential). Thus, the cumulative work done on the piston by the gas at any point in the phase 1 process is $W(x)=U(x)-U(x_i)=U(x)=nRT_H\ln\par{\frac{x_f}{x_i}}$.&lt;/p&gt;
&lt;p&gt;But then, how are we free to choose what the piston acts on? E.g. earlier I gave the examples of the piston pushing a mass against gravity, or pushing the wheels of a train. These applications also determine $U(x)$, and so it would seem our problem is over-constrained. It must be the case that some other parameter in the potential function is itself a function of $x$.&lt;/p&gt;
&lt;p&gt;In the case where the piston is pushing a mass against gravity, the gravitational potential is $U(x)=mg(x-x_i)$ where $U(0)=0$. Assuming we are close to the surface of the earth so that $g$ is constant, then our only choice is to make $m$ variable, i.e. $m$ becomes $m(x)$, a function of $x$. We find $m(x)$ by setting the gravitational potential equal to the potential function that makes phase 1 quasistatic:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}m(x)\cdot g\cdot(x-x_i)&amp;amp;=nRT_H\ln\par{\frac{x}{x_i}} \\ m(x)&amp;amp;=\frac{nRT_H}{g}\ln\left[\par{\frac{x}{x_i}}^{1/(x-x_i)}\right]\,.\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Note that there is a hole in this function at $x=x_i$, but we can fill it in by taking the limit:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lim_{x\to x_i}\frac{nRT_H}{g}\ln\left[\par{\frac{x}{x_i}}^{1/(x-x_i)}\right] = \frac{nRT_H}{gx_i}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020220317170313.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;An example plot of $m(x)$ with the hole at $x_i$ shown.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;a-note-about-quasistatic-processes&#34;&gt;A note about quasistatic processes&lt;/h3&gt;
&lt;p&gt;Another way to think about this phase 1 quasistatic process is that we are finding a potential field $U(x)$ s.t. at every $x$ the forces of the piston and gas are exactly balanced, which implies the piston and gas will be static at equilibrium for all $x$ (temperature and position will remain constant without external intervention).&lt;/p&gt;
&lt;p&gt;The idea of a quasistatic transformation is that the otherwise static system is repeatedly nudged slightly out of equilibrium from the outside over and over again, where the system is allowed to return to equilibrium in between each nudge. This nudging from the outside is how changes to $x$ and $T$ are driven and controlled. For example, if the piston&amp;rsquo;s force on the gas is due to mass resting on top of the piston and the piston is at position $x$, if the operator changes that mass from $m(x)$ to $m(x+\e)$ for some small $\e &amp;gt; 0$, then the gas will push the piston out until the piston position becomes $x+\e$ where the forces are again balanced. Technically, since the gas momentarily left equilibrium during this change, we should expect some inefficiency in the form of irreversible gain of entropy. The fudge is now that we neglect that inefficiency and assume an ideal reversible transformation.&lt;/p&gt;
&lt;h2 id=&#34;2-isentropic-adiabatic-expansion&#34;&gt;2. Isentropic (adiabatic) expansion&lt;/h2&gt;
&lt;p&gt;Reference: &lt;a href=&#34;https://en.wikipedia.org/wiki/Isentropic_process&#34;&gt;https://en.wikipedia.org/wiki/Isentropic_process&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;video src=&#34;https://thumbs.gfycat.com/DependableBrightApatosaur-mobile.mp4&#34; controls&gt;&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;Let the gas start this phase at temp $T_i$ and volume $V_i=Ax_i$, with piston area $A$ and container length $x_i$. I hope it is not confusing if I use &amp;ldquo;initial&amp;rdquo; and &amp;ldquo;final&amp;rdquo; variables to refer to the start and end of each phase separately, so that phase 2 initial temperature equals phase 1 final temperature and phase 2 initial volume equals phase 1 final volume. Thus $T_i = T_H$. I&amp;rsquo;ll use superscripts to distinguish variables from different phases when they appear together, so we have $x_i\up{2} = x_f\up{1}$, meaning that the initial piston position of phase 2 (this phase) equals the final piston position of phase 1 (the last phase).&lt;/p&gt;
&lt;p&gt;This phase is defined by the constraint that the gas remains at constant entropy (hence &amp;ldquo;iso&amp;rdquo;-&amp;ldquo;entropic&amp;rdquo;). Then let&lt;/p&gt;
&lt;p&gt;$$\D S\up{2}=0\,,$$&lt;/p&gt;
&lt;p&gt;so that $0 = nC_V \ln\par{\frac{T_f}{T_i}} + nR \ln\par{\frac{x_f}{x_i}}$, where we plugged in $V_i=Ax_i$ and $V_f=Ax_f$. Note that the gas is not in thermal contact with any heat reservoir in this phase. Then any energy transferred to the piston (as potential energy) must come from the internal energy of the gas, lowering the temperature of the gas. (In general, compression or expansion of a thermally isolated gas need not be isentropic. The piston must move in a specific way for the gas&amp;rsquo;s entropy to remain unchanged.)&lt;/p&gt;
&lt;p&gt;If we assume the user stops phase 2 at $x_f &amp;gt; x_i$, then we can solve for the final temperature $T_f$ using the change in entropy formula:&lt;br&gt;
$$\begin{aligned}\ln T_f&amp;amp;=\ln T_i + \frac{R}{C_V} \ln\par{\frac{x_i}{x_f}}\\&lt;br&gt;
T_f&amp;amp;=T_i\par{\frac{x_i}{x_f}}^{R/C_V}\,.\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Note that $T_f &amp;lt; T_i$ when $x_f &amp;gt; x_i$, implying that energy is transferred from the gas to the piston as potential energy.&lt;/p&gt;
&lt;p&gt;The work done by the gas on piston in phase 2 is $W\up{2} = U(x_f)-U(x_i)$. As was said before, the energy for that work comes entirely from the gas itself as internal energy (i.e. KE of the gas particles). Then we have $\D E_\text{int}\up{2} = -W\up{2}$, i.e. change in internal (heat) energy of the gas equals negative amount of energy used to do work (energy outflow). Using the internal energy formula of an ideal gas, we have&lt;br&gt;
$$\D E_\text{int}\up{2} = nC_V\D T = nC_V(T_f-T_i)\,.$$&lt;/p&gt;
&lt;p&gt;Then the work done on the piston by the gas during phase 2 is $W\up{2}=-\D E_\text{int}\up{2}=nC_V(T_i-T_f)$&lt;/p&gt;
&lt;p&gt;Let $T_f = T_C$, which is the temperature of a cold heat reservoir (heat sink) that will be used in phase 3. Combined with $T_i = T_H$, we have&lt;br&gt;
$$W\up{2}=nC_V(T_H-T_C)\,.$$&lt;/p&gt;
&lt;h3 id=&#34;potential-calculation-1&#34;&gt;Potential calculation&lt;/h3&gt;
&lt;p&gt;Again we are constrained to a particular $U(x)$ by requiring that phase 2 be quasistatic. As in phase 1, we have $U&#39;(x)=AP(x)=AnR\frac{T(x)}{V(x)}=nR\frac{T(x)}{x}$ where $V(x)=Ax$. Now temperature also is variable. Specifically, $T(x)=T_i\par{\frac{x_i}{x}}^{R/C_V}$, which we derived earlier (replacing $x_f$ with $x$ and $T_f$ with $T(x)$).&lt;br&gt;
Plugging in to $U&#39;(x)$, we get $U&#39;(x) = nR\frac{1}{x}T_i\par{\frac{x_i}{x}}^{R/C_V}$. Integrating, we get&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
U(x)&amp;amp;=\int_{x_i}^x U&#39;(\chi)\dd{\chi}\\&amp;amp;=nC_V T_i\par{1-\par{\frac{x}{x_i}}^{-R/{C_V}}}\\&amp;amp;=nC_V\Big(T_i-T(x)\Big)\,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;As a sanity check, we get the same result if we instead perform the following derivation: $U(x)=U(x)-U(x_i)=W(x)=-\D E_\text{int}(x) = -nC_V\Big(T(x)-T_i\Big)$, where $U(x_i)=0$, and $W(x)$ and $\D E_\text{int}(x)$ are the cumulative work done and change in gas internal energy respectively.&lt;/p&gt;
&lt;p&gt;Again, if we are using the gravitational potential $U(x)=m\cdot g\cdot (x-x_i)$ so that $U(0)=0$, then $m$ must become $m(x)$, a function of $x$. Then&lt;br&gt;
$$\begin{aligned}m(x)\cdot g\cdot(x-x_i)&amp;amp;=nC_V\Big(T_i-T(x)\Big) \\ m(x)&amp;amp;=\frac{nC_V}{g}\par{\frac{T_i-T(x)}{x-x_i}} \\ m(x)&amp;amp;=\frac{nC_V}{g}T_i\par{\frac{1-\par{\frac{x_i}{x}}^{R/C_V}}{x-x_i}}\,.\end{aligned}$$&lt;/p&gt;
&lt;p&gt;In phase 2, $T(x) \leq T_i$ and $x \geq x_i$, so this quantity is positive.&lt;/p&gt;
&lt;p&gt;This function also has a hole at $x=x_i$. Taking the limit, we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lim_{x\to x_i}\frac{nC_VT_i}{g}\par{\frac{1-\par{\frac{x_i}{x}}^{R/C_V}}{x-x_i}} = \frac{nRT_i}{gx_i}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is the same limit at $x_i$ as the mass function from phase 1.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020220317170617.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Plot of $m(x)$ in blue with the hole at $x_i$ shown. The orange plot is the mass function from phase 1 for comparison. Both share the same hole.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;3-isothermal-compression&#34;&gt;3. Isothermal compression&lt;/h2&gt;
&lt;p&gt;The purpose of the remaining two phases is to return the gas to its initial state (from phase 1), completing the cycle. This is achieved by running phase 1 in reverse (phase 3) and then phase 2 in reverse (phase 4). Note that this is not equivalent to reversing the joint 1+2 process (that would requiring reversing phase 2 and then phase 1), and the result is a net transfer of energy from the heat source to the heat sink and piston by the end of the cycle. So while the gas itself is reset (making the four phases a cycle), the environment has a net change. In this way, the useful work we get out of the first two phases is not entirely undone by the next two phases (otherwise the Carnot cycle would be useless).&lt;/p&gt;
&lt;p&gt;As promised, phase 3 is phase 1 in reverse, with $T_i=T_f=T_C$ where $T_C&amp;lt;T_H$ is the &amp;ldquo;cold&amp;rdquo; temperature, i.e. temperature of a heat reservoir that is colder than the hot reservoir. The initial piston position in this phase is the final piston position from the last phase.&lt;/p&gt;
&lt;p&gt;This phase is isothermal, so we have&lt;br&gt;
$$\D S\up{3} = nR \ln\par{\frac{x_f}{x_i}}\,.$$&lt;/p&gt;
&lt;p&gt;Unlike in phase 1, in phase 3 we have $x_f &amp;lt; x_i$ which means $\D S\up{3} &amp;lt; 0$.&lt;/p&gt;
&lt;p&gt;Unlike phases 1 and 2, the final positions of phases 3 and 4 are determined by the requirement that the net change in entropy of the gas at the end of the cycle be zero (this is what it means for the cycle to be reversible), i.e. $\D S=\D S\up{1}+\D S\up{2}+\D S\up{3}+\D S\up{4}=0$.&lt;br&gt;
Since $\D S\up{2}=\D S\up{4} = 0$ (both phases are isentropic), we must have&lt;/p&gt;
&lt;p&gt;$$\D S\up{3} = -\D S\up{1}\,.$$&lt;/p&gt;
&lt;p&gt;Let $x_i\up{1},\ x_f\up{1}$ be the initial and final piston positions in phase 1, and $x_i\up{3},\ x_f\up{3}$ be the initial and final piston positions in phase 3. Using $\D S\up{1} = nR \ln\par{\frac{x_f\up{1}}{x_i\up{1}}}$ and $\D S\up{3} = nR \ln\par{\frac{x_f\up{3}}{x_i\up{3}}}$, we have&lt;br&gt;
$$\begin{aligned}&lt;br&gt;
nR \ln\par{\frac{x_f\up{1}}{x_i\up{1}}} &amp;amp;= -nR \ln\par{\frac{x_f\up{3}}{x_i\up{3}}} \\&lt;br&gt;
\frac{x_f\up{3}}{x_i\up{3}} &amp;amp;= \frac{x_i\up{1}}{x_f\up{1}} \\&lt;br&gt;
x_f\up{3}&amp;amp;=x_i\up{3}\frac{x_i\up{1}}{x_f\up{1}}\,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Using the potential function $U(x)=nRT_H\ln\par{\frac{x}{x_i}}$ that we derived in phase 1, we calculate the work done on the piston by the gas: $$W\up{3}=U(x_f\up{3})-U(x_i\up{3})=nRT_C\ln\par{\frac{x_f\up{3}}{x_i\up{3}}}\,.$$&lt;/p&gt;
&lt;p&gt;Phase 3 is doing compression, i.e. $x_f &amp;lt; x_i$. Then $W\up{3} &amp;lt; 0$, indicating that the piston is doing positive work on the gas, i.e. transfer of piston potential energy back into the gas as heat energy. That would heat up the gas, but the gas is in thermal contact with the cold reservoir, so the heat energy is transferred there (making it a heat sink), and the gas&amp;rsquo;s temperature remains fixed at $T_C$. As in phase 1, the work done on the piston is fully accounted for by the heat energy exchanged by the heat reservoir, i.e. $Q_C = W\up{3}$. However, this time, since the piston is doing work on the gas, $W\up{3}$ is negative indicating that $Q_C$ negative, meaning we are depositing heat energy of magnitude $\abs{Q_C}$ into the heat sink. Thus in phase 3, the Carnot cycle produces heat energy $\abs{Q_C}$ as output.&lt;/p&gt;
&lt;h2 id=&#34;4-isentropic-adiabatic-compression&#34;&gt;4. Isentropic (adiabatic) compression&lt;/h2&gt;
&lt;p&gt;This is phase 2 in reverse.&lt;br&gt;
$\D S\up{4}=0$.&lt;/p&gt;
&lt;p&gt;Work done on the piston by the gas is $W\up{4}=-nC_V\D T = nC_V(T_i-T_f)=nC_V(T_C-T_H)=-W\up{2}$ where $T_C=T_i$ and $T_H=T_f$.&lt;/p&gt;
&lt;p&gt;We require that $x_f\up{4} = x_i\up{1}$ to complete the cycle.&lt;/p&gt;
&lt;h1 id=&#34;entropy-of-the-environment&#34;&gt;Entropy Of The Environment&lt;/h1&gt;
&lt;p&gt;The total entropy across the gas and the heat reservoirs remains fixed.&lt;/p&gt;
&lt;p&gt;Because the heat reservoirs are infinite in size and are held at constant temperature, their change in entropy is not well defined. However, we can approach this ideal by considering finite heat reservoirs, i.e. other gasses in containers which are much larger than the gas being manipulated. If we suppose the heat reservoir containers remains fixed in volume, then their temperature changes slightly when heat energy is added or removed. Then the isothermal steps are not perfectly held at fixed temperatures.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how the motion of the piston would differ if we enforced that the total change in entropy across the gas and the reservoir is fixed at 0. Then we have something similar to the isentropic phases 2 and 4, except we are considering the heat reservoir and the gas together. Let $n$ be the molar mass of the gas, and $N$ be the molar mass of the heat reservoir (a much larger gas). We assume that the gas and reservoir are always at the same temperature. The reservoir occupies some constant large fixed volume, whereas the gas changes in volume depending on the positions $x_i$ and $x_f$ of the piston. Then given no change in entropy, we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
0 = (n+N)C_V \ln\par{\frac{T_f}{T_i}} + nR \ln\par{\frac{x_f}{x_i}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}\ln T_f&amp;amp;=\ln T_i + \frac{n}{n+N}\frac{R}{C_V} \ln\par{\frac{x_i}{x_f}}\\&lt;br&gt;
T_f&amp;amp;=T_i\par{\frac{x_i}{x_f}}^{\g}\,,\end{aligned}$$&lt;/p&gt;
&lt;p&gt;where $$\g = \frac{n}{n+N}\frac{R}{C_V}\,.$$&lt;/p&gt;
&lt;p&gt;The work done on the gas by the piston equals the negative change in internal energy of the combined gas and reservoir:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
W &amp;amp;= -\D E_\text{int} = (n+N)C_V(T_i-T_f) \\&lt;br&gt;
&amp;amp;= (n+N)C_VT_i\par{1-\par{\frac{x_i}{x_f}}^{\g}} \,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Taking the size of the reservoir to infinity, $N\to\infty$, we approach an isothermal process, since $\g \to 0$ and so $T_f \to T_i$. Then taking the limit of work should give us the formula for work from phase 1:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lim_{N\to\infty} (n+N)C_VT_i\par{1-\par{\frac{x_i}{x_f}}^{\frac{n}{n+N}\frac{R}{C_V}}} = nRT_i \ln \left(\frac{x_f}{x_i}\right)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which indeed agrees with phase 1 work $W\up{1}$, where $T_i = T_H$.&lt;/p&gt;
&lt;p&gt;Approaching isothermality as a limit allows us to consider the entropy of the reservoir for large but still finite $N$. For the gas,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\D S_\text{gas} &amp;amp;= \lim_{N\to\infty} nC_V \ln\par{\frac{T_f}{T_i}} + nR \ln\par{\frac{x_f}{x_i}} \\&lt;br&gt;
&amp;amp;= \lim_{N\to\infty} nC_V \ln\par{\frac{T_i\par{\frac{x_i}{x_f}}^{\g}}{T_i}} + nR \ln\par{\frac{x_f}{x_i}} \\&lt;br&gt;
&amp;amp;= \lim_{N\to\infty} -\frac{n^2}{n+N}R \ln\par{\frac{x_f}{x_i}} + nR \ln\par{\frac{x_f}{x_i}} \\&lt;br&gt;
&amp;amp;= \lim_{N\to\infty} \frac{n(n+N)-n^2}{n+N}R \ln\par{\frac{x_f}{x_i}} \\&lt;br&gt;
&amp;amp;= \lim_{N\to\infty} \frac{nN}{n+N}R \ln\par{\frac{x_f}{x_i}} \\&lt;br&gt;
&amp;amp;= nR \ln\par{\frac{x_f}{x_i}}\,,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;which agrees with the phase 1 change in entropy $\D S\up{1}$.&lt;/p&gt;
&lt;p&gt;We setup the problem with the constraint that total entropy is conserved, and so it must be the case that $\D S_\text{reservoir} = -\D S_\text{gas}$. But as a sanity check, let&amp;rsquo;s compute $\D S_\text{reservoir}$ as $N\to\infty$,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
\D S_\text{reservoir} &amp;amp;= \lim_{N\to\infty} NC_V \ln\par{\frac{T_f}{T_i}} \\&lt;br&gt;
&amp;amp;= \lim_{N\to\infty} NC_V \ln\par{\frac{T_i\par{\frac{x_i}{x_f}}^{\g}}{T_i}} \\&lt;br&gt;
&amp;amp;= \lim_{N\to\infty} -\frac{nN}{n+N}R\ln\par{\frac{x_f}{x_i}} \\&lt;br&gt;
&amp;amp;= -nR\ln\par{\frac{x_f}{x_i}}\,,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;which equals $-\D S\up{1}$.&lt;/p&gt;
&lt;p&gt;This is why the isothermal phases are reversible even though the entropy of the gas changes. An increase in entropy alone does not make a process irreversible. So long as some other part of the universe decreases in entropy by the same amount, and that part can still interact with the process of interest, then the process of interest may still be reversible. Canonical irreversible processes like free expansion, heat exchange, and gas mixing, all involve an increase in entropy without any corresponding decrease in entropy somewhere else. We encounter irreversibility because unlike energy, entropy is not a conserved quantity, but rather a monotonic quantity. That is to say, the total entropy of an isolated system cannot decrease, but it can increase.&lt;/p&gt;
&lt;h1 id=&#34;efficiency&#34;&gt;Efficiency&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Thermal_efficiency#Heat_engines&#34;target=&#34;_blank&#34;&gt;efficiency of a thermodynamic process&lt;/a&gt; is defined as&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\eta = \frac{W_\text{out}}{E_\text{in}}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $W_\text{out} \geq 0$ is the useful work that we get out of the process (i.e. energy put towards making a desired change in the world), and $E_\text{in} \geq 0$ is the energy removed from some source. In practice, the denominator is energy released from some energy storage, e.g. chemical potential stored in oil, or nuclear potential stored in uranium. So the efficiency of a power plant is the amount of work extracted (to rotate a giant magnet producing an electric current) per energy released from the fuel.&lt;/p&gt;
&lt;p&gt;If we&amp;rsquo;ve are accounted for all energy moving into and out of the process, then by necessity $E_\text{in} = W_\text{out} + E_\text{unused}$, with $E_\text{unused} \geq 0$ implying $0\leq \eta \leq 1$.  The difference $E_\text{unused} = E_\text{in} - W_\text{out}$ is the remaining energy extracted from the source which was not put towards useful work. Usually, this unused energy is released into the environment as ambient heat - an irreversible operation (an instance of heat transfer through a finite temperature difference). In the reversible Carnot cycle, the heat sink (cold reservoir) absorbs all the unused energy as heat. In the reverse Carnot cycle, that energy is extracted from the sink (which becomes the source), and the piston provides additional input energy (equal to $W_\text{out}$).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s calculate the efficiency of the Carnot cycle. $W_\text{out}$ is the net work done by the four phases, and $E_\text{in}$ is the net heat energy extracted from the heat source across the four cycles.&lt;/p&gt;
&lt;p&gt;Since the only contact with the heat source is in phase 1, where $Q_H$ heat energy was extracted, then $E_\text{in} = Q_H = W\up{1} = nRT_H\ln\par{\frac{x_f\up{1}}{x_i\up{1}}} = T_H\D S\up{1}$. Recall from phase 1 that $\D S\up{1}=nR \ln\par{\frac{x_f\up{1}}{x_i\up{1}}}$, which is the amount of entropy gained by the gas during isothermal expansion.&lt;/p&gt;
&lt;p&gt;Likewise, the only contact with the heat sink is in phase 3, where $\abs{Q_C}$ heat energy was added to the sink. Then $E_\text{unused} = \abs{Q_C} = \abs{W\up{3}}=\abs{T_C\D S\up{3}}=T_C\D S\up{1}$ since $\D S\up{1} = -\D S\up{3}$.&lt;/p&gt;
&lt;p&gt;$W_\text{out}$ is the total work done across the four phases, i.e.&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
W_\text{out} &amp;amp;= W\up{1}+W\up{2}+W\up{3}+W\up{4} \\&lt;br&gt;
&amp;amp;=\quad nRT_H\ln\par{\frac{x_f\up{1}}{x_i\up{1}}}+nC_V(T_H-T_C)\\&amp;amp;\quad +\ \ \ nRT_C\ln\par{\frac{x_f\up{3}}{x_i\up{3}}}-nC_V(T_H-T_C) \\&lt;br&gt;
&amp;amp;=nRT_H\ln\par{\frac{x_f\up{1}}{x_i\up{1}}}-nRT_C\ln\par{\frac{x_f\up{1}}{x_i\up{1}}} \\&lt;br&gt;
&amp;amp;= (T_H-T_C)nR\ln\par{\frac{x_f\up{1}}{x_i\up{1}}} \\&lt;br&gt;
&amp;amp;= (T_H-T_C)\D S\up{1} \\&lt;br&gt;
&amp;amp;= E_\text{in} - E_\text{unused}\,,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;where $\frac{x_f\up{3}}{x_i\up{3}} = \par{\frac{x_f\up{1}}{x_i\up{1}}}^{-1}$ was derived in phase 3 from the constraint that $\D S\up{1}=-\D S\up{3}$.&lt;/p&gt;
&lt;p&gt;A good question to investigate is, why does the Carnot cycle produce net work? That is to say, why is $W_\text{out}$ not zero? Since phases 3 and 4 are the reversals of phases 1 and 2, it is surprising to me that there is some asymmetry  between the joint process 1+2 vs 3+4. Clearly if we instead ran the phases in the order 1+2+4+3, the work of phases 1 and 2 would be undone (since 4+3 is the reverse of 1+2). I suspect the answer is that the mass function $m(x)$ is different in the compression and expansion phases, i.e. $\D m$ during expansion is greater than $\D m$ during compression. This is how we end up with less work extracted out of the piston on the way down (compression) than work done to the piston on the way up (expansion), leaving net energy in whatever the piston is acting on, i.e. net useful work done by the heat engine.&lt;/p&gt;
&lt;p&gt;Regarding entropy, we know that the net change in entropy of the gas through the four phases is zero. However, there is a nonzero change in entropy in the environment during the cycle. In the previous section we showed that change in entropy of the reservoirs equals negative change in entropy of the gas (so that net entropy change is zero), i.e. $\D S_\text{hot} = -\D S\up{1}$ in phase 1 and $\D S_\text{cold} = -\D S\up{3} = \D S\up{1} = -\D S_\text{hot}$ in phase 3, where $\D S_\text{hot}$ and $\D S_\text{cold}$ are changes in entropy of the hot and cold reservoirs respectively. Thus, the hot reservoir loses $\D S\up{1}$ entropy units and the cold reservoir gains $\D S\up{1}$ entropy units, so we have a net transfer of $\D S\up{1}$ entropy units from the hot reservoir to the cold reservoir.&lt;/p&gt;
&lt;p&gt;The efficiency of the Carnot cycle is&lt;br&gt;
$$\eta=\frac{W_\text{out}}{E_\text{in}}=\frac{(T_H-T_C)\D S\up{1}}{T_H\D S\up{1}}=1-\frac{T_C}{T_H}\,.$$&lt;br&gt;
Thus the greater the heat difference between source and sink, the more heat energy goes into net work on the piston.&lt;/p&gt;
&lt;p&gt;Carnot&amp;rsquo;s big contribution is to prove that no process which uses a temperature difference between two heat reservoirs to move a piston can be more efficient than the Carnot cycle which does the same for a given $T_H$ and $T_C$. In general, a reversible process which performs some transformation is the most efficient possible process for that transformation. In practice, thermodynamic processes are irreversible due to unavoidable deviations from the ideal. These deviations usually take the form of irreversible heat loss due to friction or imperfect thermal isolation. For a real-world implementation of the Carnot cycle, if there is any energy leaving the combined process not accounted for by work done on the piston or heat moved to the sink, then the process will be less efficient than the ideal since then $E_\text{in} &amp;gt; W_\text{out} + E_\text{out}$, where $E_\text{out} = \abs{Q_C}$ is the recoverable energy dumped into the heat sink. Now in the irreversible case, $E_\text{unused} = E_\text{out} + E_\text{lost}$ where $E_\text{lost}$ is heat energy irreversibly lost to the environment.&lt;/p&gt;
&lt;h2 id=&#34;choosing-the-end-of-phase-1&#34;&gt;Choosing the end of phase 1&lt;/h2&gt;
&lt;p&gt;I mentioned in &lt;a href=&#34;#1-isothermal-expansion&#34;&gt;#1 Isothermal expansion&lt;/a&gt; that the operator is free to choose the stopping point of phase 1: $x_f\up{1} &amp;gt; x_i\up{1}$. We saw above that $W_\text{out} = (T_H-T_C)nR\ln\par{\frac{x_f\up{1}}{x_i\up{1}}}$, and so the net work done on the piston is proportional to the log-ratio of initial to final container length, holding all else fixed. However, we also saw that the efficiency is $\eta=1-\frac{T_C}{T_H}$, which does not depend on $x_f\up{1}$, but only the heat ratio between the hot and cold reservoir that the Carnot cycle operates between. That implies that as $W_\text{out}$ increases, so does $E_\text{in}$ and $E_\text{out}$ to maintain the constant ratio $\eta = W_\text{out}/E_\text{in}$.&lt;/p&gt;
&lt;p&gt;It would seem that the choice of $x_f\up{1}$ is then irrelevant, since whatever the value of $W_\text{out}$ is, the cycle can be repeated arbitrarily many times to extract the desired amount of work. Note that the bigger $x_f\up{1}$ is, the more time phase 1 takes.&lt;/p&gt;
&lt;p&gt;A good question to investigate is whether there is any reason to prefer a shorter or longer phase 1. I suspect that choice might affect (1) the power of the Carnot cycle (i.e. work per time) and (2) the real-world inefficiency of any practical implementation of the Carnot cycle (a very long or very short phase 1 might suffer from too much friction or heat loss).&lt;/p&gt;
&lt;h1 id=&#34;irreversible-gas-transformations&#34;&gt;Irreversible Gas Transformations&lt;/h1&gt;
&lt;p&gt;In the discussion of the Carnot cycle above, we&amp;rsquo;ve defined two kinds of reversible transformations on a gas in a container: isothermal expansion/compression and isentropic expansion/compression. There are more such reversible processes on a gas, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Isobaric_process&#34;target=&#34;_blank&#34;&gt;isobaric&lt;/a&gt; (constant pressure) and &lt;a href=&#34;https://en.wikipedia.org/wiki/Isochoric_process&#34;target=&#34;_blank&#34;&gt;isochoric&lt;/a&gt; (constant volume). See &lt;a href=&#34;https://en.wikipedia.org/wiki/Thermodynamic_cycle#A_list_of_thermodynamic_processes&#34;&gt;https://en.wikipedia.org/wiki/Thermodynamic_cycle#A_list_of_thermodynamic_processes&lt;/a&gt;. There are many other sorts of ideal reversible cycles, such as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Rankine_cycle&#34;title=&#34;Rankine cycle&#34;target=&#34;_blank&#34;&gt;Rankine&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Stirling_cycle&#34;title=&#34;Stirling cycle&#34;target=&#34;_blank&#34;&gt;Stirling&lt;/a&gt; cycles. See &lt;a href=&#34;https://en.wikipedia.org/wiki/Thermodynamic_cycle#Well-known_thermodynamic_cycles&#34;&gt;https://en.wikipedia.org/wiki/Thermodynamic_cycle#Well-known_thermodynamic_cycles&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, we haven&amp;rsquo;t discussed what sorts of gas transformations are irreversible.&lt;/p&gt;
&lt;p&gt;Standard irreversible gas transformations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Joule_expansion&#34;target=&#34;_blank&#34;&gt;Joule expansion&lt;/a&gt; (free expansion): The gas&amp;rsquo;s container suddenly increases in volume (e.g. a wall of the container is removed opening up the gas to another closed volume containing a vacuum). When the gas returns to equilibrium in the larger volume, we have an increase in entropy, $\D S = nR \ln\par{\frac{V_f}{V_i}}$, but without any reduction in entropy in the environment to compensate. There is no change in temperature of the gas because the gas is not pushing any movable wall, hence the expansion is &amp;ldquo;free&amp;rdquo; from resistance. We can think of this as slippage of the ideal counterpart: isentropic expansion.&lt;/li&gt;
&lt;li&gt;Heat transfer over a temperature difference (&lt;a href=&#34;https://web.mit.edu/16.unified/www/FALL/thermodynamics/notes/node34.html&#34;target=&#34;_blank&#34;&gt;ref 1&lt;/a&gt;, &lt;a href=&#34;https://www.grc.nasa.gov/www/k-12/airplane/heat.html&#34;target=&#34;_blank&#34;&gt;ref 2&lt;/a&gt;): The gas&amp;rsquo;s container is put in thermal contact with another gas in a container at a different temperature. Both gas temperatures will converge to the same average temperature, but their combined entropy will go up. This is also slippage of an ideal counterpart: isentropic expansion/compression followed by an isothermal expansion/compression, so that the gas&amp;rsquo;s temperature is brought up/down to the temperature of the other gas before they are put into contact, and then the volume of the container is adjusted back while in contact with the other gas.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.mit.edu/16.unified/www/FALL/thermodynamics/notes/node54.html&#34;target=&#34;_blank&#34;&gt;Mixing two gasses&lt;/a&gt;: Two distinguishable gasses (different kinds of gas particles) initially in separate containers are allowed to mix in the combined volume by opening a wall between the two containers. This is a different sort of scenario than what we encountered in the Carnot cycle, but is a very interesting example of change in entropy. Neither the temperature or volume of the gasses changes, but their combined entropy goes up.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I mentioned earlier that expansion/compression of a thermally isolated gas need not be isenstropic (phases 2 and 4). In general, if the piston moves faster (in either direction) than the isentropic ideal, $nR \ln\par{\frac{V_f}{V_i}} = -nC_V\ln\par{\frac{T_f}{T_i}}$, then we have an irreversible transformation. When the container is expanding, $nR \ln\par{\frac{V_f}{V_i}}$ is positive. When the container is compressing, $nC_V\ln\par{\frac{T_f}{T_i}}$ is positive. If the container is expanded too vigorously, then $\abs{nR \ln\par{\frac{V_f}{V_i}}} &amp;gt; \abs{nC_V\ln\par{\frac{T_f}{T_i}}}$, and so $\D S &amp;gt; 0$. If the container is compressed too vigorously, $\abs{nC_V\ln\par{\frac{T_f}{T_i}}} &amp;gt; \abs{nR \ln\par{\frac{V_f}{V_i}}}$, and so also $\D S &amp;gt; 0$. Either way, the isentropic ideal, $\D S = 0$, is a lower bound on entropy change (I assume there is no way to reduce the entropy of an isolated gas).&lt;/p&gt;
&lt;h1 id=&#34;relationship-between-energy-and-entropy&#34;&gt;Relationship between energy and entropy?&lt;/h1&gt;
&lt;p&gt;I don&amp;rsquo;t at the time of writing this understand the relationship between energy and entropy. I can surmise that the nature of that relationship is deep and fundamental, going well beyond the Carnot cycle. But the Carnot cycle let&amp;rsquo;s us pose a concrete instance of the question. However, I do caution drawing conclusions based only on the Carnot cycle, as it may not be representative of the general relationship between energy and entropy. As for myself, I see the Carnot cycle as a useful intuition pump for basic thermodynamic reasoning, but it needs to be complemented with other intuition pumps, ideally from &lt;a href=&#34;https://en.wikipedia.org/wiki/Non-equilibrium_thermodynamics&#34;target=&#34;_blank&#34;&gt;non-equilibrium thermodynamics&lt;/a&gt; where many special case aspects of the Carnot cycle fall away.&lt;/p&gt;
&lt;p&gt;One formulation of the question: Why is there not an ideal process that achieves maximum efficiency? That is to say, why does there need to be any unused energy, i.e. why $E_\text{unused} &amp;gt; 0$? Or posed another way, why cant the efficiency be 1.0 even in principle? Perhaps this question could be rephrased as, why does the efficiency of the Carnot cycle depend on the temperatures of the heat source and sink at all?&lt;/p&gt;
&lt;p&gt;Another formulation: Why is there a necessary cost in the form of waste heat to convert heat into work? That is despite change in entropy across the entire system being zero. Conversely, why is there a necessary cost in the form of input work to move heat from cold to hot, despite change in entropy across the entire system being zero? Does moving energy around generally require additional energy cost? Or is it that moving energy from many degrees of freedom to fewer degrees of freedom requires additional energy cost?&lt;/p&gt;
&lt;p&gt;Another line of questioning: Does decreasing the entropy of one system require some energy cost to be paid? Presumably, decreasing the entropy of one system requires the entropy of another system to increase so that total entropy does not decrease (&lt;a href=&#34;https://en.wikipedia.org/wiki/Second_law_of_thermodynamics&#34;target=&#34;_blank&#34;&gt;2nd law&lt;/a&gt;), but moving entropy around like that also seems to require paying an energy cost. Why is that? What determines what energy cost needs to be paid for some transfer of entropy?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Reversibility Problem</title>
      <link>https://danabo.github.io/blog/posts/the-reversibility-problem/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/the-reversibility-problem/</guid>
      <description>&lt;p&gt;This is my exploration into formalizing the reversibility problem, i.e. the question &amp;ldquo;Which processes are reversible?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;My long term goals are to,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;formally define what it means for any process to be reversible, regardless of equilibrium considerations;&lt;/li&gt;
&lt;li&gt;clarify the connection between information and reversibility (and by extension the connection between information and entropy);&lt;/li&gt;
&lt;li&gt;clarify (make well defined) the meaning of statements like &amp;ldquo;breaking a glass is irreversible because the entropy of the broken glass is higher than the entropy of the unbroken glass,&amp;rdquo; and &amp;ldquo;the entropy of the universe is monotonically increasing.&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\ms}{\mathscr}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\es}{\emptyset}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\th}{\theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\L}{\Lambda}&lt;br&gt;
\newcommand{\G}{\Gamma}&lt;br&gt;
\newcommand{\g}{\gamma}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\N}{\mb{N}}&lt;br&gt;
\newcommand{\I}{\mb{I}}&lt;br&gt;
\newcommand{\H}{\mc{H}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\Z}{\mb{Z}}&lt;br&gt;
\newcommand{\s}{\sigma}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\tup}{\par}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[1]{_{\mid #1}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\Dt}{{\Delta t}}&lt;br&gt;
\newcommand{\tr}{\rightarrowtail}&lt;br&gt;
\newcommand{\qed}{\ \ \blacksquare}&lt;br&gt;
\newcommand{\c}{\overline}&lt;br&gt;
\newcommand{\dg}{\dagger}&lt;br&gt;
\newcommand{\dd}{\mathrm{d}}&lt;br&gt;
\newcommand{\pd}{\partial}&lt;br&gt;
\newcommand{\Ue}{U_{\text{ext}}}&lt;br&gt;
\newcommand{\Ui}{U_{\text{int}}}&lt;br&gt;
\newcommand{\Us}{U_{\text{sys}}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;With goal #1 I am interested in being able to ask (make well posed) thermodynamic-type questions of non-equlibrium systems. Even if those questions don&amp;rsquo;t have tractable answers, does being able to precisely formulate those questions (as well as what an answer looks like) open up new directions on course-grained (effective theory) non-equlilibrium thermodynamics? Does doing this allow us to make any progress towards the thermodynamics of living systems (i.e. open systems far from equilibrium) ? In the philosophical direction, does formalizing this problem in generality allow for the laws of thermodynamics (or some version of them) to be derived from the laws of classical mechanics?&lt;/p&gt;
&lt;p&gt;With goals #2 and #3, I am interested in being able to answer philosophical (specifically interpretational questions) about physics and thermodynamics - specifically the role information plays, whether thermodynamics (and statistical mechanics in general) is anthropocentric (i.e. dependent on the beliefs/models of an agent), and whether the phenomenon of irreversibility and its quantitative property, entropy, generalize well beyond thermodynamics and touch on the fundamental nature of reality, ala the arrow of time and limits on our ability (as intelligent systems) to control the environment around us. Finally, is there a precise argument to be made as to how irreversible processes can exist in classical mechanics (which has &lt;a href=&#34;https://en.wikipedia.org/wiki/Time_reversibility&#34;target=&#34;_blank&#34;&gt;time-reversible dynamics&lt;/a&gt;)?&lt;/p&gt;
&lt;h1 id=&#34;reversibility-and-thermodynamics&#34;&gt;Reversibility and Thermodynamics&lt;/h1&gt;
&lt;p&gt;The importance of reversibility in thermodynamics is due to its relationship with (energy) efficiency. The energy efficiency of a process that converts an energy source into &amp;ldquo;useful&amp;rdquo; work (where &amp;ldquo;useful&amp;rdquo; is relative to a goal-driven entity) is the ratio of useful work extracted to energy consumed (both measured in Joules).&lt;/p&gt;
&lt;p&gt;The canonical problem in thermodynamics is to determine the efficiency of a process that uses energy from a heat source to move a piston against some resistance (e.g. pushing mass against gravity or moving the wheels of a locomotive). The heat source could, for instance, come from burning fuel (converting chemical potential into heat energy). The more heat energy that goes into useful work (e.g. the piston), the higher the efficiency of the engine. The theoretical limit on efficiency for any given transformation is the efficiency of a reversible process that achieves it.&lt;/p&gt;
&lt;p&gt;(In general reversible processes are not perfectly efficient, i.e. not all all input energy is converted to useful work. E.g. see &lt;a href=&#34;https://danabo.github.io/blog/posts/carnot-cycle/&#34;&gt;Carnot Cycle&lt;/a&gt;. However, in a reversible process, all the wasted energy can be recovered if the transformation is reversed.),&lt;/p&gt;
&lt;p&gt;In classical thermodynamics, entropy is a quantity (property of a system) defined out of the need to determine which processes are reversible. The role of entropy in thermodynamics is this: any process that results in a net zero change in entropy is reversible. Positive changes in entropy during a process indicate irreversibility (and negative changes in entropy require positive changes in entropy elsewhere).&lt;/p&gt;
&lt;p&gt;Statistical thermodynamics sets out to explain what entropy is in terms of the low-level rules (fine-grained representation) of classical mechanics, and to derive all the laws of thermodynamics from classical mechanics. However, as a subfield of statistical mechanics, it has another goal: derive simplified representations of high-dimensional (many degrees of freedom) complicated systems s.t. predictions of behavior can be made solely based on that simplified representation. This is called &lt;a href=&#34;https://en.wikipedia.org/wiki/Coarse-grained_modeling&#34;target=&#34;_blank&#34;&gt;course-graining&lt;/a&gt; (course-grained representations could be called &lt;a href=&#34;https://en.wikipedia.org/wiki/Effective_theory&#34;target=&#34;_blank&#34;&gt;effective theories&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;For example, rather than modeling a gas with millions of particles, it is much easier (and tractable) to model an ideal gas described by just a handful of quantities: temperature, volume, pressure, internal energy, and entropy. The course-grained theory needs to be able to predict the time-evolution of these quantities without referring to the fine-grained theory (so that we avoid modeling millions of particles). However, this course-grained representation of the gas will only make accurate predictions within a certain regime. It fails to model gasses outside of equilibrium where these course-grained quantities cease to be well-defined.&lt;/p&gt;
&lt;p&gt;I suspect that the philosophical problems I mentioned above are muddled by conflation between course-grained models as instrumental representations (they are useful approximations) and course-grained models as metaphysical assertions about what things really are. For instance, in classical thermodynamics the entropy of a gas is only well-defined when the gas is at equilibrium, but there is a strong impulse to want to generalize the idea of entropy as a universal and fundamental property of things in the universe - things that happen cannot be undone because the entropy of those things has increased. And more striking, while energy in the universe may be conserved, it becomes less useful over time because the entropy of the universe is increasing. Is entropy a well-defined concept in these use-cases?&lt;/p&gt;
&lt;p&gt;One avenue towards seeking a general understanding of entropy is to pose the reversibility problem in general - i.e. for arbitrary processes. Although equilibrium or other simplifying assumptions are not necessary to pose the problem, but determining if a process is reversible will likely require course-grained representations to make reasoning about it tractable. It seems to me that a fine-grained definition of reversibility (and entropy if it exists) is useful for clarifying the meaning of things and what we are doing (philosophical considerations), and course-grained representations are useful for making calculations and predictions tractable.&lt;/p&gt;
&lt;h2 id=&#34;towards-defining-reversibility&#34;&gt;Towards Defining Reversibility&lt;/h2&gt;
&lt;p&gt;What do we mean when we say some process (done to a system) is reversible or irreversible? I posit the following answer: a reversible (forward) process has a corresponding reverse process s.t. the combined forward+reverse process can be repeated forever.&lt;/p&gt;
&lt;p&gt;That answer by itself does not necessarily imply that the system being transformed is actually returned to its initial state (start of the forward process) at the end of the reverse process. For example, consider a chaotic closed system like the &lt;a href=&#34;https://en.wikipedia.org/wiki/Double_pendulum&#34;target=&#34;_blank&#34;&gt;double pendulum&lt;/a&gt;. The pendula will move through space in a &lt;a href=&#34;https://physics.stackexchange.com/a/363497/55723&#34;target=&#34;_blank&#34;&gt;non-repeating way&lt;/a&gt; forever. Since the system is closed, there is no exchange of energy with the outside, and so it can in principle &amp;ldquo;run&amp;rdquo; forever.&lt;/p&gt;
&lt;p&gt;Clearly we need a second condition on reversibility. This condition will be motivated by our interest in reversibility in the first place: optimal efficiency of transformations converting stored energy into useful work. The word &amp;ldquo;useful&amp;rdquo; indicates an agent-relative goal (i.e. anthropocentrism). This condition second condition is then that the forward process put the system in a desired state (where useful work is extracted), and that the reverse process undo this useful work (returning the energy to its initial source), thereby guaranteeing optimal theoretical efficiency of the forward process.&lt;/p&gt;
&lt;p&gt;This second leads to two implications about the system:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;That the system is interacting with the outside world in some way, i.e. it does work on the outside world. This means we must also consider and define the immediate boundary between system and outside, which we call the &lt;em&gt;environment&lt;/em&gt; of the system.&lt;/li&gt;
&lt;li&gt;The useful work done during the forward process would not be undone by itself, so the reverse process requires some sort of agent-driven change in the system&amp;rsquo;s environment that induces the reverse process to happen. In practice, the forward and reverse processes are both driven by an agent via the environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The inclusion of an environment (which is itself an open system, being the boundary between system and universe) makes defining reversibility in terms of a fine-grained representation (i.e. classical mechanics) a much greater challenge, simply because formalizing environments in the same fine-grained representation is somewhere between challenging and intractable.&lt;/p&gt;
&lt;p&gt;Furthermore, the reversibility of the system now also depends on the repeatibility of the environment. By that I mean, with every cycle of the forward+reverse process the environment needs to behave the same w.r.t. its interaction with the system. Of course, the entire universe beyond the system&amp;rsquo;s environment need not repeat, so this presents a problem of what it means for the system&amp;rsquo;s environment to be reversed along with the system.&lt;/p&gt;
&lt;p&gt;I will expand on these difficulties arising from formalizing the environment further later in the post. In the next section, I will naively formulate an environment and show where this breaks down.&lt;/p&gt;
&lt;h1 id=&#34;naive-formulation&#34;&gt;Naive Formulation&lt;/h1&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;h3 id=&#34;canonical-coordinates&#34;&gt;Canonical Coordinates&lt;/h3&gt;
&lt;p&gt;In everything that follows I&amp;rsquo;m dealing only with classical mechanics. My goal is to define a system (in generality), its environment, and what it means for a process the system undergoes to be reversible - all in terms of the fine-grained representation we call classical mechanics (specifically Hamiltonian or Lagrangian mechanics). In general, we can describe a system with &lt;a href=&#34;https://en.wikipedia.org/wiki/Canonical_coordinates&#34;target=&#34;_blank&#34;&gt;cannonical coordinates&lt;/a&gt; (or &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalized_coordinates&#34;target=&#34;_blank&#34;&gt;generalized coordinates&lt;/a&gt;) and a &lt;a href=&#34;https://en.wikipedia.org/wiki/Hamiltonian_mechanics#Overview&#34;target=&#34;_blank&#34;&gt;Hamiltonian&lt;/a&gt; (or &lt;a href=&#34;https://en.wikipedia.org/wiki/Lagrangian_mechanics#The_Lagrangian&#34;target=&#34;_blank&#34;&gt;Lagrangian&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Let the state of some system be described by a real-valued tuple $\o  = (\o_1,\o_2,\dots,\o_{2n}) = (q_1, q_n, p_1, p_n) \in \O$, where $\O$ is the system&amp;rsquo;s state space (set of all valid states), and the coordinates $q_i$ are degrees of freedom of that system (e.g. positions or orientations) with $p_i$ being the corresponding momenta of those degrees of freedom. In order for this system to be properly described by classical mechanics, each degree of freedom needs to be &lt;em&gt;intertial&lt;/em&gt;, meaning that they have momenta which change in the presence of a force.&lt;/p&gt;
&lt;p&gt;(Typically $p_i = m_i\dot{q}_i$ where $m_i$ is the intertial mass of the $i$-th degree of freedom, and $\dot{q}_i$ is the time-derivative of $q_i$. But technically, the relationship between $q_i$ and $p_i$ is &lt;a href=&#34;https://en.wikipedia.org/wiki/Hamiltonian_mechanics#Phase_space_coordinates_%28p,q%29_and_Hamiltonian_H&#34;target=&#34;_blank&#34;&gt;determined by the given Hamiltonian&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;In my notation, I am supposing that $q_i$ may itself be a scalar, 2-tuple or 3-tuple depending on whether it represents a spatial coordinate in 1D, 2D or 3D space, or some other sort of degree of freedom (e.g. angle of orientation or distance between two bodies).&lt;/p&gt;
&lt;h3 id=&#34;the-hamiltonian&#34;&gt;The Hamiltonian&lt;/h3&gt;
&lt;p&gt;The dynamics of the system (its possible trajectories through time) are fully determined by a collection of kinetic energy and potential energy functions of the system&amp;rsquo;s state (and time). When these terms are summed, we get the system&amp;rsquo;s total energy (i.e. the Hamiltonian).&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;internal&lt;/em&gt; dynamics of the system (how the components of the system interact with each other irrespective of the environment) are specified with an internal time-independent energy term $\Ui:\O\to\R$, which is a function mapping  states of the system to potential energies. The qualifier &amp;ldquo;time-independent&amp;rdquo; indicates that $\Ui$ is not a function of time, meaning that the internal dynamics of the system satisfy &lt;a href=&#34;https://en.wikipedia.org/wiki/Time_translation_symmetry&#34;target=&#34;_blank&#34;&gt;time-translational invariance&lt;/a&gt; (the system has no notion of any absolute time, i.e. the system will do what it does regardless of the age of the universe).&lt;/p&gt;
&lt;p&gt;We also must specify the time-independent kinetic energy term $T:\O\to\R_{\geq0}$ and &lt;em&gt;external&lt;/em&gt; time-dependent potential energy term $\Ue:\O\times\R\to\R$, the latter being a function mapping state and time to potential energy. The qualifier &amp;ldquo;external&amp;rdquo; indicates that $\Ue$ tells us what we need to know about the system&amp;rsquo;s interaction with the outside world. That is to say, $\Ue$ fully represents all the influence the environment has on the system. For example, systems are often confined to a region of space via an extenral potential well, e.g. a gas in a box, or a system acted on by Earth&amp;rsquo;s gravity (a very big potential well).&lt;/p&gt;
&lt;p&gt;All three terms fully describe the dynamics of any process the system may undergo, where the total energy  $\H=T+\Ui+\Ue$ is the Hamiltonian of the given process. ($\mc{L}=T-\Ui-\Ue$ is the Lagrangian.) Given $\H$ (or $\mc{L}$), every possible trajectory the system can take through state space is fully determined.&lt;/p&gt;
&lt;p&gt;By default $\H:\O\times\R\to\R$ is a function of state and time, making it time-dependent. The Hamiltonian&amp;rsquo;s time-dependence is due solely to the environment. For time-independent $\Ue$ the Hamiltonian $\H$ is also time-independent (and conversely if $\H$ is time-independent then so is $\Ue$).&lt;/p&gt;
&lt;h3 id=&#34;constraints-on-the-external-potential&#34;&gt;Constraints on the external potential&lt;/h3&gt;
&lt;p&gt;To specify that $\Ue$ is an external potential is to say that it doesn&amp;rsquo;t at all determine the internal interactions of the system. Formally that means $\Ue$ does not contain &lt;em&gt;interaction energy&lt;/em&gt; terms.&lt;/p&gt;
&lt;p&gt;Interaction energy is potential energy that depend on the states of two or more degrees of freedom. For example, a potential function $U\up{i,j}(q_i,q_j)$ depending only on two DoFs $i$ and $j$ (and symmetric in its arguments) is an interaction potential (interaction potentials may also depend on momenta). One possible way to construct the internal potential $\Ui$ is to make it a sum of pair-wise interaction potentials: $\Ui(q_1,\dots,q_n,p_1,\dots,p_n)=\sum_{i&amp;lt;j} U\up{i,j}(q_i,q_j,p_i,p_j)$.&lt;/p&gt;
&lt;p&gt;We require that the external potential $\Ue$ be free of all interaction potentials. This can be satisfied by requiring that $\Ue$ be a sum of a singular potential function $U\up{i}(q_i,p_i)$ for each DoF. That is to say, we require that $\Ue = \sum_{i=1}^n U\up{i}(q_i,p_i)$.&lt;/p&gt;
&lt;p&gt;(Note that a potential function of a single spatial coordinate, $U(q)$ (either 1D, 2D or 3D), is called a &lt;a href=&#34;https://en.wikipedia.org/wiki/Field_%28physics%29&#34;target=&#34;_blank&#34;&gt;field&lt;/a&gt;. When $U(q)$ is time-independent, it is a constant field.)&lt;/p&gt;
&lt;p&gt;Furthermore, in many situations we want to specify that some degrees of freedom are &lt;em&gt;indistinguishable&lt;/em&gt;, meaning that swapping them does not change the dynamics of the system (and the outside universe cannot distinguish between them). For example, in a gas containing N identical particles, the external potential cannot affect each particle differently.&lt;/p&gt;
&lt;p&gt;In general, if DoF $i$ and $j$ are specified as indistinguishable, then we require that their singular potential functions are equal, i.e. $U\up{i}(q,p)=U\up{j}(q,p)$ for all $q,p$. In general, the indistingiushability of DoFs in a system can be fully specified by a set of permutations of coordinate indices for which the dynamics of the system are invariant.&lt;/p&gt;
&lt;h3 id=&#34;trajectories-and-propagators&#34;&gt;Trajectories and propagators&lt;/h3&gt;
&lt;p&gt;It is more convenient to represent the trajectories of a process explicitly. A &lt;em&gt;trajectory&lt;/em&gt; is a function $\s:\R\to\O$ from time to state.&lt;/p&gt;
&lt;p&gt;The relationship between the possible trajectories of a process and the provided Hamiltonian is not straightforward. For this reason, it is easier to work with &lt;em&gt;propagators&lt;/em&gt;. A propagator $\t_t:\O\to\O$ is a function mapping state to state - specifically, taking a state at time $0$ and outputting the state of the system at time $t$.&lt;/p&gt;
&lt;p&gt;The possible trajectories of the system are fully determined by the family of propagators $\set{\t_t\mid t\in\R}$ - one for every time $t$. The time-dependent Hamiltonian $\H$ uniquely determines the family of propagators.&lt;/p&gt;
&lt;p&gt;Note that this is a family of time-dependent propagators since they give time-evolution w.r.t. to the absolute time $t=0$. We can derive the propagator mapping between any initial time $t_i$ and final time $t_f$ in terms of propagators relative to time $0$, specifically $\t_{t_i\to t_f}=\t_{t_f}\circ\t_{t_i}^{-1}$.&lt;/p&gt;
&lt;p&gt;If $\H$ is time-independent, then we induce a family of time-independent propagators $\set{\t_\Dt\mid \Dt\in\R}$ which do not depend on the absolute time of the given state. In this case $\t_{t_i\to t_f}=\t_{t_f-t_i}$.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;valid trajectory&lt;/em&gt; $\s:\R\to\O$ is consistent will all propagators, i.e. satisfies $\s(t_2)=\t_{t_1\to t_2}(\s(t_1))$ for all $t_1,t_2 \in \R$.&lt;/p&gt;
&lt;p&gt;When the system is isolated (i.e. total energy is constant), the system&amp;rsquo;s trajectory is uniquely determined by specifying its state at some time. That is to say, if the system is in state $\o_t$ at time $t$, there exists exactly one trajectory $\s$ s.t. $\s(t)=\o_t$. Then at time $t$, there is a unique trajectory for every state $\o_t \in\O$. An equivalent statement is that all of the valid trajectories of the system are non-intersecting, i.e. $\s_1(t) \neq \s_2(t)$ for all valid trajectories $\s_1,\s_2$ and for all times $t\in\R$. This will be true so long as every propagator $\t_t$ is a bijection.&lt;/p&gt;
&lt;h3 id=&#34;example-orbiting-bodies&#34;&gt;Example: Orbiting bodies&lt;/h3&gt;
&lt;p&gt;To make the terms $T,\Ui$ and $\Ue$ more concrete, let&amp;rsquo;s consider an example.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s model a system of $n$ small bodies in orbit around a star by supposing the star is fixed and generates a static &lt;a href=&#34;https://en.wikipedia.org/wiki/Newton%27s_law_of_universal_gravitation&#34;target=&#34;_blank&#34;&gt;gravity potential&lt;/a&gt;. The small bodies produce gravitational attraction between themselves (also via gravitational potentials).&lt;/p&gt;
&lt;p&gt;We have a kinetic energy term $T(p_1,\dots,p_n)=\sum_{i=1}^n\frac{1}{2m_i}p_i^2$ where $m_i$ is the mass of the $i$-th body, an internal potential term  $\Ui(q_1,\dots,q_n)=\sum_{i\neq j} G\frac{m_im_j}{\abs{q_i-q_j}^2}$, and a time-independent external potential term $\Ue(q_1,\dots,q_n)=\sum_{i=1}^n G\frac{Mm_i}{\abs{q_i}^2}$, where the star has mass $M \gg m_i$ and is positioned at the origin (and $G$ is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gravitational_constant&#34;target=&#34;_blank&#34;&gt;gravitational constant&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We may instead make the star move along a fixed path $x(t)$, giving us a time-dependent external potential $\Ue(q_1,\dots,q_n;\ t)=\sum_{i=1}^n G\frac{Mm_i}{\abs{x(t)-q_i}^2}$.&lt;/p&gt;
&lt;h2 id=&#34;reversibility---naive-definition&#34;&gt;Reversibility - Naive Definition&lt;/h2&gt;
&lt;p&gt;Now we are ready to define reversibility. Suppose we are given the specification of a process, operating from time $t=\th$ to $t=0$ (with $\th &amp;lt; 0$), with a time-dependent Hamiltonian $\H=V+\Ui+\Ue$, which induces the family of time-dependent propagators $\set{\t_t\mid t\in\R}$. That is to say, $\Ue$ is defined on the time interval $[\th,0]$.&lt;/p&gt;
&lt;p&gt;When we talk about reversing a process on a large system like a gas, we don&amp;rsquo;t actually care about the system retracing in reverse the exact same trajectory that it took. We also don&amp;rsquo;t care about returning the gas particles to their exact initial positions. Remember, our interest in reversibility is that it lets us determine the theoretically optimal efficiency of some transformation of energy from a source into useful work. Whichever starting states and trajectories allow the system to do and reverse that work forever are all equally good to us. This brings us to the concept of a &lt;em&gt;state region&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A state region is a subset of the entire state space $\O$ of the system. State regions represent information about state. Specifically, a state region $\L\subseteq\O$ represents the knowledge that the state of the system $\o$ is in $\L$ (and not in the complement $\O-\L$). For now, just think of state regions as encoding what aspects of the system&amp;rsquo;s state we care about, motivated by the work we want the system to perform. (See &lt;a href=&#34;#the-interpretation-of-state-regions&#34;&gt;#The Interpretation of State Regions&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;&amp;ldquo;State region&amp;rdquo; is essentially a synonym for &amp;ldquo;&lt;a href=&#34;https://en.wikipedia.org/wiki/Microstate_%28statistical_mechanics%29&#34;target=&#34;_blank&#34;&gt;macrostate&lt;/a&gt;&amp;rdquo; from thermodynamics. For example, for a gas at some temperature and volume, $\L$ would be the set of all gas states in equilibrium at that temperature and volume. Or in the case of the Szilard engine, $\L$ is the set of all left-side (or right-side) positions of the container. This is an example of a state region representing a single bit of information about state.&lt;/p&gt;
&lt;p&gt;So in addition to providing the Hamiltonian $\H=V+\Ui+\Ue$, with $\Ue$ defined on the time-interval $[\th,0]$, we also suppose an initial state region $\L_{\th}$ is provided, i.e. $\L_{\th}\subseteq\O$ is the set of potential initial states the system is in at time $t=\th$. The state region of the system at the end of the forward process (time $t=0$) is determined: $\L_0 = \t_{\th\to0}(\L_\th)=\set{\t_{\th\to0}(\o_\th) \mid \o_\th \in \L_\th}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now we come to the definition of reversibility:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The forward process is reversible iff there exists a time $\phi &amp;gt; 0$ and a time-dependent $\Ue$ defined on the time interval $(0,\phi)$ (and satisfying the &lt;a href=&#34;#constraints-on-the-external-potential&#34;&gt;#Constraints on the external potential&lt;/a&gt; specified above) s.t. $\t_{0\to\phi}(\L_0)=\L_\th$ (the behavior of all the propagators on the time interval $(0,\phi]$ is determined by the choice of $\Ue$).&lt;/p&gt;
&lt;p&gt;We could also formulate reversibility a bit more abstractly. Given the KE term $T$ and internal PE term $\Ui$, and an initial state region $\L\up{1}$ and intermediary state region $\L\up{2}$, we want to find an external potential $\Ue$ defined on all time $(-\infty,\infty)$ s.t. the system oscillates periodically from $\L\up{1}$ to $\L\up{2}$ and back to $\L\up{1}$ (the combined forward and reverse process). Formally, for the chosen $\Ue$ let $\L(t)=\t_{0\to t}(\L\up{1})$. Then the transformation from $\L\up{1}$ to $\L\up{2}$ has a reversible process iff there exits $\Ue$ and time intervals $\Dt_f$ and $\Dt_r$ s.t. $\L(k(\Dt_f+\Dt_r))=\L(0)=\L\up{1}$ for all $k\in\Z$ and $\L(\Dt_f+k(\Dt_f+\Dt_r))=\L(\Dt_f)=\L\up{2}$ for all $k\in\Z$.&lt;/p&gt;
&lt;p&gt;The reversibility problem is a special case of a more general problem:&lt;br&gt;
Given $\O$, $T$, $\Ui$, $\L_i$ (initial) and $\L_f$ (final), does there exist time interval $\Dt$ and $\Ue$ defined on $[t,t+\Dt]$ s.t. $\L_f=\t_{t\to t+\Dt}(\L_i)$ ? (choice of $t \in\R$ here is arbitrary.)&lt;/p&gt;
&lt;h3 id=&#34;example-free-expansion&#34;&gt;Example: Free Expansion&lt;/h3&gt;
&lt;p&gt;A gas expands to fill a vacuum, a.k.a. &lt;a href=&#34;https://en.wikipedia.org/wiki/Joule_expansion&#34;target=&#34;_blank&#34;&gt;Joule expansion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The system is gas (a collection of N particles) confined to a container.&lt;br&gt;
The internal potential produces elastic collisions between particles. The external potential produces the walls of the container.&lt;/p&gt;
&lt;p&gt;The elastic collisions between particles can be achieved by making the internal potential a repulsive inverse-square potential between each pair of particles, i.e. $\Ui\up{i,j}(q_i,q_j)\propto \frac{1}{\abs{q_i-q_j}^2}$ for all $i,j$. A simpler model would turn interactions off when particles are outside of collision zones and turn interactions on when they are inside, e.g. $\Ui\up{i,j}(q_i,q_j)= \begin{cases}\frac{1}{\abs{q_i-q_j}^2}-\frac{1}{r^2} &amp;amp; \abs{q_i-q_j} &amp;lt; r \\ 0 &amp;amp; \abs{q_i-q_j}\geq r\end{cases}$ for all $i,j$.&lt;br&gt;
As $r\to 0$ this potential approaches an &lt;a href=&#34;https://en.wikipedia.org/wiki/Elastic_collision&#34;target=&#34;_blank&#34;&gt;instantaneous collision&lt;/a&gt; model.&lt;/p&gt;
&lt;p&gt;Similarly for the walls, a steep potential hill can be placed within some zone around the walls. Taking the width of this zone to 0 gives us an idealized infinitely thin wall with infinite repulsive force. (see diagrams, and &lt;a href=&#34;https://danabo.github.io/blog/posts/why-doesnt-uncopying-defeat-the-2nd-law/#the-reversibility-game&#34;&gt;Why Doesn&amp;#39;t Uncopying Defeat The 2nd Law#the-reversibility-game&lt;/a&gt; for more discussion.)&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/gas_container_potential.jpg&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Steep potential hills make up the walls of the box holding a gas.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/potential_wall.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;An idealized wall is an infinitely steep and infinitely high potential hill (depicted on the right). This can be constructed by taking the limit of a finite hill (left) as its height goes to infinity and its width goes to zero.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;First to describe the forward process on $t=-100$ to $0$, we suppose the container is fixed (external potential $\Ue$ is a constant field) for time $t \in (-\infty,-100)$.  At time $t=-100$ the system is in an equilibrium state at temperature $T$ and approximately uniformly filling the container (with some spatial volume $V$). The set of all such states at time $-100$ is the state region $\L\up{1}$. Formally, let $\L\up{1}$ be the set of all positions and momenta of the N particles s.t. the gas is in equilibrium with a constant temperature $T$ and uniformly filling the container. Equilibrium states are those with &lt;a href=&#34;https://en.wikipedia.org/wiki/Ergodicity&#34;target=&#34;_blank&#34;&gt;ergodic&lt;/a&gt; trajectories. Specifically, let&amp;rsquo;s make $\L\up{1}$ the set of all such states which are ergodic over the time interval $(-\infty,-100)$ (i.e. ergodic into the past). Note that specifying that the gas is at temperature $T$ amounts to restricting ourselves to gas states s.t. the average KE is proportional to $T$ (and average KE continues to ergodically bounce around $T$ forever if the container is held fixed).&lt;/p&gt;
&lt;p&gt;At time $t=-100$, the container suddenly changes so that its spatial volume increases. This creates a vacuum for the gas to expand into.  The gas expands to fill the larger container during the interval $t\in[-100,0]$ (supposing $100$ units of time is enough for the gas to approach close to equilibrium in the larger container). We have that $\Ue$ is also a constant field on the time interval $[-100,0]$. We can determine $\L\up{2}$, the state region at time $t=0$, using a propagator, i.e. $\L\up{2}=\t_{(-100)\to0}(\L\up{1})$.&lt;/p&gt;
&lt;p&gt;This forward process is reversible if there exists $\Ue$ defined on the time-interval $(0,\phi]$ s.t. $\t_{0\to\phi}(\L\up{2})=\L\up{1}$. This would be the reverse process.&lt;/p&gt;
&lt;p&gt;We know from classical thermodynamics that the forward process from $\L\up{1}$ to $\L\up{2}$ is irreversible. In my naive formulation of the reversibility problem, there is not much we will be able to do with the external potential except to push the particles around. However, pushing particles back to their smaller volume transfers extra KE to them, which means the gas temperature rises (see &lt;a href=&#34;https://danabo.github.io/blog/posts/szilard-cycle-particle-piston-interaction-model/#particles-colliding-with-moving-walls&#34;&gt;Szilard Cycle Particle-Piston Interaction Model#particles-colliding-with-moving-walls&lt;/a&gt;). One would then have to figure out how to return the extra KE back to the environment.&lt;/p&gt;
&lt;h3 id=&#34;example-isentropic-adiabatic-expansion&#34;&gt;Example: Isentropic (adiabatic) Expansion&lt;/h3&gt;
&lt;p&gt;A gas is expanded/compressed by a driven piston. See &lt;a href=&#34;https://danabo.github.io/blog/posts/szilard-cycle-particle-piston-interaction-model/#simulation&#34;&gt;Szilard Cycle Particle-Piston Interaction Model#simulation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This process trades $q_i$ volume with $p_i$ volume while keeping total state volume fixed. A wall pushing against (compressing) gas particles adds KE to them. A wall pulling away from (expanding) gas particles absorbs KE from them. In classical thermodynamics, this process is reversible.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020220217150254.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;An example particle trajectory (blue) with a movable wall (orange) moving along a predefined path. Here the wall is moving away from the particle. When the particle collides with the moving wall, the particle loses KE.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020220301134051.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;An example particle trajectory (blue) with a movable wall (orange) moving along a predefined path. Here the wall is moving towards the particle. When the particle collides with the moving wall, the particle gains KE.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;video controls autoplay loop src=&#34;../../isentropic_expansion_one_particle.mp4&#34; caption=&#34;How the state region (position and velocity) of a 1D one-particle gas changes over time as a movable wall pulls away. As the container length increases, the particle slows down and the velocity range of this state region shrinks. The spatial range of the state region grows. The total state area should remain constant during the transformation.&#34; width=&#34;100%&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;As in the previous example, we let $\L\up{1}$ be the set of states in equilibrium (ergodic infinitely far into the past) at time $t=-100$ with temperature $T$ and spatial volume $V$. $\Ue$ is a fixed field during the time interval $(-\infty,-100)$. On the interval $[-100,0]$, $\Ue$ changes as a function of $t$ s.t. one of the walls of the container pushes or pulls along a fixed trajectory, until reaching its final position at time $t=0$. The resulting potential region is again determined by the propagator $\L\up{2}=\t_{-100\to0}(\L\up{1})$.&lt;/p&gt;
&lt;p&gt;Another result from classical thermodynamics is that this process, isentropic expansion/compression, is reversible. One possible reverse process defines $\Ue(\o;\ t)=\Ue(\o;\ -t)$ on the time interval $(0,100]$, so that the wall backtracks its movement from the forward process, returning to its initial position at time $t=-100$.&lt;/p&gt;
&lt;p&gt;(Note that an alternative way to model isentropic expansion/compression is to make the moving wall an inertial object with mass, and vary the mass of the wall as a function of its position. See &lt;a href=&#34;https://danabo.github.io/blog/posts/carnot-cycle/#2-isentropic-adiabatic-expansion&#34;&gt;Carnot Cycle#2-isentropic-adiabatic-expansion&lt;/a&gt;. The wall is now part of the system. By altering the mass function of the wall, the agent drives the process from the outside.)&lt;/p&gt;
&lt;h3 id=&#34;issues&#34;&gt;Issues&lt;/h3&gt;
&lt;p&gt;I call the above formulation of the reversibility problem naive because,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There are environments and environment interactions which we cannot model.&lt;/li&gt;
&lt;li&gt;Energy transfers between different parts of the environment are not accounted for.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some examples of environments we are not able to model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Isothermal_process&#34;target=&#34;_blank&#34;&gt;Isothermal expansion/compression&lt;/a&gt; (requires an infinite heat reservoir).&lt;/li&gt;
&lt;li&gt;Environment noise, e.g. thermal noise or shape uncertainty in the container walls of a gas. (see &lt;a href=&#34;https://danabo.github.io/blog/posts/why-doesnt-uncopying-defeat-the-2nd-law/#environment-noise&#34;&gt;Why Doesn&amp;#39;t Uncopying Defeat The 2nd Law#environment-noise&lt;/a&gt;.)&lt;/li&gt;
&lt;li&gt;Measurements of system state (the environment gains information about the system&amp;rsquo;s state). This includes any kind of &lt;a href=&#34;https://en.wikipedia.org/wiki/Maxwell%27s_demon&#34;target=&#34;_blank&#34;&gt;demon&lt;/a&gt;. (see &lt;a href=&#34;https://danabo.github.io/blog/posts/why-doesnt-uncopying-defeat-the-2nd-law/#measurement&#34;&gt;Why Doesn&amp;#39;t Uncopying Defeat The 2nd Law#measurement&lt;/a&gt; and &lt;a href=&#34;https://danabo.github.io/blog/posts/why-doesnt-uncopying-defeat-the-2nd-law/#superdemons&#34;&gt;Why Doesn&amp;#39;t Uncopying Defeat The 2nd Law#superdemons&lt;/a&gt;.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Heat reservoirs also mess up this formulation of reversibility because irreversible energy transfers between parts of the environment could take place. For instance, in &lt;a href=&#34;#example-free-expansion&#34;&gt;#Example Free Expansion&lt;/a&gt;, we could perform isothermal compression to put the gas back into its original container at its original temperature. That means the gas is in thermal contact with a heat reservoir at constant temperature. The energy transferred to the gas as KE during compression is absorbed by the heat reservoir. If the heat reservoir is considered part of the environment, then we are ignoring the conversion of potential energy in the moving wall to heat energy of the reservoir. That distinction is needed for free expansion to be considered irreversible.&lt;/p&gt;
&lt;p&gt;Remember, our primary interest is in the reversibility of processes that convert an energy source into useful work. If both the energy source and the thing work is being done to are considered part of the environment, then it wouldn&amp;rsquo;t make sense to suppose we are indifferent to all the ways energy may be moved around in the environment. It is not enough to suppose that the system is reversed simply if its energy gain/loss is returned to the environment - we care about where in the environment it goes.&lt;/p&gt;
&lt;h1 id=&#34;other-formulations&#34;&gt;Other Formulations&lt;/h1&gt;
&lt;p&gt;There are two potential avenues towards resolving the above issues with my naive formulation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Model the environment as part of the system, i.e. model the environment and system together as an isolated parent system.&lt;/li&gt;
&lt;li&gt;Model the environment as a conditional potential.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I don&amp;rsquo;t have an issue-free solution at present. Below I will detail how these two approaches work and their pros and cons.&lt;/p&gt;
&lt;h2 id=&#34;1-model-environmentsystem-as-a-joint-system&#34;&gt;1. Model environment+system as a joint system&lt;/h2&gt;
&lt;p&gt;Everything in the outside that has a causal relationship with the system is explicitly modeled in the physics. That is to say, we suppose that the environment has $m$ degrees of freedom, so that the joint state of the system and environment is $\o  = (\o_1,\o_2,\dots,\o_{2n+2m}) = (q_1, q_{n+m}, p_1, p_{n+m}) \in \O$. The provided Hamiltonian (and induced propagators) is a function of all the coordinates of both the system and environment.&lt;/p&gt;
&lt;p&gt;The Hamiltonian of the joint system must now be time-independent, implying a time-independent external potential term. This allows for some influence from the outside (outside of the environment), but in a limited fashion. The external potential is a sum of fixed (in time) potential fields, so there is no outside to drive the system anymore.&lt;/p&gt;
&lt;p&gt;Pros&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we are able to model the physics of the environment, then the definition of reversibility I gave in &lt;a href=&#34;#reversibility---naive-definition&#34;&gt;#Reversibility - Naive Definition&lt;/a&gt; works.&lt;/li&gt;
&lt;li&gt;We can properly model environment uncertainty (e.g. noise) with a state region on the joint system+environment state space.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Infinite heat reservoirs require infinite degrees of freedom in the environment, i.e. $m=\infty$. This can easily make the definition of the joint system ill posed (e.g. the Hamiltonian can become infinite).&lt;/li&gt;
&lt;li&gt;The system+environment must be otherwise isolated (except for the fixed external potential). That means we cannot model open systems, which is something of particular interest in the thermodynamics of living systems.&lt;/li&gt;
&lt;li&gt;The model of the environment needs to be physically accurate. That means no more walls moving along pre-programmed paths (equivalent to inertial walls with infinite mass), like in &lt;a href=&#34;#example-isentropic-adiabatic-expansion&#34;&gt;#Example Isentropic adiabatic Expansion&lt;/a&gt;. Also no more discrete degrees of freedom, since all the coordinates must be real-valued and the Hamiltonian must be a smooth function. That means the physical memory bits, like in the Szilard engine (&lt;a href=&#34;https://danabo.github.io/blog/posts/reversible-szilard-cycle-problem/#uncopying&#34;&gt;Reversible Szilard Cycle Problem#uncopying&lt;/a&gt;), must be modeled as some kind of continuous process (e.g. magnets). That can be quite cumbersome.&lt;/li&gt;
&lt;li&gt;It is not straightforward to have an outside agent drive the forward and reverse processes, though it is still technically possible. In &lt;a href=&#34;#example-isentropic-adiabatic-expansion&#34;&gt;#Example Isentropic adiabatic Expansion&lt;/a&gt;, at the bottom, I briefly mentioned that an agent can drive the interaction between the gas an an inertial wall (movable wall with finite mass whose degree of freedom is included in the Hamiltonian) by modifying the wall&amp;rsquo;s mass as a function of its position. The generalization of that operation is to modify the Hamiltonian at moments in time, e.g. at time $t$, in a way such that the kinetic energy and potential energy of each $i$-th coordinate is unchanged for states in the state region $\L(t)$ at time $t$. In other words, we are creating a time-dependent Hamiltonian as piecewise (in time) stitching together of time-independent Hamiltonians within various time intervals, so that the boundaries between the piecewise segments have a continuous transition (in this case, all the individual KE and PE for every coordinate is continuously transitioned between Hamiltonians).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-model-the-environment-as-a-conditional-potential&#34;&gt;2. Model the environment as a conditional potential&lt;/h2&gt;
&lt;p&gt;A conditional external potential function conditions its own trajectory on the trajectory of the system. There are a few ways to formally define conditional potentials:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$\Ue(\o;\ \s_{(-\infty,t]}, t)$ is a function of the entire history of the system, $\s_{(-\infty,t]} : (-\infty,t]\to\O$, which is a segment of the system&amp;rsquo;s trajectory defined on the time interval $(-\infty,t]$. Think of this as associating a different trajectory of $\Ue$ to every trajectory of the system. This allows the external potential&amp;rsquo;s time evolution to condition on what the system is doing, essentially allowing the environment to measure the state of the system. Given an initial state region, the external potential can fork (behave differently for different system trajectories stemming from the state region), resulting in uncertainty on $\Ue$. We could implement environment noise as initial uncertainty on $\Ue$, i.e. we have a set of initial potential functions $\Ue$ as well as an initial state region.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The field-snapshot ${\Ue}\up{t} : \o\mapsto\Ue(\o;\ t)$ is treated as the state of the environment. The propagators time-evolve both system and environment state: $(\o&#39;, \Ue&#39;) = \t_{t_1\to t_2}(\o, \Ue)$ where $\Ue,\Ue&#39; : \O\to\R$ are time-independent external potentials.&lt;br&gt;
(this is problematic, as entropy may be absorbed into the environment and carried away so that it is no longer reflected in the external potential)&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The environment has its own state $\vec{\xi}$.&lt;br&gt;
Let $\vec{q}=(q_1,\dots,q_n)$, $\vec{p}=(p_1,\dots,p_n)$, and $\vec{\xi}$ some state vector for the environment. The environment&amp;rsquo;s state need not be inertial. $\H(\vec{q},\vec{p};\ \vec{\xi}, t)=T(\vec{p})+\Ui(\vec{q},\vec{p})+\Ue(\vec{q},\vec{p};\ \vec{\xi},t)$, where $\vec{\xi}, t$ are not inertial coordinates involved in Hamilton&amp;rsquo;s equations (not involved in the physics), but merely specify which external potential to use. ($t$ can be considered part of the environment state, e.g. if someone has a clock.)  There is a family of joint propagators of the form $(\vec{q}&#39;,\vec{p}&#39;, \vec{\xi}&#39;) = \t_{t_1\to t_2}(\vec{q},\vec{p}, \vec{\xi})$. We require that the joint propagators be bijections (information preserving). This means than when the environment measures unknown state (conditions on state within the state region), the environment state becomes uncertain. Resetting the system and environment then includes erasing redundant state within the environment. Perhaps something like &lt;a href=&#34;https://en.wikipedia.org/wiki/Landauer%27s_principle&#34;target=&#34;_blank&#34;&gt;Landauer&amp;rsquo;s_principle&lt;/a&gt; can be derived from this setup.&lt;/p&gt;
&lt;p&gt;Pros&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can get away with modeling less of the environment (only need to model interactions relevant to the system). Don&amp;rsquo;t care about the state of the universe. Don&amp;rsquo;t need to reverse literally everything in the universe to reverse the system+environment. E.g. don&amp;rsquo;t need to reverse things that happened in far away galaxies or wipe the memories of people who know the process occurred.&lt;/li&gt;
&lt;li&gt;Can potentially model open environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reversibility is not longer well defined for open environments because it is not clear what it means for the environment to be reset, i.e. will the environment behave the same every time if its behavior depends on outside state which we are not modeling?
&lt;ul&gt;
&lt;li&gt;Boundary between system and environment is not well defined - is a piston part of the system or environment? Anything that cannot be repeated (requires energy that isn&amp;rsquo;t ambient in the env) is part of the system, like the piston.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mathematically unwieldy to deal with trajectories of potential fields directly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;what-is-entropy&#34;&gt;What is Entropy?&lt;/h1&gt;
&lt;p&gt;The use of state regions (subsets of state space) in the formulation of the reversibility problem above looks like &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula&#34;target=&#34;_blank&#34;&gt;Boltzmann&amp;rsquo;s definition of thermodynamic entropy&lt;/a&gt;. Instead of saying &amp;ldquo;state region&amp;rdquo;, it is standard to say &amp;ldquo;&lt;a href=&#34;https://en.wikipedia.org/wiki/Microstate_%28statistical_mechanics%29&#34;target=&#34;_blank&#34;&gt;macrostate&lt;/a&gt;&amp;rdquo;, which is a set of microstates, i.e. set of fine-grain states, i.e. elements of state space $\O$.&lt;/p&gt;
&lt;p&gt;Boltzmann defines the entropy $S$ of a finite macrostate $\L\subseteq\O$ to be&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
S \propto \log\abs{\L}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;i.e. entropy is proportional to the log of the cardinality of $\L$.&lt;/p&gt;
&lt;p&gt;For infinite $\L$, we need a &lt;a href=&#34;https://en.wikipedia.org/wiki/Measure_%28mathematics%29&#34;target=&#34;_blank&#34;&gt;measure&lt;/a&gt; on $\O$ (in the measure-theoretic sense, not physical measurement) to give us a way to quantify sizes of state regions (**ahem**, macrostates). Let $\mu : \mc{E} \to \R_{\geq0}$ be a measure on $\O$, i.e. $\mu$ is a function from measurable subsets of $\O$ to their respective sizes (not cardinality, but more like length, area or volume for 1D, 2D or 3D regions). Briefly, $\mc{E}$ is a set of measurable subsets of $\O$, and the tuple $(\O,\mc{E},\mu)$ is called a &lt;a href=&#34;https://en.wikipedia.org/wiki/Measure_space&#34;target=&#34;_blank&#34;&gt;measure space&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then Boltzmann entropy takes the general form&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
S \propto \log\mu(\L)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;When $\L$ is finite, we can define $\mu$ to be the &lt;a href=&#34;https://en.wikipedia.org/wiki/Counting_measure&#34;target=&#34;_blank&#34;&gt;counting measure&lt;/a&gt; (uses set cardinality as set size) to get back Boltzmann&amp;rsquo;s definition. I hope to show in another post that this definition of entropy of state regions makes sense in the case of gas thermodynamics.&lt;/p&gt;
&lt;p&gt;In statistical thermodynamics, there is a common derivation where state space (positions and momenta) of the system (e.g. gas) is discretized into a finite state space. Then the limit of Boltzmann entropy as the discretization size goes to zero gives the entropy of the continuous system. This is equivalent to choosing a uniform measure on state space.&lt;/p&gt;
&lt;p&gt;(However, this does not make the choice of measure unique or objective, since what is considered uniform depends on choice of coordinates. E.g. going from cartesian to polar coordinates alters what is considered a uniform measure on the respective coordinate spaces. You could argue that the measure should be uniform on physical space, but then there is no unique uniform measure on canonical coordinates which don&amp;rsquo;t correspond directly to physical space.)&lt;/p&gt;
&lt;p&gt;Before this section, no mention of measures on $\O$ is made. The formulation of the reversibility problem above does not require any quantity like entropy, and does not depend on the choice of measure $\mu$ on $\O$. This allows us to avoid a pesky interpretation problem: what does $\mu$ represent and is there a unique most appropriate choice of $\mu$ for a given system? One could say that side-stepping this issue is necessary for dealing with non-equilibrium (ir)reversibility in general.&lt;/p&gt;
&lt;p&gt;The formulation above considers state regions of $\O$ for a single system. In thermodynamics it is often the case that you might consider the separate entropies of multiple different systems (including the environment), and it is common to talk about one system transferring entropy to another. Supposing the state space $\O$ contains multiple systems (each system is described by a subset of coordinate indices) and we have a state region $\L\subseteq\O$ and measure $\mu$ on $\O$, what is the entropy of each system? I propose that the entropy of a system described by coordinates $(\o_i,\dots,\o_j)$ is the log-measure of the projection of $\L$ onto $(\o_i,\dots,\o_j)$, i.e. $S\up{i,\dots,j} \propto \log\mu\Big(\bigcup\set{[\o]_{i,\dots,j}\mid\o\in\L}\Big)$ where $S\up{i,\dots,j}$ is the entropy of the subsystem occupying coordinates $i,\dots,j$, and $[\o]_{i,\dots,j}=\set{\zeta \in \O \mid (\zeta_i,\dots,\zeta_j)=(\o_i,\dots,\o_j)}$ is the set of all states sharing the coordinates $(\o_i,\dots,\o_j)$.&lt;/p&gt;
&lt;p&gt;I hope to write more about what justifies this definition of entropy in a future post: &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;Connecting Entropy And Information&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;the-interpretation-of-state-regions&#34;&gt;The Interpretation of State Regions&lt;/h1&gt;
&lt;p&gt;I briefly mentioned a generalized problem at the bottom of &lt;a href=&#34;#reversibility---naive-definition&#34;&gt;#Reversibility - Naive Definition&lt;/a&gt;. To recap, given $\O$, $T$, $\Ui$, $\L_i$ (initial) and $\L_f$ (final), does there exist time interval $\Dt$ and $\Ue$ defined on $[t,t+\Dt]$ s.t. $\L_f=\t_{t\to t+\Dt}(\L_i)$ ?&lt;/p&gt;
&lt;p&gt;The chosen state regions are seemingly agent-specific, i.e. are dependent on an agent&amp;rsquo;s goals. We could characterize them in the following way: $\L_i$ represents what the agent knows, and $\L_f$ represents what the agent wants to have happen in the future.&lt;/p&gt;
&lt;p&gt;Given $\L_i$ and $\L_f$, whether there exists a process (specified by $\Ue$) that transforms $\L_i$ to $\L_f$ should be objective, i.e. is a question about physics with a well defined answer. The same is true with the reversibility question: when what is considered successful reversal is defined, the question of whether its achievable has an objective answer.&lt;/p&gt;
&lt;p&gt;A further note, the agent&amp;rsquo;s state of information, $\L_i$, is not subjective or arbitrary. For instance, if the agent posits to know the state of the system to higher resolution then they actually do, the reliable repeatability of the transformation from states in $\L_i$ to states in $\L_f$ will not bear out in practice. Though, the agent could disregard information they have and make $\L_i$ larger than the state region representing what they know, the agent cannot pretend to have more information than they do.&lt;/p&gt;
&lt;p&gt;The general problem of reliably transforming $\L_i$ to $\L_f$ is essentially what intelligent systems (agents) try to solve. Agents seek to control their environments. That means causing changes in their environments (i.e. taking actions) so that those environments tend towards desired states. In this way, the generalization of reversibility is controllability. That too would seem to be an objective property of physical systems, once the goal of the agent is defined.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Szilard Cycle Particle-Piston Interaction Model</title>
      <link>https://danabo.github.io/blog/posts/szilard-cycle-particle-piston-interaction-model/</link>
      <pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/szilard-cycle-particle-piston-interaction-model/</guid>
      <description>&lt;p&gt;Admittedly my first-pass interaction model in &lt;a href=&#34;https://danabo.github.io/blog/posts/reversible-szilard-cycle-problem/#part-ii-information-is-never-lost&#34;&gt;Reversible Szilard Cycle Problem#part-ii-information-is-never-lost&lt;/a&gt; is not physically realistic. By interaction model, I mean how the particle and piston interact over time. Here I explore some alternative interaction models that try to be more realistic. My main question is whether which-side information is still preserved.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\ms}{\mathscr}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\es}{\emptyset}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\d}{\delta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\L}{\Lambda}&lt;br&gt;
\newcommand{\G}{\Gamma}&lt;br&gt;
\newcommand{\g}{\gamma}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\N}{\mb{N}}&lt;br&gt;
\newcommand{\I}{\mb{I}}&lt;br&gt;
\newcommand{\H}{\mc{H}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\s}{\sigma}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[1]{_{\mid #1}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\Dt}{{\Delta t}}&lt;br&gt;
\newcommand{\tr}{\rightarrowtail}&lt;br&gt;
\newcommand{\qed}{\ \ \blacksquare}&lt;br&gt;
\newcommand{\c}{\overline}&lt;br&gt;
\newcommand{\dg}{\dagger}&lt;br&gt;
\newcommand{\dd}{\mathrm{d}}&lt;br&gt;
\newcommand{\pd}{\partial}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The physically realistic thing to do would be to make the piston an inertial body, meaning that it has its own momentum. If the particle is doing work on the piston by pushing it, then the piston must be climbing a potential gradient. Then, as an inertial body, the piston should fall back down the gradient when not being pushed.&lt;/p&gt;
&lt;p&gt;I also tried settling this matter by going to the source: &lt;a href=&#34;http://fab.cba.mit.edu/classes/863.18/notes/computation/Szilard-1929.pdf&#34;target=&#34;_blank&#34;&gt;Szilard&amp;rsquo;s original paper&lt;/a&gt;. Szilard specifies that the piston is forcibly moved by an operator. This can be modeled as a wall that moves in a predefined way, regardless of its interaction with the particle. That is certainly easier to model on a computer.&lt;/p&gt;
&lt;p&gt;Szilard specifies that the expansion is &lt;a href=&#34;https://en.wikipedia.org/wiki/Isothermal_process&#34;target=&#34;_blank&#34;&gt;isothermal&lt;/a&gt;, which means the particle regains lost KE (on average) by interacting with oscillating particles in the walls of the container. However, I am modeling the expansion as &lt;a href=&#34;https://en.wikipedia.org/wiki/Isentropic_process&#34;target=&#34;_blank&#34;&gt;isentropic&lt;/a&gt; to avoid the issue of environmental noise (see &lt;a href=&#34;https://danabo.github.io/blog/posts/why-doesnt-uncopying-defeat-the-2nd-law/#defining-environmental-noise&#34;&gt;Why Doesn&amp;#39;t Uncopying Defeat The 2nd Law#defining-environmental-noise&lt;/a&gt;). That means the particle does not exchange KE with the walls of the container, and so the particle loses net KE over time.&lt;/p&gt;
&lt;h1 id=&#34;inertial-piston-model&#34;&gt;Inertial Piston Model&lt;/h1&gt;
&lt;p&gt;Like last time I am modeling this in one spatial dimension. The piston and particle each have one position coordinate. The container has a length, with &amp;ldquo;caps&amp;rdquo; on each end.&lt;/p&gt;
&lt;p&gt;If the piston is pushing a mass $M$ up against gravity, then the potential field has a constant slope $g$. Then the motion of the piston is described by $M \ddot{x}=-g$, which gives us a parabola (think ball thrown in the air).&lt;/p&gt;
&lt;p&gt;The particle experiences no forces, except for elastic collisions with the walls of the container and the piston. We can use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Elastic_collision#One-dimensional_Newtonian&#34;target=&#34;_blank&#34;&gt;1D elastic collision formula&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\pmatrix{u_f\\v_f}=\frac{1}{M+m}\pmatrix{M-m &amp;amp; 2m \\ 2M &amp;amp; m-M}\pmatrix{u_i\\v_i}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $M$ is the mass of the piston (i.e. the mass it is pushing), $m$ is the mass of the particle, $v_i,v_f$ are the initial and final velocities of the particle before and after the collision, and $u_i,u_f$ are the initial and final velocities of the piston before and after collision.&lt;/p&gt;
&lt;p&gt;When the particle collides with a fixed wall, its velocity sign changes, i.e. $v_f = -v_i$.&lt;/p&gt;
&lt;p&gt;To have the particle push the piston slowly over the course of many back-and-forth bounces, the piston&amp;rsquo;s mass should be greater than the particle&amp;rsquo;s, and the particle&amp;rsquo;s velocity should be greater than piston.&lt;/p&gt;
&lt;p&gt;For example, here is a simulation with the following settings&lt;br&gt;
piston: (mass) $M=1$ and (initial velocity) $u_0 = 0$&lt;br&gt;
particle: (mass) $m = 1/10$ and (initial velocity) $v_0=5$&lt;br&gt;


  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020220215093814.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Blue is the particle&amp;#39;s trajectory, and orange is the piston&amp;#39;s trajectory. The particle bounces off both the piston and the fixed wall at position -1. The behavior of the combined system is locally chaotic, and globally seems to oscillate.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;With a much lower particle mass and much higher initial particle velocity, we see less chaotic behavior and a more stable oscillation:&lt;br&gt;
piston: (mass) $M=1$ and (initial velocity) $u_0 = 0$&lt;br&gt;
particle: (mass) $m = 1/100$ and (initial velocity) $v_0=20$&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020220215093846.png&#34; alt=&#34;&#34;&gt;
&lt;/p&gt;
&lt;p&gt;To avoid this sort of oscillation, I should have the piston mass decrease as it is pushed up, corresponding to &lt;a href=&#34;https://en.wikipedia.org/wiki/Isentropic_process&#34;target=&#34;_blank&#34;&gt;adiabatic (isentropic) expansion&lt;/a&gt;, where the entropy of the gas remains fixed as it expands. The gas does not absorb thermal energy from its container, and so it cools as it transfers KE to the piston. In order to maintain approximate equilibrium through out the change (making it a &lt;a href=&#34;https://en.wikipedia.org/wiki/Quasistatic_process&#34;target=&#34;_blank&#34;&gt;quasistatic process&lt;/a&gt;), the piston mass is slowly decreased so that the gas pressure and piston force opposing it always in balance.&lt;/p&gt;
&lt;p&gt;I tried running the same simulation where the piston mass is the following function of the container length: $M(L)=M_0\cdot\par{\frac{L_0}{L}}^\gamma$ where $\gamma=1$. I got this formula from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Isentropic_process#Table_of_isentropic_relations_for_an_ideal_gas&#34;target=&#34;_blank&#34;&gt;ideal gas isentropic relation&lt;/a&gt; between pressure and volume: $\frac{P_f}{P_i}=\par{\frac{V_i}{V_f}}^\gamma$. Pressure of a single particle applied to a point (end of the 1D container) is just its force, and the volume of the 1D container is its length. This resulted in very similar looking particle-piston trajectories as before, with oscillatory behavior. I suspect I would need to have the piston mass decrease as it is pushed up the potential slope, but not increase as it falls down the potential slope.&lt;/p&gt;
&lt;p&gt;At any rate, a quick workaround is to fix the piston in place once it reaches its maximally expanded position. Like this:&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020220215103320.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
Like in &lt;a href=&#34;https://danabo.github.io/blog/posts/reversible-szilard-cycle-problem/#part-ii-information-is-never-lost&#34;&gt;Reversible Szilard Cycle Problem#part-ii-information-is-never-lost&lt;/a&gt;, I sampled a thousand initial particle states on each side of the container - left side as red and right side as blue.&lt;/p&gt;
&lt;p&gt;&lt;video controls autoplay loop src=&#34;../../szilard_inertial_piston.mp4&#34; caption=&#34;&#34; width=&#34;100%&#34;&gt;&lt;/video&gt;&lt;br&gt;
It seems that this too results in disjoint state regions, and so the which-side information is again preserved. What&amp;rsquo;s also apparent is that the state regions are shrinking. &lt;a href=&#34;https://en.wikipedia.org/wiki/Liouville%27s_theorem_%28Hamiltonian%29&#34;target=&#34;_blank&#34;&gt;Liouville&amp;rsquo;s theorem&lt;/a&gt; guarantees that state volume will be conserved in a closed system. However, here there is energy being lost to the piston. If we plotted the joint state space of the particle and piston we would see that the total state region does not shrink.&lt;/p&gt;
&lt;p&gt;Since the particle&amp;rsquo;s state region is shrinking, that implies there is state uncertainty being moved into the piston dimension. What does that imply about the reversibility of this process? A process is thermodynamically reversible if both the system undergoing the process and its environment can be reset to their joint initial state. If uncertainty is transferred from the particle to the piston (piston&amp;rsquo;s state region has increased in volume), is that reversible? Since all of the transferred uncertainty is accumulating in a single degree of freedom (the total energy absorbed from the particle), it should be possible in principle to transfer that uncertainty back to the particle. It is only when uncertainty becomes spread out among many degrees of freedom (like heat transfer from many particles to many particles) that this transfer becomes irreversible (why? - An exercise left to the reader).&lt;/p&gt;
&lt;h1 id=&#34;controlled-piston-model&#34;&gt;Controlled Piston Model&lt;/h1&gt;
&lt;h2 id=&#34;particles-colliding-with-moving-walls&#34;&gt;Particles colliding with moving walls&lt;/h2&gt;
&lt;p&gt;When you forcibly move a wall into particles, you transfer KE to them. This is why compressing a gas adds heat energy (average KE) to the gas.&lt;br&gt;
The time reversal of this process is forcibly expanding a gas by pulling the wall away from the particles. This must result in KE extraction from the particles.&lt;/p&gt;
&lt;p&gt;We see this bear out in the low-level Newtonian mechanics of collision with the formula $v_f = 2u-v_i$, where $v_i$ is the incoming (initial) velocity of the particle before wall collision, $v_f$ is the outgoing (final) velocity of the particle after wall collision, and $u$ is the fixed velocity of the wall.&lt;/p&gt;
&lt;p&gt;This formula can be derived from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Elastic_collision#One-dimensional_Newtonian&#34;target=&#34;_blank&#34;&gt;one-dimensional elastic collision formula&lt;/a&gt; (see &lt;a href=&#34;#inertial-piston-model&#34;&gt;#Inertial Piston Model&lt;/a&gt;) by taking the mass of the wall to infinity (&lt;a href=&#34;https://www.physicsforums.com/threads/elastic-collision-against-a-moving-wall.236652/#post-2347189&#34;target=&#34;_blank&#34;&gt;reference&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;Taking the limit as $m_u \to \infty$, we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lim_{M\to\infty}\frac{1}{M+m}\pmatrix{M-m &amp;amp; 2m \\ 2M &amp;amp; m-M} = \pmatrix{1 &amp;amp; 0 \\ 2 &amp;amp; -1}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;giving us&lt;br&gt;
$$&lt;br&gt;
\pmatrix{u_f\\v_f}=\pmatrix{1 &amp;amp; 0 \\ 2 &amp;amp; -1}\pmatrix{u_i\\v_i} = \pmatrix{u_i\\2u_i-v_i}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $u=u_i=u_v$.&lt;/p&gt;
&lt;h2 id=&#34;simulation&#34;&gt;Simulation&lt;/h2&gt;
&lt;p&gt;In this simulation the piston has infinite mass and moves along a predefined path, representing an external operator controlling the movement of the piston. The particle collides elastically with the piston using the formula $v_f = 2u-v_i$, where $u$ is the velocity of the piston. If the piston is moving away from the particle (in the same direction as $v_i$), we see that $\abs{v_f}&amp;lt;\abs{v_i}$ and so the particle loses KE.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020220217150254.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;An example particle trajectory (blue) with a piston (orange) moving along a predefined path. When the particle collides with the moving piston it loses KE.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Here I sample an ensemble of initial particles. The blue particles bounce off the blue piston and the red particles bounce off the red piston. Since the blue and red pistons follow the same path for all particles (of the same color), I can visualize the pistons superimposed on the ensemble. Since the piston&amp;rsquo;s predefined movement depends on which side (which color) the particle is on, this controll &amp;ldquo;program&amp;rdquo; requires the which-side bit stored in memory.&lt;br&gt;
&lt;video controls autoplay loop src=&#34;../../szilard_piston_fixed_movement.mp4&#34; caption=&#34;&#34; width=&#34;100%&#34;&gt;&lt;/video&gt;&lt;br&gt;
Clearly the state regions overlap over time, and so the which-side information is no longer present in the particle system at the end of the piston expansion.&lt;/p&gt;
&lt;p&gt;Question: Why do some interaction models preserve which-side information, while other interaction models do not?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Why Doesn&#39;t Uncopying Defeat The 2nd Law?</title>
      <link>https://danabo.github.io/blog/posts/why-doesnt-uncopying-defeat-the-2nd-law/</link>
      <pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/why-doesnt-uncopying-defeat-the-2nd-law/</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;https://danabo.github.io/blog/posts/reversible-szilard-cycle-problem/&#34;&gt;Reversible Szilard Cycle Problem&lt;/a&gt; I pondered whether uncopying a bit of information at the end of the Szilard cycle makes the full cycle reversible, apparently getting around the 2nd law. This &amp;ldquo;loophole&amp;rdquo; is more much pervasive to thermodynamics than the Szilard engine. I will go through its generalization in &lt;a href=&#34;#maxwell%27s-superdemon&#34;&gt;#Maxwell&amp;rsquo;s Superdemon&lt;/a&gt;. I assume the 2nd law holds, so in &lt;a href=&#34;#slaying-the-superdemon&#34;&gt;#Slaying The Superdemon&lt;/a&gt; I consider some possible reasons why this loophole doesn&amp;rsquo;t work.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\ms}{\mathscr}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\es}{\emptyset}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\tau}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\d}{\delta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\L}{\Lambda}&lt;br&gt;
\newcommand{\G}{\Gamma}&lt;br&gt;
\newcommand{\g}{\gamma}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\N}{\mb{N}}&lt;br&gt;
\newcommand{\I}{\mb{I}}&lt;br&gt;
\newcommand{\H}{\mc{H}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\s}{\sigma}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[1]{_{\mid #1}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
\newcommand{\Dt}{{\Delta t}}&lt;br&gt;
\newcommand{\tr}{\rightarrowtail}&lt;br&gt;
\newcommand{\qed}{\ \ \blacksquare}&lt;br&gt;
\newcommand{\c}{\overline}&lt;br&gt;
\newcommand{\dg}{\dagger}&lt;br&gt;
\newcommand{\dd}{\mathrm{d}}&lt;br&gt;
\newcommand{\pd}{\partial}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Note: I will use the term &amp;ldquo;system&amp;rdquo; to refer to a state space, and &amp;ldquo;process&amp;rdquo; to refer to a particular time-evolution (i.e. propagator) that state undergoes. In other words, a system undergoes a process.&lt;/p&gt;
&lt;h1 id=&#34;maxwells-superdemon&#34;&gt;Maxwell&amp;rsquo;s Superdemon&lt;/h1&gt;
&lt;h2 id=&#34;the-reversibility-game&#34;&gt;The Reversibility Game&lt;/h2&gt;
&lt;p&gt;We can treat the general reversibility question in thermodynamics as a game. Player 1 provides a process to be reversed, and the goal for player 2 is to reverse the process back to its initial state without any net change to the environment. Player 2 is allowed to interact with and modify the system in question in any way they wish. Though this seems like it gives player 2 too much power (making this a rather easy game), player 2&amp;rsquo;s task should be impossible for a certain class of processes by the 2nd law of thermodynamics.&lt;/p&gt;
&lt;p&gt;To analyze this game formally, we need to make the game and its rules well defined. Though we can use classical mechanics to describe processes to be reversed, the question remains: what kinds of actions can player 2 take on any given process represented in classical mechanics?&lt;/p&gt;
&lt;p&gt;An answer I&amp;rsquo;ve been toying with is motivated by the idea that interactions between any two systems are ultimately mediated by force fields (e.g. gravity and the EM field). So classical mechanics already comes packaged with a general purpose interaction formalism: the potential field.&lt;/p&gt;
&lt;p&gt;If we wanted to keep the space of possible player 2 actions as broad as possible, we could specify that player 2 can arbitrarily manipulate a (time-dependent) external potential field that the process exists on top of (one which the components of the system interact with). &amp;ldquo;Time-dependent&amp;rdquo; means the state of the potential field may be a function of some absolute time (player 2 decides how it changes and when). &amp;ldquo;External&amp;rdquo; means it is not considered part of the system in question, but is generated by an external source. In this external potential field, player 2 can manifest walls and potential slopes that push/pull parts of the system around. By looking at the total change in the system&amp;rsquo;s energy, we can infer how much energy player 2 had to have spent to get there.&lt;/p&gt;
&lt;p&gt;The full action space of player 2 is then the choice of any potential field that is a function of time (may change over time any way player 2 wishes) overlaid on top of the system. This &amp;ldquo;interaction via potential field&amp;rdquo; framework allows for a very general class of interventions on physical systems, though it may be too general. As we shall see, some field manipulations player 2 can make may not be realistically achievable. However, I would not expect the validity of the 2nd law to hinge on the mere intractability of engineering what is needed to instantiate some potential field. Instead I would expect there to be some fundamental reason the specific fields needed to violate the 2nd law in some instance of this game are impossible to realize.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll make heavy use of external potential fields that contain &amp;ldquo;walls&amp;rdquo;, which are steep potential hills. So when I talk about a gas in a box, the gas particles make up the system and the walls of the box make up the environment of the gas. Those walls are potential hills in the external potential field. Player 2 is allowed to manipulate that external potential anyway they please, e.g. by modifying, adding and removing walls, among other manipulations.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/gas_container_potential.jpg&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Steep potential hills make up the walls of the box holding a gas.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/potential_wall.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;An idealized wall is an infinitely steep and infinitely high potential hill (depicted on the right). This can be constructed by taking the limit of a finite hill (left) as its height goes to infinity and its width goes to zero.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;measurement&#34;&gt;Measurement&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ve specified how player 2 can affect the target system in this game, but we also need to specify how player 2 can be effected by the system.&lt;/p&gt;
&lt;p&gt;Player 2 acts like the environment of the system by manipulating the external potential field the system lies over. But the system may also produce its own potential field(s) which affect the environment. In this way the system may leak information about its state into the environment (any environment state whose trajectory depends in part on the system&amp;rsquo;s state will gain information about the system).&lt;/p&gt;
&lt;p&gt;Below I will consider cases when player 2 does not have complete information about the system or the environment. In those situations, player 2 is allowed to perform measurements on the system to gain information. A measurement apparatus is a system within the environment whose state evolution depends on the system&amp;rsquo;s state evolution.&lt;/p&gt;
&lt;p&gt;Using the data (un)copying process from &lt;a href=&#34;https://danabo.github.io/blog/posts/reversible-szilard-cycle-problem/#uncopying&#34;&gt;Reversible Szilard Cycle Problem#uncopying&lt;/a&gt; as an intuition pump, we can suppose that the target system&amp;rsquo;s state determines some potential field which the measurement apparatus sits upon. As the system&amp;rsquo;s state changes over time, that potential field changes over time, which may guide the measurement apparatus&amp;rsquo;s state.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/potential_field_measurement.jpg&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;The measurement apparatus state has a degree of freedom $x$, which is guided by the potential. The potential is produced by the system being measured (the target system). Whether $x$ is guided into the left or right potential well depends on how the potential changes over time, which is determined by how the state of the target system  changes over time.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In this way, any potential fields the target system emits can be used to perform measurements on the target system, which amounts to correlating environment state with the state of the target system.&lt;/p&gt;
&lt;h2 id=&#34;example-free-expansion&#34;&gt;Example: Free Expansion&lt;/h2&gt;
&lt;p&gt;A canonical example of an irreversible process is the free expansion of a gas, a.k.a. &lt;a href=&#34;https://en.wikipedia.org/wiki/Joule_expansion&#34;target=&#34;_blank&#34;&gt;Joule expansion&lt;/a&gt;. This is where a gas is allowed to expand into a larger region of space unimpeded. The gas does no work in the process (e.g. does not push a piston), and so its thermodynamic potential to do work is lost.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020220217152946.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;\&amp;#34;A free expansion of a gas can be achieved by moving the piston out faster than the fastest molecules in the gas.\&amp;#34; Source: [Wikipedia](https://upload.wikimedia.org/wikipedia/commons/5/5e/Before_during_after_sudden_expansion.jpg).&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;A summary of the free expansion process:&lt;br&gt;
(a) There is a gas in a box at equilibrium.&lt;br&gt;
(b) The box is made rapidly larger so that the gas expands into the resulting vacuum (e.g. by removing a barrier to a larger container, or by moving a piston).&lt;br&gt;
(c) The gas returns to equilibrium in the larger container.&lt;/p&gt;
&lt;p&gt;We can represent the state of an N-particle gas with the vectors $\vec{x},\vec{v}$ of the combined positions and velocities of the particles respectively (in one, two or three spatial dimensions). The process (how this state changes over time) is fully determined by a Lagrangian (or Hamiltonian), which describe the total energy of the system (kinetic and potential). Specifically, the Lagrangian/Hamiltonian is the difference/sum of the total kinetic energy $V(\vec{x},\vec{v})$ and total potential energy $U(\vec{x},\vec{v})$ of the system, as functions of its state. There may also be an external potential term $U_\text{ext}$ which is a function of the system state and some environment state. The external potential defines the interaction between the system and the environment (e.g. collisions with container walls). Player 2 interacts with the system by modifying $U_\text{ext}$ over time.&lt;/p&gt;
&lt;p&gt;From the given Lagrangian/Hamiltonian, we can derive the &lt;em&gt;propagator&lt;/em&gt;, $\t_\Dt$, for the time interval $\Dt$. This $\t_\Dt$ is a function that propagates state through time, i.e. $\vec{x}_{t+\Dt},\vec{v}_{t+\Dt}=\t_\Dt(\vec{x}_t,\vec{v}_t)$ where $\vec{x}_t,\vec{v}_t$ is the state of the gas at time $t$, and $\vec{x}_{t+\Dt},\vec{v}_{t+\Dt}$ is the state of the gas at time $t+\Dt$. This relationship holds for all $t\in\R$ and for all $\Dt\in\R$ (including negative intervals).&lt;/p&gt;
&lt;p&gt;Note that I assume the system and its environment are time-independent, meaning that $V$, $U$ and $U_\text{ext}$ are not functions of some absolute time variable $t$. That makes the Lagrangian/Hamiltonian time-independent, and thus the propagator time-independent (it only depends on state and time interval). However, player 2 may modify $U_\text{ext}$ to be time-dependent, which results in a time-dependent propagator, i.e. $\t_\Dt$ becomes a function of the state at time $t=0$ only.&lt;/p&gt;
&lt;p&gt;If the gas starts at state $\vec{x}_0,\vec{v}_0$ at time $t=0$, then the entire trajectory of the gas through time is given by $\t_t(\vec{x}_0,\vec{v}_0)$ for all $t \in\R$.&lt;/p&gt;
&lt;p&gt;Suppose player 1 provides a freely expanding gas as the process that player 2 is tasked with reversing. Specifically, the gas starts in state $\vec{x}_0,\vec{v}_0$, occupying the small container. By time $T$, the gas occupies the large container represented by the state $\vec{x}_T,\vec{v}_T=\t_T(\vec{x}_0,\vec{v}_0)$. How does player 2 intervene on this gas starting at state $\vec{x}_T,\vec{v}_T$ to return it to state $\vec{x}_0,\vec{v}_0$ at some later time?&lt;/p&gt;
&lt;p&gt;Physics obeys time-reversal symmetry, meaning that time-inverted trajectories are physically valid. That is to say, the system traveling in reverse (but forward in time) along its historical trajectory is a valid process for it to undergo. We need only negate the velocities at some moment in time to put the system on its time-reversed trajectory. To be specific, if player 2 could somehow make it so that $\vec{v}_{T+\e}=-\vec{v}_{T}$ and $\vec{x}_{T+\e}=\vec{x}_T$ for some time-interval $\e&amp;gt;0$, then $\vec{x}_{T+\e+t},\vec{v}_{T+\e+t}=\vec{x}_{T-t},\vec{v}_{T-t}$ for all $t\geq0$. That means $\vec{x}_{2T+\e},\vec{v}_{2T+\e}=\vec{x}_{0},\vec{v}_{0}$ at which point the system is returned to its initial state.&lt;/p&gt;
&lt;p&gt;Player 2 can achieve this with micro-walls, i.e. very small, localized walls in the potential field. As I mentioned, a wall is essentially a very steep (or infinitely steep) hill in the external potential field. Walls deflect particles (are repulsive). A micro-wall takes up very little space.&lt;br&gt;


  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/microwall.jpg&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;A micro-wall is a spatially small (localized) wall in the potential field.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Assuming player 2 can create and remove these micro-walls for free (when there is no active collision between them and the particles), then player 2 can manifest these micro-walls just during the time interval from $T$ to $T+\e$. If a micro-wall is placed in front of every particle in just the right orientation, then each particle will be deflected in the opposite direction, i.e. its velocity will be negated.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../gas_reversal_1.jpg&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../gas_reversal_1.jpg&#34; 
         alt=&#34;State of the gas at time $T$. Blue lines are walls (steep thin hills) in the external potential. Orange lines are velocity vectors of the particles.&#34; width=&#34;308&#34;/&gt;&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;caption&#34;&gt;
            &lt;p&gt;State of the gas at time $T$. Blue lines are walls (steep thin hills) in the external potential. Orange lines are velocity vectors of the particles.&lt;/p&gt;
        &lt;/span&gt;
&lt;/span&gt; &lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../gas_reversal_2.jpg&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../gas_reversal_2.jpg&#34; 
         alt=&#34;Microwalls placed between time $T$ and $T&amp;#43;\e$. Gas particles are about to collide with their respective microwalls.&#34; width=&#34;300&#34;/&gt;&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;caption&#34;&gt;
            &lt;p&gt;Microwalls placed between time $T$ and $T+\e$. Gas particles are about to collide with their respective microwalls.&lt;/p&gt;
        &lt;/span&gt;
&lt;/span&gt; &lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../gas_reversal_3.jpg&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../gas_reversal_3.jpg&#34; 
         alt=&#34;State of the gas at time $T&amp;#43;\e$. Microwalls are removed.&#34; width=&#34;310&#34;/&gt;&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;caption&#34;&gt;
            &lt;p&gt;State of the gas at time $T+\e$. Microwalls are removed.&lt;/p&gt;
        &lt;/span&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since the microwalls are present between the absolute times $T$ and $T+\e$, the external potential $U_\text{ext}$ becomes time-dependent.&lt;/p&gt;
&lt;p&gt;Note that I assume that manipulations of the potential field which do not lift or lower matter, or change the gradient (slope) of the field under matter (changing the force felt by the matter, and thus changing the system&amp;rsquo;s total energy), cost no energy to perform (this is an idealization). So the manifestation and dissolution of walls and other shapes in the potential landscape can be done for free so long as there is not matter directly on top of these shapes as they are modified.&lt;/p&gt;
&lt;h2 id=&#34;the-role-of-uncertainty&#34;&gt;The Role of Uncertainty&lt;/h2&gt;
&lt;p&gt;The above example demonstrates that player 2 can reverse an isolated process given perfect knowledge of its state. Let&amp;rsquo;s see what happens if player 2 lacks state information.&lt;/p&gt;
&lt;p&gt;Suppose that player 2 does not know the initial state of the gas, but only that the gas starts off inside the smaller container (and maybe that its at equilibrium at a certain temperature). Player 2 will have to look at the state of the gas (perform a measurement) at time $T$ in order to pull the same trick as before. But like with the Szilard engine, whether player 2 can do this reversal for free hinges on whether the gas measurement can be reset for free (remember that reversal requires the environment, i.e. physical memory, to be reset as well).&lt;/p&gt;
&lt;p&gt;Note that player 2&amp;rsquo;s state uncertainty is propagated down to any device player 2 employs. For example, if player 2 builds a device that creates the micro-walls needed in the external potential to reverse the gas, how does this device know the correct placement and orientation of the micro-walls? No matter what, the way in which the external potential changes over time needs to be a function of the state of the gas, i.e. is conditional on the gas. For the external potential to be conditioned on the state of the system, some environment state needs to become correlated with the state of the system in the way described earlier: &lt;a href=&#34;#measurement&#34;&gt;#Measurement&lt;/a&gt;. This is how the intervention on the system needed to reverse it depends, fundamentally at a physical level, on information about the system&amp;rsquo;s state.&lt;/p&gt;
&lt;p&gt;Since the gas evolved deterministically according to the propagator $\t_\Dt$ (for every $\Dt\in\R$), its state information (positions and velocities of the particles) is preserved through time. Suppose the state of the gas can be recorded with a special camera (an infinite resolution camera). One photo captures the positions of all the particles, and two rapidly sequential photos to capture their velocities. As discussed, player 2 can use this information to reverse the free expansion of the gas.&lt;/p&gt;
&lt;p&gt;But player 2 might be able to do more than that. Suppose player 2 takes two rapidly sequential photos at time $T$, and stores these photos in some physical medium. Since player 2 knows the dynamics of the system (i.e. knows $\t_\Dt$), then in principle player 2 can predict what the photos taken at time $T+\e$ would look like. We might suppose player 2 can even apply $\t_{\e}$ to the photos, replacing them in memory with the output of $\t_{\e}$. This should be in principle a reversible process since $\t_{\e}$ is a bijection (no information is lost by applying this function). If we suppose that the process of taking these photos is reversible and costs no energy (like the copy process from the Szilard engine discussion), then player 2 can apply the reverse of the camera process, i.e. the &amp;ldquo;uncamera&amp;rdquo; process, to the time $T+\e$ photos in memory at time $T+\e$, which would reset the state of the photo storage to its default state at no energy cost. Player 2 is performing an uncopy.&lt;/p&gt;
&lt;h2 id=&#34;the-general-uncopy-loophole&#34;&gt;The General Uncopy Loophole&lt;/h2&gt;
&lt;p&gt;Classical mechanics allows us to consider an arbitrary system described with &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalized_coordinates&#34;target=&#34;_blank&#34;&gt;generalized coordinates&lt;/a&gt;, $\vec{q}$ and $\dot{\vec{q}}$, a vector of parameters and their time-derivatives, which fully specifies the state of the system. These parameters can be positions in some reference frame, but can represent other degrees of freedom such as rotations or distances between parts within the system. As before, a Lagrangian (or Hamiltonian) is provided that fully determines the trajectory the system takes through time given any initial state.&lt;/p&gt;
&lt;p&gt;Let $\t_\Dt$ be the propagator induced by the given Lagrangian/Hamiltonian and let $\vec{q}_t,\dot{\vec{q}}_t = \t_t(\vec{q}_0,\dot{\vec{q}}_0)$ for some initial state $\vec{q}_0,\dot{\vec{q}}_0$.  Assuming player 2&amp;rsquo;s potential field interacts with every degree of freedom in $\vec{q}$, then player 2 can construct generalized micro-walls in the potential field that has the effect of negating all of the velocity coordinates so that $\dot{\vec{q}}_{T+\e}=\dot{\vec{q}}_T$ for some time $T$. Then the system will proceed to run in reverse until it returns to its initial state $\t_t(\vec{q}_0,\dot{\vec{q}}_0)$.&lt;/p&gt;
&lt;p&gt;For a system with initial state unknown to player 2, this is the general reversal procedure:&lt;/p&gt;
&lt;p&gt;(a) The system progresses from time 0 to time $T$ (player 2 does not yet intervene).&lt;/p&gt;
&lt;p&gt;(b) At time $T$, player 2 takes two &amp;ldquo;pictures&amp;rdquo; of the states $\vec{q}_T$ and $\vec{q}_{T+\d}$ (for very small $\d&amp;gt;0$) in quick succession. These state pictures are saved to physical memory (initial in a known default state).&lt;/p&gt;
&lt;p&gt;(c) Player 2 transforms its saved pictures by applying $\t_\e$ (using their own computational or simulation apparatus), resulting in pictures of the states $\vec{q}_{T+\e}$ and $\vec{q}_{T+\e+\d}$ in memory, all done in the time span between $T$ and $T+\e$.&lt;/p&gt;
&lt;p&gt;(d) Player 2 performs the reverse of the camera process which takes the state $\vec{q}_{T+\e}$ of the gas and its duplicated representation in memory and results in the memory being returned to its default state. Same is done for the second picture of $\vec{q}_{T+\e+\d}$ a moment later.&lt;/p&gt;
&lt;p&gt;(e) Player 2 waits another time interval of $T$ for the system to return to its initial state.&lt;/p&gt;
&lt;p&gt;Call this procedure the &lt;em&gt;uncopy loophole&lt;/em&gt;. A loophole in physics is a potential way around some proposed &lt;a href=&#34;https://en.wikipedia.org/wiki/No-go_theorem&#34;target=&#34;_blank&#34;&gt;no-go theorem&lt;/a&gt; (a theorem that says you can&amp;rsquo;t do something). A famous example are the &lt;a href=&#34;https://en.wikipedia.org/wiki/Loopholes_in_Bell_tests&#34;target=&#34;_blank&#34;&gt;loopholes in Bell&amp;rsquo;s theorem&lt;/a&gt;. All these loopholes (except &lt;a href=&#34;https://en.wikipedia.org/wiki/Loopholes_in_Bell_tests#Superdeterminism_loopholehttps://en.wikipedia.org/wiki/Loopholes_in_Bell_tests#Superdeterminism_loophole&#34;target=&#34;_blank&#34;&gt;superdeterminism&lt;/a&gt;) have been &lt;em&gt;closed&lt;/em&gt;, meaning its been shown that these tricks don&amp;rsquo;t invalidate Bell&amp;rsquo;s theorem (in this case experimentally, but theoretical proof could also be sufficient).&lt;/p&gt;
&lt;p&gt;Some other entropy increasing processes which could be reversed in this way (if the loophole can&amp;rsquo;t be closed) include the &lt;a href=&#34;https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Supplemental_Modules_%28Physical_and_Theoretical_Chemistry%29/Thermodynamics/Ideal_Systems/Thermodynamics_of_Mixing&#34;target=&#34;_blank&#34;&gt;mixing of two kinds of gases&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Thermal_equilibrium&#34;target=&#34;_blank&#34;&gt;heat diffusion&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;superdemons&#34;&gt;Superdemons&lt;/h3&gt;
&lt;p&gt;What I&amp;rsquo;ve constructed is a much more beefed up version of &lt;a href=&#34;https://en.wikipedia.org/wiki/Maxwell%27s_demon&#34;target=&#34;_blank&#34;&gt;Maxwell&amp;rsquo;s demon&lt;/a&gt;. While pondering the recently formulated laws of thermodynamics, Maxwell (of &lt;a href=&#34;https://en.wikipedia.org/wiki/Maxwell%27s_equations&#34;target=&#34;_blank&#34;&gt;Maxwell&amp;rsquo;s equations&lt;/a&gt; fame) conceived of a thought experiment where a tiny &amp;ldquo;demon&amp;rdquo; selectively allows particles through a partition in a box containing a gas - specifically allowing high velocity particles through and blocking low velocity particles from passing through. As time passes, one side will be increasingly hotter than the other side. If the demon can carry out this discrimination without energy cost (or low energy cost), then the diffusion of heat between two gasses (one initially hot and one initially cold) can be reversed without energy cost, seemingly violating the 2nd law.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020220216190805.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;\&amp;#34;Schematic figure of Maxwell&amp;#39;s demon thought experiment\&amp;#34;. Source: [Wikipedia](https://en.wikipedia.org/wiki/Maxwell%27s_demon#/media/File:Maxwell&amp;#39;s_demon.svg)&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;I want a term for a very powerful player 2 that can perform any of the potential field manipulations allowed for by the game specified above, such as micro-walls (for negating velocity). Let&amp;rsquo;s call this a &lt;em&gt;superdemon&lt;/em&gt;. Think of a superdemon as a maximally adversarial Maxwellian demon which, welding an arbitrarily manipulable potential field, will pull any trick it can to reverse any process you give it without energy cost to itself.&lt;/p&gt;
&lt;h1 id=&#34;slaying-the-superdemon&#34;&gt;Slaying The Superdemon&lt;/h1&gt;
&lt;p&gt;How can the uncopy loophole be closed? What prevents it from being carried out in practice?&lt;/p&gt;
&lt;p&gt;One possible reason is that copy and uncopy are processes that take time. For chaotic and fast moving systems like gasses, the target data to be copied/uncopied is nonstationary, and changing quickly at that. On the other hand, is this even a problem in practice? There are experimental &lt;a href=&#34;https://en.wikipedia.org/wiki/Femto-photography&#34;target=&#34;_blank&#34;&gt;femtosecond cameras&lt;/a&gt; that capture images within the span of one ten-trillionths of a second ($10^{-13}$ s). Presumably that is fast enough to accurately capture the state of a room temperature gas (including velocities).&lt;/p&gt;
&lt;h2 id=&#34;copying-takes-time&#34;&gt;Copying Takes Time&lt;/h2&gt;
&lt;p&gt;In theory, the time duration of the copy and uncopy processes can be made as short as desired. The question remains - however brief the process is, because the state being copied is changing quickly, can the copy process have infinite fidelity (copy at infinite resolution)? I am not currently sure what the answer is, assuming there is no environmental noise.&lt;/p&gt;
&lt;p&gt;The processes thermodynamics is concerned about, where the question of irreversibility comes up, are likely all chaotic. For a gas, even slight changes to their state at time $t$ will result in a very large change in future states over long time spans. Then it also is the case that the smallest of errors in the reversal of a gas&amp;rsquo;s trajectory would result in the gas&amp;rsquo;s state not being reversed. That is to say, if the micro-walls created by a superdemon are even slightly off in their position and orientation, so that the velocity vectors are not perfectly negated, then the gas will not return to its initial state. That means a superdemon with finite copy and uncopy precision will likely not be able to reverse the free expansion of a gas (or the mixing of two gasses, or the diffusion of heat), because so many particle collisions have to happen exactly right for all the particles in the gas to return to their smaller container. (or for two gasses to be become unmixed, etc.)&lt;/p&gt;
&lt;p&gt;This brings me to the more pervading reason the uncopy loophole won&amp;rsquo;t work: environmental noise.&lt;/p&gt;
&lt;h2 id=&#34;environment-noise&#34;&gt;Environment noise&lt;/h2&gt;
&lt;p&gt;Environmental noise makes carrying out the uncopy loophole intractable. Why?&lt;/p&gt;
&lt;p&gt;A more realistic model of a gas includes the interaction between the gas particles and the particles of its container. This interaction makes the future states of the gas dependent, in part, on the state of those container particles, i.e. environment state. For a superdemon to reverse the trajectory of a gas, it would need to also reverse the trajectories of all the particles in its container as well. Now we&amp;rsquo;ve expanded the scope of what needs to be reversed to include the immediate environment the system interacts with. But that immediate environment also interacts with things beyond it. In practice no system is perfectly isolated. There is endless chain of systems interacting with systems so that one can never take into account enough state to perfectly reverse a gas (or any other chaotic system).&lt;/p&gt;
&lt;p&gt;Supposing the superdemon has perfect information about the state of the entire universe, but cannot alter the trajectory of the environment outside the system, can the superdemon reverse a chaotic system like a gas? Maybe it is possible to determine an intricate sequence of interventions on the system that guide it back to its reversed state given the forward-trajectory of the environment on which it depends.&lt;/p&gt;
&lt;p&gt;Now consider what happens when the superdemon does not know the state of the entire universe. The superdemon either has to measure that unknown state (by interacting), which consumes physical memory storage, or has to perform an intervention on the system that does not depend on that unknown state.&lt;/p&gt;
&lt;p&gt;If we limit the power of the superdemon to potential field manipulations that are finite (either finite in energy changes or finite in the spatial region of the change), then the superdemon cannot use the uncopy trick to reset its physical memory. That physical memory is itself interacting with more of the environment with unknown state, and the superdemon would need to take that unknown state into account to uncopy the memory, and we have an infinite regress.&lt;/p&gt;
&lt;h3 id=&#34;defining-environmental-noise&#34;&gt;Defining Environmental Noise&lt;/h3&gt;
&lt;p&gt;I define environment noise as environmental uncertainty - i.e. environment state that is unknown (i.e. unknown to player 2) - represented as a state region (the set of possible states it could take on).&lt;/p&gt;
&lt;p&gt;If the system in question starts off in a known state, as it interacts with the part of the environment which is uncertain, the system&amp;rsquo;s state will also become uncertain over time. One could think of the system&amp;rsquo;s time-evolution as being nondeterministic, in the sense that the system&amp;rsquo;s future state is a function of both the system&amp;rsquo;s current state and environmental state which is literally not determined in the model (it takes on a set of possibilities).&lt;/p&gt;
&lt;p&gt;Some example forms that environmental noise takes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Thermal motion of wall particles.&lt;/li&gt;
&lt;li&gt;Irregular shape of containing walls.&lt;/li&gt;
&lt;li&gt;Measurement error (includes time uncertainty).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;thermal-noise&#34;&gt;Thermal Noise&lt;/h3&gt;
&lt;p&gt;We could model the thermal activity of a container in the following way:&lt;br&gt;
Instead of fixed walls, we divide the container walls up into small slices. Each slice is given mass and placed on the end of a spring. A collision between this &amp;ldquo;springed wall-slice&amp;rdquo; slice and a particle disturbs the spring and the springed wall-slice will oscillate. To model the wall having thermal noise, initialize the springed wall-slices with some KE. The average KE across all the slices is the temperature of the wall.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/wall_springs.jpg&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
We can model thermal diffusion in the wall by connecting the wall-slices to their neighbors with springs as well. To simulate the container being connected to an infinite heat reservoir, we could suppose the springed wall-slices are themselves connected by springs to more masses on springs, going on forever. (or just a very large but finite number of masses on springs). Alternatively we could have &amp;ldquo;injection sites&amp;rdquo; in the wall slices where we add or subtract KE as a function of the average KE across all the slices.&lt;/p&gt;
&lt;p&gt;When gas particles collide with the springed wall-slices, they exchange KE depending on their respective velocities. The the return velocity of the particle after the collision depends on the incoming velocity of the wall-slice - something that is unknown to us. This is how the gas&amp;rsquo;s trajectory becomes more uncertain due to this thermal noise as time progresses.&lt;/p&gt;
&lt;h3 id=&#34;wall-irregularity&#34;&gt;Wall Irregularity&lt;/h3&gt;
&lt;p&gt;Shape uncertainty has a similar effect as thermal uncertainty. I want to note the difference between an irregular wall shape and an unknown wall shape.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/irregular_walls.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Set of irregularly shaped walls.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In principle if we knew the exact shape of the container of a gas, no matter how irregular, we could model the time-evolution of the gas (also in known state) deterministically. Then if the gas undergoes free expansion, a superdemon can reverse that process as described above.&lt;/p&gt;
&lt;p&gt;However, if we model the wall as a set of possible walls (each with some irregularity in shape), then the trajectory of the gas is non-deterministic (literally, depends partly on non-determined state), since it depends on the unknown orientation of the local region of the wall at collisions.&lt;/p&gt;
&lt;p&gt;This is an example of Bayesian uncertainty, where what we are uncertain about is not changing, but fixed for all time - i.e. we are not sampling a different wall on each interaction. This gives us the ability to, in principle, gain information, i.e. narrow down possibilities via observation, direct or indirect, about which wall is the case.&lt;/p&gt;
&lt;h3 id=&#34;measurement-error&#34;&gt;Measurement Error&lt;/h3&gt;
&lt;p&gt;Finally the nail in the coffin for the uncopy loophole: measurement error is inevitable and results in effectively finite measurement precision.&lt;/p&gt;
&lt;p&gt;Measurement error is another form of environmental noise - specifically due to our uncertainty about the state of the measurement apparatus (or the state of physical memory).&lt;/p&gt;
&lt;p&gt;Returning to the model of generalized measurement, the physical memory itself would realistically have thermal noise or other state uncertainty. If we are storing analog data like the image taken by a camera, the thermal noise in the storage medium creates a precision limit.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>