<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>personal on Dan&#39;s Notepad</title>
    <link>https://danabo.github.io/blog/tags/personal/</link>
    <description>Recent content in personal on Dan&#39;s Notepad</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Â©2021 Daniel Abolafia.</copyright>
    <lastBuildDate>Sun, 21 Feb 2021 12:22:08 -0600</lastBuildDate>
    
        <atom:link href="https://danabo.github.io/blog/tags/personal/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Classical vs Bayesian Reasoning</title>
      <link>https://danabo.github.io/blog/posts/classical-vs-bayesian-reasoning/</link>
      <pubDate>Wed, 24 Feb 2021 19:15:59 -0600</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/classical-vs-bayesian-reasoning/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mc{E}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\Iff}{\Leftrightarrow}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;My goal is to identify the core conceptual difference between someone who accepts &amp;ldquo;Bayesian reasoning&amp;rdquo; as a valid way to obtain knowledge about the world, vs someone who does not accept Bayesian reasoning, but does accept &amp;ldquo;classical reasoning&amp;rdquo;. By classical reasoning, I am referring to the various forms of boolean logic that have been developed, starting with Aristotillian logic, through propositional logic like that of Freige, and culminating in formal mathematics (e.g. higher order type theory). In such logics, the goal is to uniquely determine the truth values of things (such as theorems and propositions) from givens.&lt;/p&gt;
&lt;p&gt;My thesis is that the difference between Bayesian and classical reasoners comes down to how they deal with non-determined objects (e.g. if you cannot determine the truth value of something from your givens). The classical reasoner will shrug their shoulders and say &amp;ldquo;the answer cannot be determined, collect more givens or modify your definitions&amp;rdquo;. The Bayesian reasoner will regard at the proportion of self-consistent instantiations of unknowns that make the target proposition true as useful information regarding whether it is really true. That is to say, the Bayesian reasoner continues on without uniquely determined truth values, and the classical reasoner does not.&lt;/p&gt;
&lt;p&gt;This difference extends into the realms of machine learning and epistemology. Classical epistemology is interested in truth (i.e. uniquely determined quantities), whereas Bayesian epistemology is interested in degrees of certainty. In machine learning, the givens and unknowns in question are not boolean valued, but have arbitrary data types (e.g. vectors of reals). The classical learner is interested in what can be uniquely determined from data, and the Bayesian learner is interested in proportions of possibilities.&lt;/p&gt;
&lt;p&gt;This philosophical difference leads to a practical methodological difference. A classical reasoner/learner will define universes such that unknowns can be uniquely determined. Otherwise, the definitions are not useful. A Bayesian reasoner/learner will define universes such that calculating posterior probabilities are tractable. Otherwise, the definitions are not useful.&lt;/p&gt;
&lt;p&gt;A note on the definition of &lt;strong&gt;model&lt;/strong&gt;. In mathematics, a model is an instantiation of an unspecified object which satisfies given axioms. In machine learning, a model refers to a family of instantiations of free parameters, i.e. a model is the set of definitions which invoke free variables that are to be determined.&lt;/p&gt;
&lt;h1 id=&#34;propositional-logic&#34;&gt;Propositional logic&lt;/h1&gt;
&lt;h2 id=&#34;the-math-perspective&#34;&gt;The math perspective&lt;/h2&gt;
&lt;p&gt;I will be using the example in Russell and Norvig&amp;rsquo;s &lt;a href=&#34;http://aima.cs.berkeley.edu/&#34;target=&#34;_blank&#34;&gt;Artificial Intelligence: A Modern Approach&lt;/a&gt; (3rd edition), chapter 7.&lt;/p&gt;
&lt;p&gt;We are introduced to &amp;ldquo;wumpus world&amp;rdquo;, a &lt;a href=&#34;http://gridworld.info&#34;target=&#34;_blank&#34;&gt;grid world&lt;/a&gt; containing an enemy called the wumpus, death pits, and gold.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223110223.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
The relevant rules are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the agent starts in the bottom left corner&lt;/li&gt;
&lt;li&gt;the agent can only see what is contained in the cell it currently occupies&lt;/li&gt;
&lt;li&gt;the agent will detect a &amp;ldquo;breeze&amp;rdquo; if it&amp;rsquo;s cell is adjacent to a pit&lt;/li&gt;
&lt;li&gt;if the agent moves into a pit, it dies&lt;/li&gt;
&lt;li&gt;the goal is to get to the gold&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose the agent moves right:&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223110534.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
No breeze is detected in the starting cell [1,1], so the agent knows there is no pit up or to the right. In cell [2,1], there is a breeze, so the agent knows there is a pit above or to the right (or both).&lt;/p&gt;
&lt;p&gt;Each case can be defined by the following propositions:&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210223110745.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210223110745.png&#34; width=&#34;300&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
We define the following boolean variables:&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210223110852.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210223110852.png&#34; 
         alt=&#34;$P_{x,y}$ is instantiated as a different variable for each coordinate, i.e. $P_{1,1}, P_{1,2}, P_{2,1},\ldots$ are all different variables.&#34; width=&#34;500&#34;/&gt;&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;caption&#34;&gt;
            &lt;p&gt;$P_{x,y}$ is instantiated as a different variable for each coordinate, i.e. $P_{1,1}, P_{1,2}, P_{2,1},\ldots$ are all different variables.&lt;/p&gt;
        &lt;/span&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;model&lt;/strong&gt; (in the math sense) is an instantiation of these variables (in contrast to the machine learning sense where a model is the entire definition of the game and all the variables). There are 4 variables for each coordinate, and 16*4 = 64 variables in total. Thus a model can be viewed as a length 64 boolean vector. Suppose $m$ is such a vector. Then if $\a_1$ is true for $m$, we say that $m$ is a model of $\a_1$.&lt;/p&gt;
&lt;p&gt;Let $M(\a)$ be the set of all models (i.e. length 64 boolean vectors) satisfying some arbitrary sentence $\a$. Let $\beta$ be another sentence. We say that $\a$ &lt;strong&gt;entails&lt;/strong&gt; $\beta$, notated $\a \models \beta$, iff $M(\a) \subseteq M(\beta)$.&lt;/p&gt;
&lt;p&gt;Diagrammatically, we can depict the sets $M(\a_1)$ and $M(\a_2)$ (referring to the sentences above), as well as our knowledge base (KB) (i.e. what is given):&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223113244.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
In our wumpus world, the &amp;ldquo;sentences&amp;rdquo; above can be formally states as propositions, along with the rules for the game:&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223111036.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
We can prove simple things like &amp;ldquo;there is no pit in [1,2]&amp;rdquo; using the rules of logical inference. The following propositions are true in all models where $R_1,\ldots,R_5$ are true:&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223112618.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
Each $R_i$ is entailed by the proceeding $R_j$ for $j &amp;lt; i$. A sequence of such propositions resulting in a desired proposition is a proof. From $R_{10}$, we can conclude $\neg P_{1,2}$. This sequence of propositions is a proof of $\a_1$ = &amp;ldquo;there is no pit in [1,2]&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We can also verify the statement $\a_1$ is true with brute force instead of logical inference: by enumerating all models ($2^{64}$ of them), selecting the models which satisfy $R_1,\ldots,R_5$ (i.e. $M(R_1\and\ldots\and R_5)$), and then checking if $\neg P_{1,2}$ is true in all of them.&lt;/p&gt;
&lt;p&gt;Notice that we cannot prove $P_{2,2}$ or its negation $\neg P_{2,2}$ given $R_1,\ldots,R_5$. That is because some models of $R_1\and\ldots\and R_5$ are consistent with $P_{2,2}$ while others are consistent with $\neg P_{2,2}$. That is to say, the truth value of variable $P_{2,2}$ is not uniquely determined by the givens $R_1,\ldots,R_5$. Speaking informally, we do not have enough information to know whether there is a pit at [2,2]. Therefore a rational agent would not make decisions based on $P_{2,2}$ being true or false. This is a core tenet of classical logic, whereas we shall see later, a Bayesian reasoner might be able to do more with the same information.&lt;/p&gt;
&lt;h2 id=&#34;the-machine-learning-perspective&#34;&gt;The machine learning perspective&lt;/h2&gt;
&lt;p&gt;In machine learning, a &lt;strong&gt;model&lt;/strong&gt; is a function $f : \O \to \X$, where $\O$ is called the state set, and $\X$ is called the observation set. This is different from the math notion of a model we saw above.&lt;/p&gt;
&lt;p&gt;Typically $\O$ and $\X$ are each the Cartesian products of other primitive types (which are sets, e.g. the reals, the integers, the booleans, etc.). Thus elements of $\O$ and $\X$ are typically tuples. Each element of the tuple is called a dimension. Generally, $\O$ and $\X$ are high-dimensional (elements of $\O$ and $\X$ are very long tuples, possibly infinite).&lt;/p&gt;
&lt;p&gt;An element $\o \in \O$ is called a state, and is considered to be a possible state of the world, where the world is the model. Often, $\O = \T\m\E$, where $\T$ is called the parameter set, and $\E$ is the noise set (both sets are themselves multi-dimensional). In the typical ML formulation, $\t\in\T$ is explicitly represented but $\e\in\E$ is not, because $\t$ is what is being &amp;ldquo;solved for&amp;rdquo; while $\e$ represents random inputs. In my formulation, I combine both into a single state $\o = (\t,\e)$. If $\o$ is some tuple, then a subtuple of $\o$ is called a substate, so $\t$ and $\e$ are substates of $\o = (\t, \e)$.&lt;/p&gt;
&lt;p&gt;An element $x \in \X$ is called an observation. Usually we are only given a partial observation. If $\X = T_1 \m T_2 \m T_3 \m \dots$ for primitive types $T_1, T_2, T_3, \ldots$, then a full observation is $x = (t_1, t_2, t_3, \ldots) \in \X$. A partial observation is a subset of elements in the tuple $x$. We denote partial observations with subscripts: $x_{1,5,10}$ is the tuple $(t_1, t_5, t_{10})$. We can also take slices: $x_{a:b} = (t_a, t_{a+1}, \ldots, t_{b-1}, t_b)$ is the slice from index $a$ to $b$. The shorthand $x_{&amp;gt;a}$ is the tuple of all indices larger than $a$, and $x_{&amp;lt;a}$ is the tuple of all indices less than $a$. It is sometimes convenient to think of a partial observation $x_I$ (for index tuple $I$) as a subset of $\X$, i.e. the set of all $x\in\X$ satisfying the partial observation. For example, $x_{1,5,10} = \set{(t_1, t_2, \ldots)\in\X \mid t_1 = x_1 \and t_5 = x_5 \and t_{10} = x_{10}}$. Let $x_{a:b}`x_{x:d} = x_{a:b,c:d}$ denote tuple concatenation.&lt;/p&gt;
&lt;p&gt;The goal of machine learning is to determine an unobserved partial observation  from an observed partial observation. If the unobserved part is going to be observed in the future, we call this prediction (if it happened in the past, we call this retrodiction). If the unobserved part is atemporal, or never observed, we call this inference. An unobservable partial observation is called latent.&lt;/p&gt;
&lt;h3 id=&#34;unsupervised-learning&#34;&gt;Unsupervised learning&lt;/h3&gt;
&lt;p&gt;The observation space is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\X = \Xi_1\m\Xi_2\m\dots\m\Xi_i\m\dots&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\x_i \in \Xi_i$ is called an &lt;em&gt;example&lt;/em&gt; (I am using &amp;ldquo;xi&amp;rdquo;, $\x$, instead of $x$ since $x$ already denotes a full observation).&lt;/p&gt;
&lt;p&gt;We are given a partial observation $D$, called the dataset:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
D = (\x_1, \x_2,\x_3, \ldots, \x_n)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Given model $f : \O \to \X$ and dataset $D$ (regarding $D$ as a subset of $\X$), then $\inv{f}(D)$ is the set of all states in $\O$ compatible with $D$.&lt;/p&gt;
&lt;p&gt;Typically in machine learning, the dataset $D$ does not uniquely determine a state $\o\in\O$. To further narrow down the possibilities, additional constraints are added. Typically, $\O = \T\m\E$, where the $\T$ component of the state tuple is narrowed down further by maximizing the data probability $p_\t(D)$ w.r.t. $\t\in\T$, and $\e \sim p_\t(D)$ is randomly chosen from the distribution. Additionally, the maximization of $p_\t(D)$ may be &amp;ldquo;regularized&amp;rdquo; by jointly minimizing some real-valued function $L(\t)$. Even then, $\t\in\T$ may not be uniquely determined (as is the case in deep learning), and so $\t$ will be arbitrarily chosen from the remaining possibilities.&lt;/p&gt;
&lt;p&gt;In the case of unsupervised learning, $f$ is called a generative model, and we use it to generate unobserved examples. Assume we&amp;rsquo;ve narrowed down the possibility space $\O$ to one state $\o^*$. We simply looking at&lt;/p&gt;
&lt;p&gt;$$f(\o^*) = (\x_1, \x_2, \ldots, \x_n, \x_{n+1}, \x_{n+2}, \dots)$$&lt;/p&gt;
&lt;p&gt;which provides $\x_{n+1}, \x_{n+2}, \dots$ outside of the partial observation $D$. Note that $\o$ typically contains a choice of noise $\e\in\E$, which injects randomness into the generated examples (generative models are normally thought of as probability distributions, and generating examples is a process of sampling from $p_{\t^*}(\x_i)$ for chosen parameter $\t^*$).&lt;/p&gt;
&lt;p&gt;Because $D$ does not uniquely determine an input $\o$ to $f$ (i.e. $\inv{f}(D)$ is not singleton), but a particular $\o^* \in \inv{f}(D)$ is chosen anyway, this results in some difficulties. Some $\o$ will produce generated examples $\x_{n+1}, \x_{n+2}, \dots$ which &amp;ldquo;look like the examples in $D$&amp;rdquo; where other choices of $\o$ (still compatible with $D$) will not. We say that $f(\o)$ generalizes if it outputs the unobserved partial observation that humans consider to be correct or appropriate (e.g. looks like the data in $D$). This is all very subjective, and it is very difficult to provide appropriate constraints (like the ones I mentioned above) so that for all possible $D$, the resulting $\o_D^*$ generalizes (i.e. many humans agree that $\x_{n+1}, \x_{n+2}, \dots$ &amp;ldquo;look like&amp;rdquo; $D$).&lt;/p&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;In general, the classically rational agent only regards uniquely determined partial observables (output dimensions on it&amp;rsquo;s ML-model $f$) as knowledge, and does not act on undetermined partial observables. In contrast, the Bayesian rational agent takes the relative proportion of possible states that produce each outcome as knowledge.&lt;/p&gt;
&lt;p&gt;How does the Bayesian agent pull this off? Do these &amp;ldquo;Bayesian&amp;rdquo; probabilities really constitute knowledge? We can turn the question around and ask if uniquely determined observables really constitute knowledge. It is rare for something to be uniquely determined in practice. I suspect that many of the difficulties encountered in applications of statistical inference and machine learning are because of this. A classical reasoner needs to make simplifications and assumptions in service of being able to then uniquely determine something of interest. It seems to me that most of informal rational thought comes down to some version of choosing an ML-model s.t. something of interest can be uniquely determined. Described in this way, classical reasoning sounds biased. On the flip side, in practice Bayesian ML-models requires special simplifications and assumptions to be computationally tractable, so a similar sort of ML-model-choosing bias occurs.&lt;/p&gt;
&lt;p&gt;Note that the size of $\O$ and the ML-model $f$ determine how many states produce each partial observation. Ostensibly these things are arbitrarily chosen by the agent. The classical reasoner objects that state-counts (probabilities) don&amp;rsquo;t constitute actual knowledge about the world, but are an artifact of the choice of ML-model $f$, and so it is inappropriate to treat these quantities as knowledge. The Bayesian reasoner counters that a classical ML-model (e.g. boolean logic) is also arbitrarily chosen. Unique determination is a property of $f$, not reality, and depends on the arbitrary simplifications and assumptions made by the agent. Thus, the Bayesian reasoner concludes, my approach is no less rational than yours.&lt;/p&gt;
&lt;p&gt;The classical reasoner would counter that the parts of the ML-model output which are observed can be arbitrarily inspected for &amp;ldquo;goodness of fit&amp;rdquo; to reality. Unique determination is much less fragile (i.e. stable w.r.t. modeling inaccuracies) than state-counts. Unique determination is robust against worst-case modeling errors (though in practice this is clearly not true).&lt;/p&gt;
&lt;h1 id=&#34;the-bayesian-axiom&#34;&gt;The Bayesian Axiom&lt;/h1&gt;
&lt;p&gt;This epistemological debate is still raging. The efficacy of classical reasoning has been argued about for the last two and a half millennia. Bayesian reasoning is a more modern invention that, depending on how you count it, has been going on for 100 to 300 years (early 20th century Bayesians to Thomas Bayes). The frontier of contemporary inquery is the complex: from brains to human societies to high-dimensional physical systems, the neat and orderly unique-determination of classical reasoning is hard to come by. For this reason, Bayesian reasoning is gaining traction and is posturing to topple classical reasoning as the common-sense epistemological default.&lt;/p&gt;
&lt;p&gt;Given the unsettled nature of these questions, it is my opinion that the &amp;ldquo;state-counts as knowledge&amp;rdquo; premise be taken as the &amp;ldquo;Bayesian axiom&amp;rdquo;. This neatly delineates classical and Bayesian epistemology down to one difference: Bayesian epistemology is classical epistemology plus an additional axiom. Some may accept this axiom and other&amp;rsquo;s may reject it, leading to different kinds of reasoning and knowledge. I don&amp;rsquo;t know if we will ever be able to determine that this axiom should or should not be used. In that case, like &lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_postulate&#34;target=&#34;_blank&#34;&gt;Euclid&amp;rsquo;s fifth axiom&lt;/a&gt;, &amp;ldquo;flat&amp;rdquo; and &amp;ldquo;curved&amp;rdquo; rationality shall forever remain parallel self-consistent options.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Active inference tutorial (actions)</title>
      <link>https://danabo.github.io/blog/posts/active-inference-tutorial-actions/</link>
      <pubDate>Wed, 24 Feb 2021 05:34:36 -0600</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/active-inference-tutorial-actions/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\kl}[2]{D_{KL}\left(#1\ \| \ #2\right)}&lt;br&gt;
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\ }&lt;br&gt;
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}}\ }&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\atup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\r}{\rho}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Previous attempts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In 



  &lt;a href=&#34;https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/&#34;&gt;Free Energy Principle 1st Pass&lt;/a&gt;


, I used a &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt; to try to understand the free energy formalism. I figured out the &amp;ldquo;timeless&amp;rdquo; and actionless case, but I became confused when actions and time were added.&lt;/li&gt;
&lt;li&gt;In 



  &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;Free energy principle and Solomonoff induction&lt;/a&gt;


, I tried to translate between the formalism presented in &lt;a href=&#34;https://danijar.com/apd/&#34;&gt;https://danijar.com/apd/&lt;/a&gt; (which is a deep learning collaboration between  Danijar Hafner and Karl Friston) and the tutorial &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt;. I also tried to make the connection to Solomonoff induction.&lt;/li&gt;
&lt;li&gt;In 



  &lt;a href=&#34;https://danabo.github.io/blog/posts/varational-solomonoff-induction/&#34;&gt;Varational Solomonoff Induction&lt;/a&gt;


, I thought about whether free energy (as variational inference) could be applied to deep program synthesis to approximate Solomonoff induction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the same tutorial as before, &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;, I will go through the free energy formalism again and try to work out time and actions.&lt;/p&gt;
&lt;h1 id=&#34;review&#34;&gt;Review&lt;/h1&gt;
&lt;p&gt;To recap what I figured out in 



  &lt;a href=&#34;https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/&#34;&gt;Free Energy Principle 1st Pass&lt;/a&gt;


:&lt;/p&gt;
&lt;p&gt;Suppose $o\in\mc{O}$ is the observation space and $s\in\mc{S}$ is the hypothesis/state space (to use the notation of the &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt;). For now, let&amp;rsquo;s assume that $\mc{S}$ is the hidden state space of the environment in some timestep. $p(o,s)$ is the agent&amp;rsquo;s model of the environment, relating observations to hidden states, and the prior probability of the environment being in any particular hidden state. Let&amp;rsquo;s also ignore questions about the meaning of these probabilities (objective or subjective) and where they come from. If it&amp;rsquo;s easier to think about, assume these probabilities are subjective.&lt;/p&gt;
&lt;p&gt;If $o$ is observed (one timestep only), then we want to calculate the posterior probability $p(s\mid o)$ of each $s\in\mc{S}$. If this is intractable to do, we can instead find an approximation $q_o(s)$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
q_o := \argmin{q \in \mc{Q}} \kl{q(s)}{p(s\mid o)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\mc{Q}$ is some space of distributions that you choose $q_o$ from. Presumably $\mc{Q}$ is restricted somehow, otherwise the solution is $q=p$ in which case you are doing exact Bayesian inference.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\kl{q(s)}{p(s\mid o)} &amp;amp;= \E_{s\sim q}\left[\lg\frac{q(s)}{p(s\mid o)}\right] \\&lt;br&gt;
&amp;amp;= \E_{s\sim q}\left[\lg\frac{q(s)}{p(s)}\right] + \E_{s\sim q}\left[\lg \frac{1}{p(o \mid s)}\right] + \E_{s\sim q}[p(o)]\\&lt;br&gt;
&amp;amp;= \kl{q(s)}{p(s)} - \E_{s\sim q}\left[\lg p(o \mid s)\right] - \lg\frac{1}{p(o)} \\&lt;br&gt;
&amp;amp;= \mc{F}[q] - \lg\frac{1}{p(o)}\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= \kl{q(s)}{p(s)} - \E_{s\sim q}[\lg p(o \mid s)]\\&lt;br&gt;
&amp;amp;= \E_{s\sim q}\left[\lg\frac{q(s)}{p(s,o)}\right]&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is called &lt;strong&gt;variational free energy&lt;/strong&gt;. $\kl{q(s)}{p(s)}$ is called &lt;strong&gt;accuracy&lt;/strong&gt; and $\E_{s\sim q}[\lg p(o \mid s)]$ is called &lt;strong&gt;complexity&lt;/strong&gt;. $\lg\frac{1}{p(o)}$ is called &lt;strong&gt;surprise&lt;/strong&gt; (self-information of the observation $o$).&lt;/p&gt;
&lt;p&gt;Minimizing $\mc{F}[q]$ w.r.t. $q$ minimizes $\kl{q(s)}{p(s\mid o)}$. The surprise $\lg\frac{1}{p(o)}$ is constant w.r.t. this minimization. (Remember this is all assuming $o$ is given and fixed.)&lt;/p&gt;
&lt;h1 id=&#34;new-stuff&#34;&gt;New stuff&lt;/h1&gt;
&lt;p&gt;In the &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt;, we have free energy defined just as in my notes, except that everything now depends on policy $\pi$:&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210207121048.png&#34; alt=&#34;&#34;&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210207121055.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
The policy $\pi$ is a probability distribution over actions (, e.g. $\pi(a_t \mid o_{1:t}, a_{1:t-1})$. More on that later.&lt;/p&gt;
&lt;h2 id=&#34;interaction-loop&#34;&gt;Interaction loop&lt;/h2&gt;
&lt;p&gt;It is not clear to me whether $o,s$ are sequences over time, or just one time-step. In 



  &lt;a href=&#34;https://danabo.github.io/blog/posts/varational-solomonoff-induction/&#34;&gt;Varational Solomonoff Induction&lt;/a&gt;


 I showed how to interpret $s$ as a hypothesis that explains an infinite sequence of observations, i.e. $s$ is not a sequence but $o$ is. When I write $o_{1:\infty}$, that is an observation sequence over time. When I write $s_{1:\infty}$ that is a state sequence over time. I&amp;rsquo;ll use $h$ later to denote a time-less hypothesis on observation sequences $o_{1:\infty}$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s unpack the agent-environment interaction loop. Given policy $\pi$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(o_{1:\infty},s_{1:\infty}\mid\pi) = \sum_{a_{1:\infty}}\prod_t p(o_t,s_t\mid a_t,s_{t-1})\pi(a_t\mid o_{1:t-1},a_{1:t-1})&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $p(o_t,s_t\mid a_t,s_{t-1})$ is the probability of next observation and hidden state given input action $a_t$ and previous state, and $\pi(a_t\mid o_{1:t-1},a_{1:t-1})$ is the probability of the agent taking action $a_t$ given its entire history $o_{1:t-1},a_{1:t-1}$ (alternative we can give the agent internal state and condition on that).&lt;/p&gt;
&lt;p&gt;On the other hand, if $s$ (or $h$) is a time-less hypothesis: then we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(o_{1:\infty},h\mid\pi) = \sum_{a_{1:\infty}}\prod_t p(o_t\mid a_{1:t},o_{1:t-1},h)p(h)\pi(a_t\mid o_{1:t-1},a_{1:t-1})&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $p(h)$ is the prior on hypothesis $h$.&lt;/p&gt;
&lt;p&gt;Below, I will leave it ambiguous whether $s$ is a sequence of states or a time-less hypothesis. The math should be the same either way.&lt;/p&gt;
&lt;h2 id=&#34;active-inference&#34;&gt;Active inference&lt;/h2&gt;
&lt;p&gt;How are actions chosen? This is the big question I could not penetrate in my previous attempts. From the tutorial:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When inferring optimal actions, however, one cannot simply consider current observations. This is because actions are chosen to bring about preferred future observations. This means that, to infer optimal actions, a model must predict sequences of future states and observations for each possible policy, and then calculate the expected free energy (ð¸ð¹ð¸) of those different sequences of future states and observations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is talking about taking an expectation over future states and observations. Let&amp;rsquo;s assume $p(o_{1:\infty}, s_{1:\infty} \mid \pi)$ is the true environment dynamics. We are introduced to a new term $q(o_{1:\infty}\mid\pi)$ which are the agent&amp;rsquo;s observation preferences over time given a particular policy.&lt;/p&gt;
&lt;p&gt;The text is saying we want to choose policy $\pi$ to maximize $G(\pi)$. What&amp;rsquo;s troubling is that there is another term, $p(\pi)$, a prior over policies. If we are choosing policies freely, what does this prior represent?&lt;/p&gt;
&lt;p&gt;The text says that the preferred policy also minimizes free energy:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Since âpreferredâ here formally translates to âexpected by the modelâ, then the policy expected to produce preferred observations will be the one that maximizes the accuracy of the model (and hence minimizes ð¸ð¹ð¸).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;exact-inference&#34;&gt;Exact inference&lt;/h2&gt;
&lt;p&gt;To simplify things, let&amp;rsquo;s suppose the agent can do perfect Bayesian inference, so that $q_o(s\mid\pi) = p(s \mid o,\pi)$. Let&amp;rsquo;s see what happens if we plug in $p(s\mid o,\pi)$ for $q_o(s\mid \pi)$ in our free energy definition:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mc{F} = \E_{s \sim p(s\mid o,\pi)}\left[\lg \frac{p(s\mid o,\pi)}{p(s,o\mid\pi)}\right] = \E_{s \sim p(s\mid o,\pi)}\left[\lg \frac{1}{p(o\mid \pi)}\right] = \lg \frac{1}{p(o\mid \pi)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is just the surprise (i.e. self-information due to observing $o$). Minimizing free energy means choosing $\pi$ to maximize the data likelihood:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\pi^* := \argmax{\pi} p(o\mid\pi)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Remember that $\mc{F}$ depends on a fixed $o$, which is what has already been observed. If $o$ is not observed, then we are talking about future $o$, and we need to take an expectation w.r.t. $o$, e.g.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\pi^* := \argmin{\pi} \E_{o\sim p(o\mid\pi)}\lg\frac{1}{p(o\mid\pi)} = \argmin{\pi}\mb{H}[p(o\mid\pi)]&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is saying, choose a policy s.t. the future is as predictable as possible, i.e. minimizes entropy over observations, i.e. minimizes future expected surprise.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s introduce the agent&amp;rsquo;s preferences, encoded as a distribution on observations. The tutorial (and other texts) use $q(o)$, but I&amp;rsquo;m using $\r(o)$, because this is a very different thing from the approximate posterior $q_o$. Specifically $\r(o)$ is given and held fixed, while $q_o(s)$ depends on the particular observation $o$, as well as choice of optimization space $\mc{Q}$. In short, $q_o(s)$ is an output, while $\rho(o)$ is given.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s replace $p(o\mid\pi)$ with $\rho(o)$ (this should not depend on $\pi$). So instead of taking an expectation w.r.t. the model probabilities for $o$, we are taking an average weighted by preference for $o$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\pi^* &amp;amp;:= \argmin{\pi} \E_{o\sim \r(o)}\lg\frac{1}{p(o\mid\pi)} \\&amp;amp;= \argmin{\pi} H(\r(o), p(o\mid\pi)) \\&amp;amp;= \argmin{\pi} \left\{\kl{\r(o)}{p(o\mid\pi)} + \mb{H}[\r(o)]\right\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross_entropy&#34;target=&#34;_blank&#34;&gt;cross-entropy&lt;/a&gt; of $q(o)$ and $p(o\mid\pi)$ (average number of bits if you encode a stream $o_{1:\infty}$ under $p$ while actually sampling from $p$). Since $\r(o)$ is fixed, then we are minimizing $\kl{\r(o)}{p(o\mid\pi)}$. That is to say, choose policy (thereby choosing actions) that make the environment (as the agent believes it to be) dynamics $p(o\mid\pi)$ conform to preferences $\r(o)$.&lt;/p&gt;
&lt;h3 id=&#34;reward-equivalence&#34;&gt;Reward equivalence&lt;/h3&gt;
&lt;p&gt;According to  &lt;a href=&#34;https://danijar.com/apd/,&#34;&gt;https://danijar.com/apd/,&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\r(o_{1:n}) \propto \exp(r(o_{1:n}))&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $r(o_{1:n})$ is the total reward received for observations $o_{1:n}$. Written another way, $\r(o_{1:n}) = \exp(r(o_{1:n}))/\mc{Z}$ for some normalization constant $\mc{Z}$, so then $r(o_{1:n}) = \ln\r(o_{1:n}) + \mc{C}$ for some constant offset $\mc{C}$.&lt;/p&gt;
&lt;p&gt;If we are just trying to maximize expected total reward $r(o_{1:n})$ w.r.t. the environment model, we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\pi^* &amp;amp;:= \argmax{\pi} \E_{o \sim p(o \mid \pi)} [r(o_{1:n})] \\&lt;br&gt;
&amp;amp;= \argmax{\pi} \E_{o_{1:n},a_{1:n} \sim p(o_{1:n},a_{1:n} \mid \pi)} [\ln\r(o_{1:n})]\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;So far, I am not seeing anything conceptually new here. Storing agent preferences in a probability distribution $\r(o)$ is not really any different from storing preferences in a reward $r(o)$, and the two are easily converted into each other.&lt;/p&gt;
&lt;h2 id=&#34;putting-it-all-together&#34;&gt;Putting it all together&lt;/h2&gt;
&lt;p&gt;Now let&amp;rsquo;s suppose that $o$ has not yet been observed as before, but use approximate (future) posterior $q_{o,\pi}(s)$ and compute expected future free energy under preference $\rho(o)$.&lt;/p&gt;
&lt;p&gt;We get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
G[\pi]&amp;amp;=\E_\rho[\mc{F}[o,\pi]] \\&lt;br&gt;
&amp;amp;= \E_{o\sim\rho}\E_{s \sim q_{o,\pi}}\left[\lg\frac{q_{o,\pi}(s)}{p(s,o\mid\pi)}\right] \\&lt;br&gt;
&amp;amp;= \E_{o\sim\rho}\kl{q_{o,\pi}(s)}{p(s\mid\pi)} - \E_{o\sim\rho}\E_{s\sim q_{o,\pi}}\left[\lg p(o \mid s,\pi)\right]&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $q_{o,\pi}$ is the optimal approximate posterior for the given observation $o$ and policy $\pi$ used to obtain $o$. From the perspective of $q_{o,\pi}$, $o$ is already observed using policy $\pi$ which determines the probability of that observation.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
q_{o,\pi} := \argmin{q} \mc{F}[o,\pi] = \argmin{q}\E_{s \sim q}\left[\lg\frac{q(s)}{p(s,o\mid\pi)}\right]&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;I believe the tutorial paper has a typo, where $p(o,s,\pi)$ should be $p(o,s,\mid\pi)$.&lt;/p&gt;
&lt;p&gt;We are choosing $\pi$ to minimize $G[\pi]$, which is just the expected free energy under $\rho(o)$ (preference for future observations).&lt;/p&gt;
&lt;p&gt;Do the optimizations on $\pi$ and $q$ interact? It seems like they don&amp;rsquo;t. $\pi$ is an outer optimization that depends on running the optimization on $q$ internally. There is not a single $q$, but many of them which the optimization on $\pi$ iterates through. So then what is the significance of connecting free energy minimization ($q$) to active inference ($\pi$)? If the policy optimization part were reformulated in terms of RL, we really just have a fancy kind of approximate Bayesian model combined with RL. The action learning and model updating are totally independent.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://arxiv.org/abs/1911.10601&#34;&gt;https://arxiv.org/abs/1911.10601&lt;/a&gt; for a discussion about the equivalence between variational approaches to RL and active inference.&lt;/p&gt;
&lt;h1 id=&#34;the-meaning-of-p&#34;&gt;The meaning of $p$&lt;/h1&gt;
&lt;p&gt;If $p(o_{1:\infty},s_{1:\infty}\mid\pi)$ is a dynamics model of the environment, how would the agent know it? Or alternatively, how are different hypotheses for environment dynamics handled in this framework?&lt;/p&gt;
&lt;p&gt;The two cases are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$p(o_{1:\infty},s_{1:\infty}\mid\pi)$ is the literal true dynamics of the environment.&lt;/li&gt;
&lt;li&gt;$p(o_{1:\infty},s_{1:\infty}\mid\pi)$ is the agent&amp;rsquo;s dynamics model of the environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first case is unreasonable, because we cannot assume any agent knows the truth. The second case does not allow the agent to update its ontology, i.e. change the state space $\mc{S}$ and it&amp;rsquo;s beliefs about how observations interact with, $p(o\mid s)$ and $p(s\mid o)$.&lt;/p&gt;
&lt;p&gt;We could suppose there is a latent $h$ for the environment hypothesis which is being marginlized, e.g. $p(o_{1:n},s_{1:n},h)$, but then $p(o_{1:n}\mid s_{1:n}) = \E_{h\sim p(h)}p(o_{1:n}\mid s_{1:n},h)$, which we can generally expect to be intractable but is required for free energy calculation. The free energy approximation was supposed to be tractable. Now do we have to approximate the approximation?&lt;/p&gt;
&lt;p&gt;I think the time-less hypothesis formulation is better, i.e. $p(o_{1:\infty}, h)$, because it allows the hypothesis to invent its own states (because states are no longer explicitly defined), and put emphasis on not just the present, but possible states in the past and future, i.e. the agent may be more interested in inferring past or future states. Furthermore, states may not be well defined things. I have a model of the world filled with all sorts of objects, each having independent states until they interact. I cannot comprehend thinking of everything I know as one gigantic state.&lt;/p&gt;
&lt;p&gt;Something I&amp;rsquo;ve heard hinted at elsewhere is that the agent, as a physical system, expresses some Bayesian prior $p$ and preferences $\r$ in an objective sense. What is the nature of this mapping between physical makeup and active-inference description? Is this entirely based on the agent&amp;rsquo;s behavior, or if we looked inside an agent, we could determine its model and preferences? I expect that if we look at behavior alone, then $p$ and $\r$ are underspecified.&lt;/p&gt;
&lt;p&gt;So then what about the agent&amp;rsquo;s physical makeup gives it a model $p$ and preferences $\r$? The optimization process to find $q_{o,\pi}$ must be physically carried out, and so presumably this could be observed. In optimizing for $q_{o,\pi}$, the agent would actually be engaged in two processes that implicitly specify $p$. Splitting free energy into accuracy and complexity:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Explaining observations: $\E_{h\sim q}\left[\lg p(o_{1:n} \mid h,a_{1:n})\right]$&lt;br&gt;
The agent thinks of hypotheses $h$ (sampling them from $q$) to explain observations $o_{1:n}$ given actions $a_{1:n}$.&lt;/li&gt;
&lt;li&gt;Regularization: $\kl{q(h)}{p(h)}$&lt;br&gt;
The agent updates its hypothesis generator $q$, implicitly conforming to $p$ which represents the agent&amp;rsquo;s grand total representation capacity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under this perspective, the agent&amp;rsquo;s ability to modify its own hypothesis generator $q(h)$ is somehow described by $p(h)$, which is fixed throughout the agent&amp;rsquo;s lifetime (unless the agent can self-modify). For a particular hypothesis $h$, the data probability $p(o_{1:n}\mid h)$ is the likelihood of the data under $h$. So $p$ is simultaneously encoding the agent&amp;rsquo;s theoretical capacity to generate hypotheses (which it never fully reaches because of limitations on $q(h)$) and the meaning of every hypothesis it can come up with. It is unclear to me whether $p(h, o_{1:n})$ can be uniquely determined given an agent&amp;rsquo;s physical makeup.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m also unconvinced about the way behavior is handled in this framework. Why think in terms of policies $\pi$ rather than actions $a_{1:n}$? Is the space of policies fixed through the agent&amp;rsquo;s lifetime? If $\pi$ is supposed to represent some kind of high level strategy, then how does the agent learn different kinds of strategies (updating its ontology). This is the same problem that Bayesian inference faces, that $q$ ostensibly fixes. But now we need to fix the same problem again for $\pi$.&lt;/p&gt;
&lt;p&gt;Question: $G[\pi]$ appears to be intractable to compute or optimize directly. Why do we not have a variational approximation to this as well?&lt;/p&gt;
&lt;p&gt;Why not just do RL? What is gained by &amp;ldquo;active inference&amp;rdquo;, which seems to me to be secretly RL on top of variational Bayes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How this blog works</title>
      <link>https://danabo.github.io/blog/posts/how-this-blog-works/</link>
      <pubDate>Sun, 21 Feb 2021 12:22:08 -0600</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/how-this-blog-works/</guid>
      <description>&lt;p&gt;This blog is a window into my second brain. That is where I store all of my personal notes, ranging from journal entries to productive materials like study notes and math problems. I can mark any of these items for publication on my blog, and I have a script take care of the rest.&lt;/p&gt;
&lt;h1 id=&#34;second-brain&#34;&gt;Second brain&lt;/h1&gt;
&lt;p&gt;I use the &lt;a href=&#34;https://obsidian.md/&#34;target=&#34;_blank&#34;&gt;Obsidian&lt;/a&gt; editor to organize my second brain. It looks like this:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210206135929.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Writing some blog posts in Obsidian.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Every note is a markdown file, with support for extra features like &lt;a href=&#34;https://www.mathjax.org/&#34;target=&#34;_blank&#34;&gt;MathJax&lt;/a&gt; (latex math mode), and some of the features in &lt;a href=&#34;https://roamresearch.com/&#34;target=&#34;_blank&#34;&gt;Roam&lt;/a&gt;, namely &lt;a href=&#34;https://publish.obsidian.md/help/How&amp;#43;to/Format&amp;#43;your&amp;#43;notes&#34;target=&#34;_blank&#34;&gt;wiki-style internal links&lt;/a&gt;. So &lt;code&gt;[[Blogging experiment]]&lt;/code&gt; becomes 



  &lt;a href=&#34;https://danabo.github.io/blog/posts/blogging-experiment/&#34;&gt;Blogging experiment&lt;/a&gt;


. It has some convenient features like being able to paste images from my clipboard and auto-generate an image embed command:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210206140636.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Previous screenshot I pasted into this very markdown file.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There is also a nifty graph view of all my notes:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210206140906.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Looks like a constellation! Each node is a markdown file. Each edge is due to one file linking to another with `[[...]]`. I&amp;#39;m not yet sure how helpful this is, but it&amp;#39;s nice to have a way to look at everything at once, in lieu of a traditional hierarchical structure.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;publishing-flow&#34;&gt;Publishing flow&lt;/h1&gt;
&lt;p&gt;My blog is written in &lt;a href=&#34;https://gohugo.io/&#34;target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt; (for now), which is a static site generator that takes markdown files as input. Ideally, I could just run hugo directly on my Obsidian directory, but (1) I don&amp;rsquo;t want to publish everything and (2) Obsidian defines its own extended markdown syntax, as I explained above. My workaround is to have a script copy and transform my Obsidian notes marked for publication. Here&amp;rsquo;s how I do it&amp;hellip;&lt;/p&gt;
&lt;p&gt;Markdown files can optionally have a frontmatter, which is a yaml header at the top of the page. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Hello World&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;author&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;John Doe&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In any note in my second brain (Obsidian), I can set &lt;code&gt;blog: true&lt;/code&gt; and that note will be published on my blog.&lt;/p&gt;
&lt;p&gt;Every so often, I run &lt;a href=&#34;https://github.com/danabo/blog/blob/master/publish.sh&#34;target=&#34;_blank&#34;&gt;publish.sh&lt;/a&gt; from the blog repo, which in turn runs &lt;a href=&#34;https://github.com/danabo/blog/blob/master/blog.py&#34;target=&#34;_blank&#34;&gt;blog.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/danabo/blog/blob/master/blog.py&#34;target=&#34;_blank&#34;&gt;blog.py&lt;/a&gt; is where the magic happens. It&amp;rsquo;s a Python script that uses &lt;a href=&#34;https://github.com/google/python-fire&#34;target=&#34;_blank&#34;&gt;fire&lt;/a&gt; to give it a &lt;a href=&#34;https://en.wikipedia.org/wiki/Command-line_interface&#34;target=&#34;_blank&#34;&gt;CLI&lt;/a&gt;. It will go through all the markdown files in my second brain directory and look for the ones with frontmatter containing &lt;code&gt;blog: true&lt;/code&gt;. For those files, it will do a few transformations, like converting internal links, &lt;code&gt;[[...]]&lt;/code&gt; and Obsidian&amp;rsquo;s image command &lt;code&gt;![[...]]&lt;/code&gt; to regular markdown. It also scrubs markdown comments, &lt;code&gt;&amp;lt;!-- ... --&amp;gt;&lt;/code&gt;, and anything inside a &lt;code&gt;&amp;lt;!-- hide --&amp;gt;...&amp;lt;!-- endhide --&amp;gt;&lt;/code&gt; pair so that I can have private sections inside published notes.&lt;/p&gt;
&lt;p&gt;Code to transform Obsidian markdown to &lt;a href=&#34;https://gohugo.io/&#34;target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt; markdown:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;transform_body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# Remove local-only blocks&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;(&amp;lt;!--\s*hide\s*--&amp;gt;.*&amp;lt;!--\s*endhide\s*--&amp;gt;)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DOTALL&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MULTILINE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;# Remove everything after unclosed `&amp;lt;!-- hide --&amp;gt;`&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;(&amp;lt;!--\s*hide\s*--&amp;gt;.*)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DOTALL&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MULTILINE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;# Remove comments&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# https://stackoverflow.com/a/28208465&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;(&amp;lt;!--.*?--&amp;gt;)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DOTALL&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MULTILINE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;# Transform internal links (wiki-style links).&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# https://gohugo.io/content-management/cross-references/#use-ref-and-relref&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;([^!])\[\[(.*?)\]\]&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;\1\{\{\&amp;lt; locallink &amp;#34;\2&amp;#34; \&amp;gt;\}\}&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DOTALL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;[&lt;em&gt;Edit: I&amp;rsquo;ve since updated &lt;a href=&#34;https://github.com/danabo/blog/blob/master/blog.py&#34;target=&#34;_blank&#34;&gt;blog.py&lt;/a&gt; to use an iterator-based parser so that I can ignore comments and wiki-links inside code blocks, which is a problem I ran into writing this very post!&lt;/em&gt;]&lt;/p&gt;
&lt;p&gt;For internal links, I call the &lt;a href=&#34;https://github.com/danabo/blog/blob/master/layouts/shortcodes/locallink.html&#34;target=&#34;_blank&#34;&gt;locallink&lt;/a&gt; Hugo &lt;a href=&#34;https://gohugo.io/content-management/shortcodes/&#34;target=&#34;_blank&#34;&gt;shortcode&lt;/a&gt; I made, i.e. &lt;code&gt;{{ locallink &amp;quot;...&amp;quot; }}&lt;/code&gt;, which checks if the given post name exists. If so, it returns an anchor to the absolute URL for that note. If not, it returns a &lt;em&gt;red&lt;/em&gt; anchor indicating the post does not exist, 



  &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;like this&lt;/a&gt;


. That way, if I&amp;rsquo;ve referenced a note that is not marked for publication, the current note will be published. The red link is kind of like a &lt;a href=&#34;https://en.wikipedia.org/wiki/Wikipedia:Red_link&#34;target=&#34;_blank&#34;&gt;missing wiki page&lt;/a&gt;. Perhaps if readers become curious about notes I didn&amp;rsquo;t publish, I might become motivated to publish them.&lt;/p&gt;
&lt;p&gt;locallink &lt;a href=&#34;https://gohugo.io/&#34;target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt; shortcode:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-html&#34; data-lang=&#34;html&#34;&gt;{{ $name := (.Get 0) }}
{{ $postFile := (print &amp;#34;content/posts/&amp;#34; $name &amp;#34;.md&amp;#34;) }}
{{ if (fileExists $postFile) }}
&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;href&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;\=&amp;#34;{{&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;ref&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;}}&amp;#34;\&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;{{ $name }}&lt;span class=&#34;err&#34;&gt;&amp;lt;&lt;/span&gt;/a\&amp;gt;
{{ else }}
&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;href&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;\=&amp;#34;&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;\=&amp;#34;&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;broken&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&amp;#34;\&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;{{ $name }}&lt;span class=&#34;err&#34;&gt;&amp;lt;&lt;/span&gt;/a\&amp;gt;
{{ end }}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/danabo/blog/blob/master/publish.sh&#34;target=&#34;_blank&#34;&gt;publish.sh&lt;/a&gt; will first run blog.py, and then run &lt;code&gt;git commit -v&lt;/code&gt; which shows me the diff. If I add a commit description in the prompt, publish.sh will go ahead and push the changes, and then update the gh-pages branch. If I quit the editor without adding a commit message, publish.sh will abort.&lt;/p&gt;
&lt;h1 id=&#34;reader-experience&#34;&gt;Reader experience&lt;/h1&gt;
&lt;p&gt;Currently the reader sees a typical blog layout: a &amp;ldquo;blog roll&amp;rdquo; of recent posts with previews and tags. I don&amp;rsquo;t intend my notes to have any particular time ordering. Notes are objects in flux. I might edit anything. Since I&amp;rsquo;m using time of last edit as the post date, anything I touch will float back to the top. I might decide to change that.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Varational Solomonoff Induction</title>
      <link>https://danabo.github.io/blog/posts/varational-solomonoff-induction/</link>
      <pubDate>Sun, 21 Feb 2021 12:22:08 -0600</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/varational-solomonoff-induction/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\kl}[2]{D_{KL}\left(#1\ \| \ #2\right)}&lt;br&gt;
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\ }&lt;br&gt;
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}}\ }&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\ve}{\varepsilon}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\sm}{\mathrm{softmax}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Free_energy_principle&#34;target=&#34;_blank&#34;&gt;free energy principle&lt;/a&gt; is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Variational_Bayesian_methods&#34;target=&#34;_blank&#34;&gt;variational Bayesian method&lt;/a&gt; for approximating posteriors. Can free energy minimization combined with program synthesis methods from machine learning tractably approximate &lt;a href=&#34;https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference&#34;target=&#34;_blank&#34;&gt;Solomonoff induction&lt;/a&gt; (i.e. universal inference)? In these notes, I explore what the combination of these ideas looks like.&lt;/p&gt;
&lt;h1 id=&#34;machine-learning&#34;&gt;Machine learning&lt;/h1&gt;
&lt;p&gt;I want to make an important clarification about &amp;ldquo;Bayesian machine learning&amp;rdquo;. First, I&amp;rsquo;ll briefly define some &amp;ldquo;modes&amp;rdquo; of machine learning.&lt;/p&gt;
&lt;p&gt;In parametric machine learning, we have a function $f_\t$ parametrized by $\t\in\T$. Let $q_\t(D)$ be a probability distribution on datasets $D$ defined in terms of $f_\t$. For supervised learning, $q_\t(D) = \prod_{(x,y)\in D} Pr(y; f_\t(x))$ is the product of probabilities of each target $y$ given distribution parameters $f_\t(x)$, e.g. $f_\t(x)$ returns the mean and variance of a Gaussian over $y$. For unsupervised learning, $f_\t(x)$ might return a real number which serves as the log-probability of each $x \in D$. In general $f_\t$ can be any kind of parametric ML model, but these days it is likely to be a neural network.&lt;/p&gt;
&lt;p&gt;Typical usage &amp;ldquo;modes&amp;rdquo; of $q_\t$ in machine learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MLE&lt;/strong&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;target=&#34;_blank&#34;&gt;maximum likelihood&lt;/a&gt;): Training produces hypothesis with highest data likelihood.
&lt;ul&gt;
&lt;li&gt;$\t\in\T$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$\t^* = \argmax{\t}\log q_\t(D)$ for dataset $D$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MAP&lt;/strong&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation&#34;target=&#34;_blank&#34;&gt;maximum a posteriori&lt;/a&gt;): Training produces model with highest posterior probability.
&lt;ul&gt;
&lt;li&gt;$\t\in\T$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$\t^* = \argmax{\t}\left[\log q_\t(D) + \log p(\t)\right]$ for dataset $D$ and prior $p(\t)$.&lt;/li&gt;
&lt;li&gt;$\log p(\t)$ can be viewed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Regularization_%28mathematics%29&#34;target=&#34;_blank&#34;&gt;regularizer&lt;/a&gt;. MAP is just MLE plus regularization - the most typical form of parametric machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bayesian Inference&lt;/strong&gt;: Prior over the parameter induces a posterior over parameters given data.
&lt;ul&gt;
&lt;li&gt;$\t\in\T$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$p(\t \mid D) = \frac{q_\t(D) p(\t)}{\int_\T q_\t(D) p(\t) d\t}$ is the exact posterior on hypotheses $\T$ given dataset $D$.&lt;/li&gt;
&lt;li&gt;Unlike in MLE and MAP, there is no notion of optimal parameter $\t^*$. Instead we have much more information: $p(\t \mid D)$ &amp;ldquo;scores&amp;rdquo; every parameter in $\T$, and all the scores taken together constitute our information.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variational Bayes&lt;/strong&gt; (free energy minimization): training produces a (approximate) posterior distribution over hypotheses.
&lt;ul&gt;
&lt;li&gt;$z \in \mc{Z}$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$\tilde{\t} = \argmin{\t}\kl{q_\t(z)}{p(z \mid D)}$ is the target parameter for dataset $D$ and model $p(D,z)$. This is assumed to be intractable to find.&lt;/li&gt;
&lt;li&gt;$\t^* = \argmin{\t}\kl{q_\t(z)}{p(z)} - \E_{z\sim f_\t(z)}\left[\lg p(D \mid z)\right]$ is the approximation. This is what we find through optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The variational Bayes &amp;ldquo;usage mode&amp;rdquo; is clearly different from the others. MLE and MAP are fitting $f_\t$ to the data, i.e. finding a single $\t^*\in\T$ that maximizes the probability of the data under $q_\t$. Bayesian inference is finding a distribution $p(\t \mid D)$ on $\T$ which represents the model&amp;rsquo;s beliefs about various parameters $\t\in\T$ being likely or unlikely as explanations of the data. This is not the same as fitting $f_\t$ to data, since we are not choosing any particular parameter in $\T$. Variational Bayes uses $q_{\t^*}$ as an approximation of $p(\t \mid D)$, where $\t^*\in\T$ is the optimal parameter of distribution $q_\t(z)$ and $z\in\mc{Z}$ is a hypothesis.&lt;/p&gt;
&lt;p&gt;In the first three modes, $\T$ are hypotheses and we are either selecting one or finding a distribution over them. In the variational Bayes mode, $\T$ are not hypotheses. Instead we introduce $\mc{Z}$ as the hypothesis space and $\T$ is the parameter space for the approximate posterior $q_\t(z)$ on $\mc{Z}$, i.e. $q_\t(z)$ approximates $p(z\mid D)$. We don&amp;rsquo;t have $\mc{Z}$ in the first three modes, and we are interested in $p(\t \mid D)$ rather than $p(z \mid D)$. Also in the first three modes, $q_\t(D)$ is a distribution on what is observed, datasets $D$, rather than over latent $\mc{Z}$.&lt;/p&gt;
&lt;h2 id=&#34;what-is-bayesian-machine-learning&#34;&gt;What is Bayesian machine learning?&lt;/h2&gt;
&lt;p&gt;Conventionally, a Bayesian model has a prior probability distribution over it&amp;rsquo;s parameters, and inference involves finding posterior distributions. This corresponds to the Bayesian inference mode above. Out of the four modes, MLE is definitively non-Bayesian. MAP might be called semi-Bayesian, simply because there is a prior on parameters $p(\t)$, but only the argmax of the posterior is being found, rather than a full posterior. The variational Bayes mode is where things get wonky. There are two models: $q_\t(z)$ and $p(z, D)$. The first is parametrized and is optimized greedily with something like gradient descent, as in the MLE or MAP cases. The second is Bayesian.&lt;/p&gt;
&lt;p&gt;Is variational Bayes a Bayesian ML method? In one sense yes, in another sense no. It&amp;rsquo;s efficacy depends on $q_\t(z)$ being a good approximation of the posterior $p(z \mid D)$, and whether $q_\t(z)$ is a good approximation depends on the efficacy of the chosen machine learning method (e.g. neural networks trained with gradient descent). I&amp;rsquo;d expect $q_\t(z)$ to be a non-Bayesian model (If it were Bayesian, how then do you tractably approximate it? That is the very thing we are trying to do with $p(z, D)$.) So then the efficacy of variational Bayes comes down to the properties of non-Bayesian machine learning. If at the end of the day, point-estimates of parameters are always doing the heavy lifting (i.e. generalizing well), why be Bayesian in the first place?&lt;/p&gt;
&lt;h1 id=&#34;solomonoff-induction&#34;&gt;Solomonoff induction&lt;/h1&gt;
&lt;p&gt;I learned about this topic from &lt;a href=&#34;https://www.springer.com/gp/book/9781489984456&#34;target=&#34;_blank&#34;&gt;An Introduction to Kolmogorov Complexity and Its Applications&lt;/a&gt; and &lt;a href=&#34;http://www.hutter1.net/ai/uaibook.htm&#34;target=&#34;_blank&#34;&gt;Universal Artificial Intelligence&lt;/a&gt;. I recommend both books as references.&lt;/p&gt;
&lt;p&gt;There are different formulations of Solomonoff induction, each utilizing a hypothesis space containing all programs - but different kinds of programs for each formulation. I outline three of them: &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt;, &lt;a href=&#34;#version-2&#34;&gt;#Version 2&lt;/a&gt;, &lt;a href=&#34;#version-3&#34;&gt;#Version 3&lt;/a&gt;. Only an understanding of &lt;a href=&#34;#version-2&#34;&gt;#Version 2&lt;/a&gt; is needed for the subsequent sections.&lt;/p&gt;
&lt;h2 id=&#34;notation&#34;&gt;Notation&lt;/h2&gt;
&lt;p&gt;Let $\B = \{0,1\}$ be the binary alphabet, $\B^n$ be the set of all length-$n$ binary strings, and $\B^\infty$ be the set of all infinite binary strings.&lt;/p&gt;
&lt;p&gt;Let $\B^* = \B^0 \times \B^1 \times \B^2 \times \B^3 \times\ldots$ be the set of all finite binary strings of any length.&lt;br&gt;
$\epsilon$ is the empty string, i.e. $\B^0 = \{\epsilon\}$.&lt;/p&gt;
&lt;p&gt;Let $x \sqsubseteq y$ denote that binary string $x$ is a prefix of (or equal to) binary string $y$.&lt;br&gt;
Let $x`y$ denote the string concatenation of $x$ and $y$.&lt;br&gt;
Let $x_{a:b}$ denote the slice of $x$ from position $a$ to $b$ (inclusive).&lt;br&gt;
Let $x_{&amp;lt;n} = x_{1:n-1}$ denote the prefix of $x$ up to $n-1$.&lt;br&gt;
Let $x_{&amp;gt;n} = x_{n+1:\infty}$ denote the &amp;ldquo;tail&amp;rdquo; of $x$ starting from $n+1$.&lt;/p&gt;
&lt;h2 id=&#34;version-1&#34;&gt;Version 1&lt;/h2&gt;
&lt;p&gt;Let $U$ be a universal machine, i.e. if $z\in\B^*$ is a program then $U(z) \in \B^*$ is some binary string, or $U(z)$ is undefined because $U$ does not halt on $z$. We do not give program $z$ input, but $z$ can include &amp;ldquo;data&amp;rdquo;, in the sense that it&amp;rsquo;s program specifies a binary string that gets loaded into memory when the program starts.&lt;/p&gt;
&lt;p&gt;Let $p(z)$  be a prior on finite binary strings.&lt;br&gt;
Then for observation $x \in \B^*$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p(x \mid z) = \begin{cases}1 &amp;amp; x \sqsubseteq U(z) \\ 0 &amp;amp; \mathrm{otherwise}\end{cases}$&lt;/li&gt;
&lt;li&gt;$p(z, x) = p(x \mid z)p(z) = \begin{cases}p(z) &amp;amp; x \sqsubseteq U(z) \\ 0 &amp;amp; \mathrm{otherwise}\end{cases}$&lt;/li&gt;
&lt;li&gt;$p(x) = \sum_{z\in\B^*} p(x,z) = \sum_{z \in \B^*;\ x \sqsubseteq U(z)} p(z)$&lt;/li&gt;
&lt;li&gt;$p(z \mid x) = p(z, x)/p(x) = \begin{cases}p(z)/p(x) &amp;amp; x \sqsubseteq U(z) \\ 0 &amp;amp; \mathrm{otherwise}\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$p(x)$ is the data probability.&lt;br&gt;
$p(z)$ is the prior.&lt;br&gt;
$p(z\mid x)$ is the posterior.&lt;/p&gt;
&lt;p&gt;If we observe $x \in \B^*$, we may ask for the probability of subsequently observing some $y\in\B^*$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(y \mid x) &amp;amp;= \frac{1}{p(x)}\sum_{z\in\B^*}p(x`y,z) \\&lt;br&gt;
&amp;amp;= \frac{1}{p(x)}\sum_{z \in \B^*;\ x`y \sqsubseteq U(z)} p(z) \\&lt;br&gt;
&amp;amp;= \sum_{z \in \B^*;\ x`y \sqsubseteq U(z)} p(z \mid x)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;What prior $p(z)$ should we choose? Solomonoff recommends&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(z) = 2^{-\ell(z)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\ell(z)$ is the length of program $z$. This has the effect of putting more prior probability on short programs, essentially encoding a preference for &amp;ldquo;simple&amp;rdquo; explanations of the data. This prior also has the benefit that it is fast to calculate $p(z)$ for any $z$. In general, choice of prior is a matter of taste, and should depend on practical considerations like tractability and regularizations such as &amp;ldquo;simplicity&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;version-2&#34;&gt;Version 2&lt;/h2&gt;
&lt;p&gt;Let $\mu$ be a probability measure on $\B^\infty$, meaning $\mu$ maps (measurable) subsets of $\B^\infty$ to probabilities. $\mu$ can be specified by defining it&amp;rsquo;s value on the &lt;strong&gt;cylinder sets&lt;/strong&gt; $\Gamma_x = \set{\o \in \B^\infty \mid x \sqsubset \o}$ for every $x\in\B^*$, i.e. the set of all infinite binary strings starting with $x$. I&amp;rsquo;ll let $\mu(x)$ be a shorthand denoting $\mu(\Gamma_x)$. Then $\mu(x)$ is the probability of finite string $x$. For any such measure $\mu$, it must be the case that&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x) = \mu(x`0) + \mu(x`1)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x) \geq \mu(x`y)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $x,y\in\B^*$.&lt;/p&gt;
&lt;p&gt;$\mu$ is a &lt;strong&gt;semimeasure&lt;/strong&gt; iff it satisfies&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x) \geq \mu(x`0) + \mu(x`1)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $x \in \B^*$. That is to say, if $\mu$ is a semimeasure then probabilities may sum to less than one (this is like supposing that some probability goes missing).&lt;/p&gt;
&lt;p&gt;The following are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mu$ is &lt;strong&gt;computable&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;There exists some program which computes the probability $\mu(x)$ for all inputs $x$.&lt;/li&gt;
&lt;li&gt;There exists some program which outputs $x$ with probability $\mu(x)$ (for all $x$) when given uniform random input bits.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\mu$ is &lt;strong&gt;semicomputable&lt;/strong&gt; (a.k.a. enumerable) if there exists some program which approximates the probability $\mu(x)$ (for all $x$) by outputting a sequence of rational numbers $\set{\hat{p}_n}$ approaching $\mu(x)$, but where it is impossible to determine how close the sequence is to $\mu(x)$ at any point in time. That is to say, you cannot know the error sequence $\varepsilon_n = \abs{\mu(x) - \hat{p}_n}$, but you know that $\varepsilon_n \to 0$ as $n\to\infty$. In contrast, if $\mu$ is computable then there exists a program that outputs both the sequence of rationals $\set{\hat{p}_n}$ AND the errors $\set{\varepsilon_n}$ (computability implies there exists a program that takes a desired error $\varepsilon&amp;gt;0$ as input and outputs in finite time (i.e. halts) the corresponding approximation $\hat{p}$ s.t. $\varepsilon&amp;gt;\abs{\mu(x)-\hat{p}}$).&lt;/p&gt;
&lt;p&gt;Let $\mc{M}$ be the set of all semicomputable semimeasures on infinite binary sequences. Let $p(\mu)$ be a prior on $\mc{M}$.&lt;/p&gt;
&lt;p&gt;Then for observation $x \in \B^*$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p(x \mid \mu) = \mu(x)$&lt;/li&gt;
&lt;li&gt;$p(x, \mu) = p(x \mid \mu)p(\mu) = p(\mu)\mu(x)$&lt;/li&gt;
&lt;li&gt;$p(x) = \sum_{\mu \in \mc{M}} p(x \mid \mu) = \sum_{\mu \in \mc{M}} p(\mu)\mu(x)$&lt;/li&gt;
&lt;li&gt;$p(\mu \mid x) = p(\mu)\frac{\mu(x)}{p(x)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$p(x)$ is the data probability.&lt;br&gt;
$p(\mu)$ is the prior.&lt;br&gt;
$p(\mu\mid x)$ is the posterior.&lt;/p&gt;
&lt;p&gt;If we observe $x \in \B^*$, we may ask for the probability of subsequently observing some $y\in\B^*$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(y \mid x) &amp;amp;= \sum_{\mu \in \mc{M}} p(y,\mu\mid x) \\&lt;br&gt;
&amp;amp;= \sum_{\mu \in \mc{M}} p(\mu\mid x)\mu(y \mid x) \\&lt;br&gt;
&amp;amp;= \sum_{\mu \in \mc{M}}p(\mu)\frac{\mu(x)}{p(x)}\mu(y\mid x)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\mc{M}$ is all semicomputable semimeasures, rather than all computable measures, because the former is a decidable set while the latter is not, i.e. in practice the former set of hypotheses can be feasibly enumerated by enumerating all programs, while the latter cannot be. If we required only measures, then we could not decide which programs produced proper measures (if the program doesn&amp;rsquo;t halt on $x$, that is like saying the probability that would have gone to strings starting with $x$ &amp;ldquo;disappears&amp;rdquo;). Allowing non-halting programs means we don&amp;rsquo;t have to filter out programs which don&amp;rsquo;t halt. Similar issue for computable vs semicomputable.&lt;/p&gt;
&lt;p&gt;Versions 1 and 2 are equivalent. That is to say, we can get the same data distribution $p(x)$ for the right choice of prior in each version.&lt;/p&gt;
&lt;p&gt;A typical choice of the prior in this version is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(\mu) = 2^{-K(\mu)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $K(\mu)$ is the &lt;a href=&#34;https://www.math.wisc.edu/~jmiller/Notes/contrasting.pdf&#34;target=&#34;_blank&#34;&gt;&lt;strong&gt;prefix-free Kolmogorov complexity&lt;/strong&gt;&lt;/a&gt; of $\mu$, i.e. the length of the shortest program that (semi)computes $\mu$ (given a space of prefix-free programs, i.e. program strings contain their own length information).&lt;/p&gt;
&lt;h2 id=&#34;version-3&#34;&gt;Version 3&lt;/h2&gt;
&lt;p&gt;As you might guess, this will also turn out to be (sorta) equivalent to the first two versions. This is like version 2, except instead of considering a hypothesis to be a program that samples data sequences given uniform random input bits, a hypothesis is such a program packaged together with a particular infinite input sequence. Thus, hypotheses are in a sense infinite programs.&lt;/p&gt;
&lt;p&gt;Let $U$ be a &lt;strong&gt;universal monotone machine&lt;/strong&gt;. That means $U$ can execute infinitely long programs in a streaming fashion by producing partial outputs as the program is read. Let $z \in \B^\infty$. Then for every finite prefix $z_{1:n}$, we get a partial output $U(z_{1:n}) = \o_{1:m} \in \B^m$. We require that $U(z_{1:n}) \sqsubseteq U(z_{1:n&#39;})$ for $n \leq n&#39;$. The output of $z$ is infinite if $m\to\infty$ as $n\to\infty$, is finite if $m &amp;lt; \infty$ for all $n$, or is undefined if $U$ does not halt on any $z_{1:n}$.&lt;/p&gt;
&lt;p&gt;Let $\tilde{z}$ be a program that samples from $\mu$ from version 2, i.e. $\tilde{z}$ takes uniform random bits as input and outputs some $x_{1:m}$ with probability $\mu(x_{1:m})$. Then we can produce an infinite &amp;ldquo;version 3&amp;rdquo; program by appending an infinitely long uniform random sequence $u$, i.e. $z = \tilde{z}`u$ where $U(\tilde{z}) = \epsilon$ (empty string, i.e. executing $\tilde{z}$ outputs nothing), and $U(\tilde{z}`u_{1:n&#39;})$ outputs some prefix of $x$ (if we let $x$ be infinite) by running $\tilde{z}$ on input $u_{1:n&#39;}$.&lt;/p&gt;
&lt;p&gt;If we feed uniform random bits into $U$, then $U$ itself samples from a distribution over infinite data sequences. The induced distribution is $p(x_{1:m})$ which is a Solomonoff distribution that we can use for universal inference. This is equivalent to putting a uniform prior on the infinite programs $\B^\infty$, i.e.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(z_{1:n}) = 2^{-n}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;For partial observation $x_{1:m} \in \B^*$ (the remaining part of $x$ is the unobserved future),&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x_{1:m}) = \sum_{\zeta\in\Phi(x_{1:m})} 2^{-\ell(\zeta)}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\ell(\zeta)$ is the length of finite string $\zeta$.&lt;/li&gt;
&lt;li&gt;$\Phi(x_{1:m})$ is a prefix-free set of all finite sequences $\zeta$ which output at least $x_{1:m}$ when fed into $U$ (i.e. for all $z_{1:n} \in \B^*$ if $x_{1:m} \sqsubseteq U(z_{1:n})$ then $z_{1:n} \in \bigcup_{\zeta\in\Phi(x_{1:m})} \Gamma_\zeta$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To calculate $p(x_{1:m})$, we ostensibly want to sum up the prior probabilities of all programs which output at least $x_{1:m}$, but remember that our programs are infinitely long, and the prior probability of any infinite program is 0 (because $2^{-n}\to 0$ as $n\to\infty$). The sum above performs a &lt;a href=&#34;https://en.wikipedia.org/wiki/Lebesgue_integration&#34;target=&#34;_blank&#34;&gt;Lebesgue integral&lt;/a&gt; over the infinite programs $\B^\infty$ by dividing them into &amp;ldquo;intervals&amp;rdquo; (i.e. sets of programs sharing the same prefix - geometrically an interval if you consider an infinite binary sequence to be a real number between 0 and 1) and summing up the lengths (prior probabilities) of the intervals. The function $\Phi$ is a convenient construction for producing this set of intervals for us. Finding $\Phi(x_{1:m})$ is complex, and not especially important to go into.&lt;/p&gt;
&lt;p&gt;The joint distribution is&lt;/p&gt;
&lt;p&gt;$$p(x_{1:m}, z_{1:n}) = \sum_{\zeta\in\Phi(x);\ z_{1:n}\sqsubseteq\zeta} 2^{-\ell(\zeta)}\,.$$&lt;/p&gt;
&lt;p&gt;From here, we can straightforwardly compute the probability of the data under partial hypothesis $z_{1:n}$:&lt;/p&gt;
&lt;p&gt;$$p(x_{1:m} \mid z_{1:n}) =p(x_{1:m}, z_{1:n})/p(z_{1:n}) = \sum_{\zeta\in\Phi(x);\ z_{1:n}\sqsubseteq\zeta} 2^{n-\ell(\zeta)}\,.$$&lt;/p&gt;
&lt;p&gt;And finally the data posterior of the future slice $x_{m:s}$ given $x_{&amp;lt;m}$ (for $m&amp;lt;s$):&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x_{m:s}\mid x_{&amp;lt;m}) = \frac{1}{p(x_{&amp;lt;m})}\sum_{\zeta\in\Phi(x_{1:s})} 2^{-\ell(\zeta)}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;h1 id=&#34;variational-solomonoff-induction&#34;&gt;Variational Solomonoff induction&lt;/h1&gt;
&lt;p&gt;Suppose we observe finite sequence $x_{1:t} \in \B^*$ and we want to find the posterior $p(h \mid x_{1:t})$. Usually this is intractable to calculate, and in the case of Solomonoff induction, the posterior is not even computable. We can get around this limitation by approximating the posterior with a parametrized distribution $q_\t(h)$ over hypotheses $h\in\mc{H}$. For now I will be agnostic as to what kind of hypothesis space $\mc{H}$ is, and it can be any of the hypothesis spaces discussed above: &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt;, &lt;a href=&#34;#version-2&#34;&gt;#Version 2&lt;/a&gt;, &lt;a href=&#34;#version-3&#34;&gt;#Version 3&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this case, let&amp;rsquo;s find $\t^*\in\T$ that minimizes the KL-divergence $\kl{q_\t(h)}{p(h\mid x_{1:t})}$ so that $q_\t(h)$ is as close as possible to $p(h\mid x_{1:t})$. Note that $q_\t(h)$ does not depend on $x_{1:t}$ because we find $\t^*$ after $x_{1:t}$ is already observed ($x_{1:t}$ is like a constant w.r.t. this optimization), whereas the joint distribution $p(h, x)$ is defined up front before any data is observed.&lt;/p&gt;
&lt;p&gt;However, if $p(h \mid x_{1:t})$ is intractable to calculate, then so is $\kl{q_\t(h)}{p(h\mid x_{1:t})}$. With a few tricks, we can find an alternative optimization target that is tractable. Rewriting the KL-divergence:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp;\kl{q_\t(h)}{p(h\mid x_{1:t})} \\&lt;br&gt;
&amp;amp;\quad= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h\mid x_{1:t})}\right)\right] \\&lt;br&gt;
&amp;amp;\quad= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h)}\right)-\lg p(x_{1:t} \mid h) + \lg p(x_{1:t})\right] \\&lt;br&gt;
&amp;amp;\quad= \kl{q_\t(h)}{p(h)} - \E_{h\sim q_\t}\left[\lg p(x_{1:t} \mid h)\right] - \lg \frac{1}{p(x_{1:t})} \\&lt;br&gt;
&amp;amp;\quad= \mc{F}[q_\t] - \lg \frac{1}{p(x_{1:t})} \,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\mc{F}[q_\t]$ is defined as&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q_\t] &amp;amp;= \kl{q_\t(h)}{p(h)} - \E_{h\sim q_\t}\left[\lg p(x_{1:t} \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h)}\right)-\lg p(x_{1:t} \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h,x_{1:t})}\right)\right]\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\mc{F}[q_\t]$ is called the &lt;strong&gt;variational free energy&lt;/strong&gt;. It depends explicitly on choice of parameter $\t$, but also keep in mind it depends implicitly on the observation $x_{1:t}$ and distribution $p(h, x_{1:t})$.&lt;/p&gt;
&lt;p&gt;Because $\lg \frac{1}{p(x_{1:t})}$ (called the &lt;strong&gt;surprise&lt;/strong&gt; of $x_{1:t}$) is positive and constant (because observation $x_{1:t}$ is constant), then minimizing $\mc{F}[q_\t]$ to $\lg \frac{1}{p(x_{1:t})}$ guarantees that $\kl{q_\t(h)}{p(h\mid x_{1:t})}$ is 0 (KL-divergence cannot be negative), which in turn guarantees that $q_\t(h)$ and $p(h\mid x_{1:t})$ are equal distributions on $\mc{H}$. If our optimization process does not fully minimize $\mc{F}[q_\t]$, then $q_\t(h)$ will approximate $p(h\mid x_{1:t})$ with some amount of error.&lt;/p&gt;
&lt;p&gt;The optimization procedure we want to perform is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp;\argmin{\t\in\T} \mc{F}[q_\t] \\&lt;br&gt;
=\ &amp;amp; \argmin{\t\in\T} \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h,x_{1:t})}\right)\right] \\&lt;br&gt;
=\ &amp;amp; \argmax{\t\in\T}\set{ \E_{h \sim q_\t}\left[R(h)\right] + \mb{H}_{h \sim q_\t}[q_\t(h)]}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This now has the form of a one-timestep reinforcement learning objective, where $R(h) = \lg p(h,x_{1:t})$ is the reward for &amp;ldquo;action&amp;rdquo; $h$, and $\mb{H}_{h \sim q_\t}[q_\t(h)]$ is the entropy of $q_\t(h)$. Here $q_\t(h)$ is called the &lt;strong&gt;policy&lt;/strong&gt;, i.e. the distribution actions are sampled from. Maximizing this objective jointly maximizes expected reward under the policy and entropy of the policy. An entropy term is typically added to RL objectives as a regularizer to encourage exploration (higher entropy policy means more random actions), but in this case the entropy term comes included.&lt;/p&gt;
&lt;p&gt;We can use standard &lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html&#34;target=&#34;_blank&#34;&gt;policy gradient methods&lt;/a&gt; (e.g. &lt;a href=&#34;https://deepmind.com/research/publications/impala-scalable-distributed-deep-rl-importance-weighted-actor-learner-architectures&#34;target=&#34;_blank&#34;&gt;IMPALA&lt;/a&gt;) to maximize the above RL objective (equivalent to minimizing free energy), so long as $q_\t(h)$ is fast to sample from, and the reward $R(h) = \lg p(h,x_{1:t})$ is fast to compute. We can control both.&lt;/p&gt;
&lt;h1 id=&#34;tractability&#34;&gt;Tractability&lt;/h1&gt;
&lt;p&gt;Tractability depends on our choice of $\mc{H}$ and prior $p(h)$. What operations do we want to be tractable? Typically we want:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To approximate hypothesis posteriors: $q_{\t^*}(h) \approx p(h \mid x_{1:t})$.&lt;/li&gt;
&lt;li&gt;To approximate predictive data distributions (data posterior): $p(x_{&amp;gt;t} \mid x_{1:t})$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;hypothesis-posterior&#34;&gt;Hypothesis posterior&lt;/h2&gt;
&lt;p&gt;The approximation $q_{\t^*}(h)$ allows us to do this. The tractability of finding a good parameter $\t^*$ for $q$ using policy gradient methods will require that the reward $R(h) = \lg p(h,x_{1:t})$ is fast to calculate.&lt;/p&gt;
&lt;p&gt;We can write the reward as the sum of two terms:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lg p(h,x_{1:t}) = \lg p(h) + \lg p(x_{1:t} \mid h)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then we need fast calculation of prior probabilities $p(h)$, and data probabilities under hypotheses, $p(x_{1:t} \mid h)$.&lt;/p&gt;
&lt;h2 id=&#34;data-posterior&#34;&gt;Data posterior&lt;/h2&gt;
&lt;p&gt;We want to approximate $p(y \mid x)$, i.e. the probability of observing string $y$ after observing $x$. This is similar to the problem of calculating $p(x)$, the data probability.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x) = \E_{h\sim p(h)} [p(x\mid h)]\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If it were fast to compute $p(x\mid h)$ for a given $h$, and fast to sample from the prior $p(h)$, then we can approximate the data probability with Monte Carlo sampling:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x) \approx \hat{p}(x) = \sum_{h \in H^{(k)}} p(x\mid h)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $H^{(k)} = \set{h_1, h_2, \ldots, h_k} \sim p(h)$ is an i.i.d. sample from $p(h)$ of size $k$.&lt;/p&gt;
&lt;p&gt;In the same way, we can approximate the data posterior using the identity&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y \mid x) = \E_{h\sim p(h \mid x)} [p(y\mid x, h)]\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The Monte Carlo approximation is:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y\mid x) \approx \hat{p}(y\mid x) = \sum_{h \in H^{(k)}} p(y\mid x, h)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $H^{(k)} = \set{h_1, h_2, \ldots, h_k} \sim q_{\t^*}(h)$ is an i.i.d. sample drawn from the optimized approximate posterior $q_{\t^*}(h)$. So approximating the data posterior requires approximating the hypothesis posterior.&lt;/p&gt;
&lt;p&gt;$p(y\mid x, h)$ is the conditional data distribution under hypothesis $h$. If $h$ is a probability measure, then $p(x \mid h) = h(x)$ and $p(y\mid x, h) = h(y\mid x)$.&lt;/p&gt;
&lt;p&gt;For approximating the data distribution, we need fast sampling from hypothesis priors $p(h)$ and fast data-under-hypothesis probabilities $p(x \mid h)$.&lt;/p&gt;
&lt;p&gt;For approximating the data posterior, we need fast approximate posteriors $q_{\t^*}(h)$, and we need hypothesis data-conditionalization $p(y\mid x, h)$ to be fast.&lt;/p&gt;
&lt;h2 id=&#34;generalization&#34;&gt;Generalization&lt;/h2&gt;
&lt;p&gt;Speed is necessary but not sufficient for tractability. The approximations we find need to be good ones. Choosing an appropriate model $q$, which is a distribution over programs, is within the realm of program synthesis and machine learning. These days, program synthesis is done with neural language models on code tokens.&lt;/p&gt;
&lt;p&gt;Can neural networks approximate the true posterior $p(h \mid x)$? This is a generalization problem. The optimized generative model on programs, $q_{\t^*}(h)$, will have been trained on finitely many programs. Whether $q_{\t^*}(h&#39;) \approx p(h&#39; \mid x)$ for some program $h&#39;$ unseen during training will depend entirely on the generalization properties of the particular program synthesizer that is used in $q_\t$.&lt;/p&gt;
&lt;p&gt;The difficulty of applying machine learning to program synthesis is dealing with reward sparsity and generalizing highly non-smooth functions. To maximize reward $R(h) = \lg p(h,x_{1:t})$, the model $q$ needs to upweight programs $h$ that jointly have a high prior $p(h)$ and high likelihood $p(x_{1:t} \mid h)$. If the prior $p(h)$ is simple, perhaps $q$ can learn that function. On the other hand, if this prior encodes information about how long $h$ runs for (as I discuss in the &lt;a href=&#34;#prior&#34;&gt;#Prior&lt;/a&gt; section), the prior is then not even computable. Same for $p(x_{1:t} \mid h)$. Without actually running $h$ on $x_{1:t}$, determining the output will not be possible in general. For $q$ to predict these things based on $h$&amp;rsquo;s code but without running $h$ is in general impossible. The function $p(h \mid x_{1:t})$ (as a function of $h$) highly chaotic, and we cannot expect $q$ to generalize in any strong sense. Innovations in program synthesis are still needed to do even somewhat well.&lt;/p&gt;
&lt;p&gt;As a reinforcement learning problem, maximizing this reward suffers from sparsity issues. Most programs will be trivial, in the sense that they just output constant values, or nothing. I expect that Solomonoff induction doesn&amp;rsquo;t start to become effective until you get to programs of moderate length that exhibit interesting behavior. In the context of this reinforcement learning problem, that means the policy $q$ needs to find moderately long programs with moderately high reward. When training first starts, it can take an excessive amount of episodes before any non-trivial reward is discovered. This can make reinforcement learning intractable. Innovations are needed here too.&lt;/p&gt;
&lt;h1 id=&#34;choices&#34;&gt;Choices&lt;/h1&gt;
&lt;p&gt;To summarize the requirements we found above:&lt;br&gt;
Is there a choice of $\mc{H}$ and $p(x,h)$ s.t.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prior $p(h)$ is fast to calculate and sample from.&lt;/li&gt;
&lt;li&gt;Approximate hypothesis posterior $q_{\t^*}(h)$ is fast to sample from.&lt;/li&gt;
&lt;li&gt;Hypothesis data-probability $p(x\mid h)$ is fast to calculate.&lt;/li&gt;
&lt;li&gt;Hypothesis data-conditionalization $p(y\mid x, h)$ is fast to calculate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;program-space&#34;&gt;Program space&lt;/h2&gt;
&lt;p&gt;Hypotheses can be deterministic or stochastic. Deterministic hypotheses would be represented by deterministic programs. Stochastic hypotheses can either be represented by stochastic programs (output is non-deterministic) or by deterministic programs that output probabilities. I think we should choose the latter.&lt;/p&gt;
&lt;p&gt;If our hypotheses are deterministic, then we get Solomonoff induction &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt;. Conditionalization is easy because $p(y \mid x, h) = p(y`x \mid h) = 1$ if $h$ outputs $x$ and $0$ otherwise. However, the vast majority of programs will not output $x$, so the reward $R(h) = \lg p(h,x)$ will be very sparse. That is to say, the reward will be $-\infty$ most of the time (in practice you would clip and scale the reward to something reasonable). This is bad for policy gradient methods and will result in high gradient variance (learning will be extremely slow).&lt;/p&gt;
&lt;p&gt;We should use stochastic hypotheses. If we use non-deterministic programs, conditionalization is hard. Thus we should use programs that output their probabilities.&lt;/p&gt;
&lt;p&gt;The choice of $\mc{H}$ is equivalent to choosing a programming language plus syntax rules so that only valid programs can be constructed. In this case, we want to restrict ourselves to programs that will obey the properties of probability measures $\mu$ on infinite sequences: $\mu(x) = \mu(x`0) + \mu(x`1)$ and  $\mu(x) \geq \mu(x`y)$.&lt;/p&gt;
&lt;p&gt;To ensure this, I propose that programs $h$ take the form of auto-regressive language models. That is to say these programs read in a sequence of input tokens and for each token output a vector of real numbers with the same length as the token space. Passing that vector through a softmax induces a probability distribution over the next input token. The programs maintain their own internal state. A language should be chosen such that all generated programs can be guaranteed to take this form.&lt;/p&gt;
&lt;p&gt;If the program has output probabilities $\hat{p}_1, \ldots, \hat{p}_t$ for input $x_{1:t}$ but does not halt to produce $\hat{p}_{t+1}$, then the probability $\mu_h(x_{1:n})$ for $n&amp;gt;t$ is undefined, and the induced measure $\mu_h$ becomes a semimeasure.&lt;/p&gt;
&lt;h2 id=&#34;prior&#34;&gt;Prior&lt;/h2&gt;
&lt;p&gt;Weighting by program length suffices as a prior:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(h) = 2^{-\ell(h)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;One difficulty in working with programs is long-running execution. This can make computing data probabilities take a long time. One remedy is to down-weight long-running programs in the prior. &lt;a href=&#34;http://www.scholarpedia.org/article/Universal_search&#34;target=&#34;_blank&#34;&gt;Levin search&lt;/a&gt; is an alternative to Solomonoff induction where the prior is weighted solely by runtime. We can mix both kinds of priors.&lt;/p&gt;
&lt;p&gt;This is straightforward in Solomonoff induction &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt; where each program takes no input and outputs a deterministic string. Let $p(h) = 2^{-\ell(h)} / f(\tau(h))$ where $\tau(h)$ is the total runtime of $h$, and $f$ is an increasing function that goes to infinity. For example, if $f(t) = 2^{t}$, then we have prior $2^{-\ell(h)-\tau(h)}$. If you wanted to compute $p(x)$ within some precision $\ve &amp;gt; 0$, you can enumerate all programs $h\in\mc{H}$ by length and run them in parallel (called dovetailing). For each program, stop execution when $2^{-\ell(h)-\tau(h)} &amp;lt; \ve$. Shorter programs will be given more runtime over longer programs. (Thank you Lance Roy for the helpful discussion about this.)&lt;/p&gt;
&lt;p&gt;However, if we are using the programs I previously suggested that output data probabilities, then these programs may be fast on some inputs and slow on others. I don&amp;rsquo;t have a solution, but a reasonable suggestion is to do some kind of heuristic analysis of the programs on some sample inputs and assign a slowness penalty in the prior.&lt;/p&gt;
&lt;h1 id=&#34;lifelong-learning&#34;&gt;Lifelong learning&lt;/h1&gt;
&lt;p&gt;Solomonoff induction is a framework for general-purpose life-long learning, which is a paradigm where an intelligent agent learns to predict it&amp;rsquo;s future (or gain reward) from only one continuous data stream. The agent must learn on-line, and there are no independence assumptions (the data is a timeseries).&lt;/p&gt;
&lt;p&gt;In the variational setup outlined above, we converted the problem of Bayesian inference into a reinforcement learning problem. At time $t$, data $x_{1:t}$ is observed, and a policy $q_{\t^*}$ on programs is trained using policy gradient methods. However, every $t$ requires its own $q_{\t_t^*}$, thus we would need to perform RL training at every timestep. One way to speed this up is to reuse policies from previous timesteps. That is to say, at time $t+1$ perform $\argmax{\t\in\T}\set{ \E_{h \sim q_\t}\left[R(h)\right] + \mb{H}_{h \sim q_\t}[q_\t(h)]}$ using Monte Carlo gradient descent starting from the previous parameter $\t_t^*$. This can be considered fine-tuning. However, this may fail to work if the posterior changes drastically between timesteps.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blogging experiment</title>
      <link>https://danabo.github.io/blog/posts/blogging-experiment/</link>
      <pubDate>Sat, 20 Feb 2021 07:23:41 -0600</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/blogging-experiment/</guid>
      <description>&lt;p&gt;What is this blog?&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a collection of notes pulled directly from my digital journal. My digital journal is a bunch of &lt;a href=&#34;https://www.markdownguide.org/&#34;target=&#34;_blank&#34;&gt;markdown&lt;/a&gt; files that I edit in &lt;a href=&#34;https://obsidian.md/&#34;target=&#34;_blank&#34;&gt;obsidian&lt;/a&gt;. You could call it my &amp;ldquo;second brain&amp;rdquo;. I write everything here. It&amp;rsquo;s a journal in the traditional sense, in that I write about what&amp;rsquo;s going on in my life and what&amp;rsquo;s on my mind in order to collect my thoughts. I also write down logical stuff like TODO-lists, lists of links I want to keep, things I might want to look into. I take copious notes here when I am reading about technical topics. I write down and develop my spurious ideas.&lt;/p&gt;
&lt;p&gt;I want to give the world a window into some of what&amp;rsquo;s in this journal; not because I derive pleasure from exposing my private life to the world (I&amp;rsquo;m actually fairly shy), but because I don&amp;rsquo;t want to be isolated. Learning and thinking alone is less than ideal. I risk reinforcing my own misunderstandings. Feedback from people can reveal things I would not otherwise see.&lt;/p&gt;
&lt;p&gt;Beyond just the benefits for academic studying, feedback from the world helps to give purpose and context to my life. &lt;a href=&#34;https://astralcodexten.substack.com/p/still-alive&#34;target=&#34;_blank&#34;&gt;Scott Alexander recently wrote&lt;/a&gt; that a lot of his  intellectual growth came as a result of blogging, because his blog caused people to reach out to him and tell him things. So the basic premise is that if I broadcast to the world what&amp;rsquo;s on my mind, someone somewhere will take interest and connect with me. Otherwise, we may never know each other exists.&lt;/p&gt;
&lt;p&gt;There is, however, a fundamental tension between quality and broadcasting. Can my notes ever be too raw, too personal, too incomplete, or too short to publish? I want to have a steady stream of output, but I don&amp;rsquo;t want to hide the occasional good stuff in a barrage of no-effort text that one cares about. Not to mention the privacy issues of having a public journal. I don&amp;rsquo;t have a good solution to this tension, so that is why I&amp;rsquo;m experimenting. I said I&amp;rsquo;m providing a window into my personal journal because I will have a process for auto-publishing notes I mark for publication. Hopefully that will remove the activation barrier for posting while also allowing me to keep my finger on quality control (and privacy). More on how this works below.&lt;/p&gt;
&lt;h1 id=&#34;intellectual-journey&#34;&gt;Intellectual journey&lt;/h1&gt;
&lt;p&gt;While I intend to post about miscellaneous topics, there will be a main topic of interest.&lt;/p&gt;
&lt;p&gt;I have been embarking on an academic project to make sense of ideas floating around in machine learning, artificial intelligence, and other fields like neuroscience and epistemology. I want to form my own perspective, independently, on information, uncertainty, randomness, and epistemology.&lt;/p&gt;
&lt;p&gt;Progress is slow. It takes months of on-and-off reading to understand a theoretical topic like &lt;a href=&#34;http://www.scholarpedia.org/article/Algorithmic_randomness&#34;target=&#34;_blank&#34;&gt;algorithmic randomness&lt;/a&gt;. I want people to see what I&amp;rsquo;m working on. I have another blog, &lt;a href=&#34;https://zhat.io/,&#34;&gt;https://zhat.io/,&lt;/a&gt; where I posted explainers on topics that I&amp;rsquo;ve been learning about. The problem is that it takes so long to write pedagogical material, that I haven&amp;rsquo;t even gotten to the topics I&amp;rsquo;ve been studying (still explaining the prerequisites). I tried posting &amp;ldquo;notes&amp;rdquo; (separate from &amp;ldquo;posts&amp;rdquo;) which are informal and often incomplete snapshots of my actual study notes. However I found that I didn&amp;rsquo;t have a clear sense of when my study notes were ready to be published as notes. There was a certain amount of work that I had to do to translate my raw notes over to blog notes, which created a sense of officiality.&lt;/p&gt;
&lt;p&gt;I air on the side of caution when I feel I need to be correct about everything I say. That of course makes total sense. However that stands in contrast with a piece of advice I&amp;rsquo;ve been given for blogging: don&amp;rsquo;t revise, just publish. Blogging is fast and lose. Blogs are just public personal notes. In &lt;a href=&#34;https://zhat.io/&#34;&gt;https://zhat.io/&lt;/a&gt; I made the mistake of taking an authoritative tone, which created a burden of perfectionism. I can&amp;rsquo;t claim something and be wrong about it. But the result was very long technical posts that no one was reading anyway.&lt;/p&gt;
&lt;p&gt;This new blog is an experiment in something more lightweight and streamlined. It&amp;rsquo;s a window into my intellectual journey. My posts are journalism. I&amp;rsquo;m writing about what I saw and experienced while reading and thinking. This is my way of handling the quality-broadcasting tension for academic writing. I&amp;rsquo;m not claiming to be an expert on any topic, or to be explaining any topic. If readers cannot follow along, then that is a good problem to have (that means I have readers). Ideally I&amp;rsquo;ll post often, receive feedback through various channels (such as not following something), and that will provide motivation to explain things. I like writing pedagogy, but I need to know it will actually be read for it to be worth the time investment.&lt;/p&gt;
&lt;p&gt;I intend this blog to be very incremental. Everything is in flux (even it&amp;rsquo;s name, visual style and domain). I want to avoid having to spend a month writing drafts of a long post on a big topic. Instead, I will write a little bit every so often. Think of them as teasers. If people want to hear more, I&amp;rsquo;ll write more. This allows me to get things out of my head and feel good about it. Perhaps I can create more flow in my studying if I feel accomplished more often.&lt;/p&gt;
&lt;p&gt;In that vein, I leave the details of my blogging system and digital journal to future posts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Free Energy Principle 1st Pass</title>
      <link>https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/</link>
      <pubDate>Sat, 20 Feb 2021 07:23:41 -0600</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\kl}[2]{D_{KL}\left(#1\ \| \ #2\right)}&lt;br&gt;
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\ }&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\atup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Related notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;



  &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;The free-energy principle a unified brain theory&lt;/a&gt;


&lt;/li&gt;
&lt;li&gt;



  &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;Friston&amp;#39;s Free Energy (Active Inference)&lt;/a&gt;


&lt;/li&gt;
&lt;li&gt;Hackmd version of this note: &lt;a href=&#34;https://hackmd.io/@z5RLVXyrTg-JLCnL9c_xOQ/r1KSFUjkO&#34;&gt;https://hackmd.io/@z5RLVXyrTg-JLCnL9c_xOQ/r1KSFUjkO&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;my-current-understanding&#34;&gt;My current understanding&lt;/h1&gt;
&lt;h2 id=&#34;note-on-probability-notation&#34;&gt;Note on probability notation&lt;/h2&gt;
&lt;p&gt;These are my informal notes. Probability notation can be cumbersome and overly verbose. As is customary in machine learning and many of the sciences, I&amp;rsquo;m not going to bother using probability notation correctly. That is to say, I&amp;rsquo;m going to mangle and confuse probability measures, random variables, and probability mass/density functions. Hopefully readers can figure out the intended meaning of my notation from context, and I try to clarify when needed.&lt;/p&gt;
&lt;p&gt;In general, figuring out good notational conventions for probability in many fields (I&amp;rsquo;m looking at you machine learning, but the neuroscience free energy literature also has this problem) seems to be an unsolved problem, and one that I&amp;rsquo;d like to work on at some point! However that is not my concern here. I&amp;rsquo;m just taking the easiest route to expressing my knowledge. If you want to know what &amp;ldquo;proper&amp;rdquo; probability notation looks like, check out &lt;a href=&#34;http://zhat.io/articles/primer-probability-theory&#34;target=&#34;_blank&#34;&gt;my post on probability theory&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;bayesian-basics&#34;&gt;Bayesian basics&lt;/h2&gt;
&lt;p&gt;Suppose $\mc{H}$ is a set of possible hidden states, and $\mc{X}$ is a set of possible observations. Each $h\in\mc{H}$ &lt;strong&gt;explains&lt;/strong&gt; data $x\in\mc{X}$ through the &lt;strong&gt;conditional data distribution&lt;/strong&gt; $p_{X \mid H}(x \mid h)$, or also notated $p_{X \mid H=h}(x)$. A &lt;strong&gt;Bayesian mixture&lt;/strong&gt; is the mixture distribution&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p_{X,H}(x,h) = p_H(h)\cdot p_{X \mid H}(x \mid h)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $p_H(h)$ is called the &lt;strong&gt;prior&lt;/strong&gt;. We can then compute the &lt;strong&gt;posterior&lt;/strong&gt; $p_{H \mid X}(h\mid x)$. The marginal distribution $p_X$ is called the &lt;strong&gt;subjective data distribution&lt;/strong&gt;, since it is partly determined by the choice of prior (if we assume the prior is subjective, i.e. quantifies personal belief). $p_X$ is an agent&amp;rsquo;s best prediction for what they will observe given what they believe (the prior).&lt;/p&gt;
&lt;h2 id=&#34;the-philosophy-of-bayesian-information&#34;&gt;The philosophy of Bayesian information&lt;/h2&gt;
&lt;p&gt;There are different ways to interpret quantity of information information. In the Bayesian setting, I like to think about the amount by which a possibility space was narrowed down. A probability $p_X(x)$ on $x\in\mc{X}$ represents the fraction of possibilities that remain after observing $x$. If we suppose there is a finite possibility space $\Omega$, and the function $X : \Omega \to \mc{X}$ is a random variable that tells us &amp;ldquo;which $x$&amp;rdquo; a given $\omega\in\Omega$ encodes, then the probability of $x$ is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p_X(x) = \frac{\abs{\set{\omega\in\Omega : X(\omega) = x}}}{\abs{\Omega}}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is the number of $\omega$s that encode $x$ over the total number of possibilities. This setup is supposing that $x$ is a &lt;strong&gt;partial observation&lt;/strong&gt;, meaning that even after observing $x$, we still don&amp;rsquo;t know which $\omega$ was drawn.&lt;/p&gt;
&lt;p&gt;In bits, we have $-\lg p_X(x)$, which is the number of halvings of the possibility space $\Omega$ due to observing $x$. Bits and probabilities can be viewed as different units for the same quantity. $-\lg p_X(x)$ is called the &lt;strong&gt;self-information&lt;/strong&gt; of $x$, and in the Bayesian setting, the &lt;strong&gt;surprise&lt;/strong&gt; due to observing $x$, implying that a higher number of bits makes $x$ more surprising, which makes sense because $x$ caused you to rule out much more of your possibility space.&lt;/p&gt;
&lt;p&gt;The connection between $p_X$, information gain $-\lg p_X(x)$, and a physical agent, is that for an agent to have a possibility space, it must have the physical representational capacity. If we presume that a system bounded in a finite region of space contains finite information, i.e. can only occupy finitely many distinguishable states, then our agent must have a finite possibility space $\Omega$ in its &lt;strong&gt;model&lt;/strong&gt; of the environment. Gaining information $-\lg p_X(x)$ requires that the agent physically update its internal possibility space, reducing it by the amount $p_X(x)$. That is to say, information gain quantifies a physical update to an agent. In this sense, subjective information quantifies a change to an agent due to its model of the environment and observations, whereas objective information quantifies a change in the environment.&lt;/p&gt;
&lt;p&gt;Note that $p_X$ may be the marginal distribution of $p_{X,H}$, in which case $p_X$ is a subjective data distribution. It is not strictly necessary to actually define hypotheses and priors. $p_X$ can be regarded as a prior, as the end result is the same: the agent has some possibility space, reflecting what the agent is capable of believing, and the proportions of each $x\in\mc{X}$ in that possibility space correspond to how confident the agent is in any given outcome relative to other outcomes.&lt;/p&gt;
&lt;p&gt;If $\Omega$ is infinite (countable or uncountable), then we cannot just divide by the size of $\Omega$ to compute probabilities. In this case, we need to provide a measure that tells us how much of the possibility space $\Omega$ any subset is worth, i.e. $P(A)$ for $A \subseteq \Omega$ measures the fraction of $\Omega$ that $A$ is worth even when $\Omega$ and $A$ are infinite. $P$ is called a &lt;strong&gt;probability measure&lt;/strong&gt;, but don&amp;rsquo;t worry about that. The point is that even in the case of infinite possibilities, we can still think of information gain in terms of narrowing down a possibility space.&lt;/p&gt;
&lt;h2 id=&#34;bayesian-surprise&#34;&gt;Bayesian surprise&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Mutual_information&#34;&gt;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Mutual_information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let $\mc{X}$ be data space, $\mc{H}$ be hypothesis space, and $X,H$ be data and hypothesis random variables.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
I[X,H] &amp;amp;= \E_X[\kl{p_{H\mid X}}{p_H}] \\&lt;br&gt;
&amp;amp; = \E_{x\sim X}[H(p_{H\mid X=x}, p_H) - H(p_{H\mid X=x})] \\&lt;br&gt;
&amp;amp; = \E_{x\sim X}[\E_{h \sim p_{H\mid X=x}}[-\lg p_H(h)] - \E_{h \sim p_{H\mid X=x}}[-\lg p_{H\mid X}(h \mid x)]] \\&lt;br&gt;
&amp;amp; = \E_{x,h \sim p_{X,H}}\left[\lg \left( \frac{p_{H\mid X}(h\mid x)}{p_H(h)}\right)\right] \\&lt;br&gt;
&amp;amp; = \sum_{x,h \in \mc{X} \times \mc{H}}p_{X,H}(x,h)\lg \left( \frac{p_{H\mid X}(h\mid x)}{p_H(h)}\right) \\&lt;br&gt;
&amp;amp; = \sum_{x,h \in \mc{X} \times \mc{H}}p_{X,H}(x,h)\lg \left( \frac{p_{X,H}(x,h)}{p_X(x)p_H(h)}\right)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$I[X,H]$ is called &lt;strong&gt;Bayesian surprise&lt;/strong&gt;, which is the expected (over data) KL divergence from your prior to posterior (after observing data), which is itself the expected difference in uncertainty (measured in bits, the number of halvings of the full possibility space).&lt;/p&gt;
&lt;p&gt;Pointwise Bayesian information gain (information gained about hypothesis $h$ from data $x$) is $\lg (1/p_H(h)) - \lg (1/p_{H \mid X}(h \mid x)) = \lg (1/p_X(x)) - \lg (1/p_{X \mid H}(x\mid h))$, which is the change in amount of hypothesis weight (posterior mass) that shifted onto $h$.&lt;/p&gt;
&lt;p&gt;$\lg (1/p_{X \mid H}(x \mid h))$ is the &lt;strong&gt;surprise&lt;/strong&gt; (also &lt;strong&gt;self-information&lt;/strong&gt;) of observing $x$ under $h$. The higher this quantity, the more the possibility space of $h$ was narrowed down by $x$.&lt;/p&gt;
&lt;h2 id=&#34;variational-bayes&#34;&gt;Variational Bayes&lt;/h2&gt;
&lt;p&gt;Variational approximation to calculating the Bayesian posterior:&lt;br&gt;




  &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;The free-energy principle a unified brain theory#Free Energy&lt;/a&gt;


&lt;/p&gt;
&lt;p&gt;sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&#34;&gt;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1901.07945&#34;target=&#34;_blank&#34;&gt;What does the free energy principle tell us about the brain?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose $x\in\mc{X}$ is observed and the posterior $p_{H \mid X=x}$ is intractable to compute. We can instead approximate it by minimizing&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
q_x^* = \argmin{q \in \mc{Q}} \kl{q}{p_{H \mid X=x}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\mc{Q}$ is some set of probability mass functions $q : \mc{H} \to [0, 1]$, chosen for convenience.&lt;/p&gt;
&lt;p&gt;Assuming we cannot perform this minimization directly, we can make use of the identity&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\kl{q}{p_{H \mid X=x}} = \mc{F}[q] - \lg (1/p_X(x))&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= \kl{q}{p_{H,X=x}} \\&lt;br&gt;
&amp;amp;= \E_{h \sim q} \lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right) \\&lt;br&gt;
&amp;amp;= \sum_{h\in\mc{H}} q(h) \lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right)&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is the &lt;strong&gt;free energy&lt;/strong&gt;. $\lg (1/p_X(x))$ is the expected surprise of $x$ across all hidden states (weighted by the prior $p_H$).&lt;/p&gt;
&lt;p&gt;Free energy also equals&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= H(q, p_{H, X=x}) - H(q) \\&lt;br&gt;
&amp;amp;= \E_{h \sim q}\left[\lg\left(\frac{1}{p_{H, X}(h,x)}\right) - \lg\left(\frac{1}{q(h)}\right) \right] \\&lt;br&gt;
&amp;amp;= \sum_{h \in \mc{H}} q(h) \lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right)\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $H(q)$ is the entropy of $q$, and $H(q, p_{H, X=x})$ is the &lt;strong&gt;total energy&lt;/strong&gt;, which is equal to the cross-product of $p_{H, X=x}$ under $q$.&lt;/p&gt;
&lt;p&gt;Note that $p_{H, X=x}(h) = p_{H,X}(h,x)$ is not the same as the conditional distribution $p_{H \mid X=x}(h) = p_{H,X}(h,x) / p_X(x)$, and is not a valid probability distribution because its unnormalized.&lt;/p&gt;
&lt;p&gt;We also have free energy as &lt;strong&gt;complexity&lt;/strong&gt; minus &lt;strong&gt;accuracy&lt;/strong&gt;,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= \kl{q}{p_H} - \E_{h\sim q}\left[\lg p_{X \mid H}(x \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q}\left[\lg\left(\frac{q(h)}{p_H(h)}\right)-\lg p_{X \mid H}(x \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q}\left[\lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right)\right]\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This form of free energy can be used in practice. Given any particular $q$ (e.g. as a neural network), the complexity $\kl{q}{p_H}$ and the accuracy $\E_{h\sim q}\left[\lg p_{X \mid H}(x \mid h)\right]$ can be approximated using Monte-Carlo sampling from $q$. This is assuming we have access to a prior $p_H$ over hidden states and predictive (or generative) distribution $p_{X\mid H}$. If $p_{H\mid X}$ is intractable, then a suitable $q^*$ that approximately and sufficiently minimizes $\mc{F}[q]$ becomes our approximation of that posterior.&lt;/p&gt;
&lt;p&gt;Note that there is a $q^*$ for every partial observation $x_{1:t}$, i.e. we need to perform another minimization to arrive at $q_{x_{1:t}}^*$ for every $t$.&lt;/p&gt;
&lt;h2 id=&#34;bayesian-inference-over-time&#34;&gt;Bayesian inference over time&lt;/h2&gt;
&lt;p&gt;I am basing this on Solomonoff induction (as formulated by Marcus Hutter in his &lt;a href=&#34;http://www.hutter1.net/ai/uaibook.htm&#34;target=&#34;_blank&#34;&gt;book&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We now suppose the agent observes an endless stream of data over time. The full possible set of observations are all infinite sequences $\mc{X}^\infty$ where $\mc{X}$ is the set of possible observations at each point in time, e.g. a frame of sensory data such as a video or audio frame. Let $X_{a:b}$ be the random variable denoting a slice of the data stream from time $a$ to time $b$  (inclusive). $X_{1:n}$ is the first $n$ timesteps of data, and $X_{n+1:\infty}$ is everything that is observed after time $n$. I will also use the shorthands $X_{&amp;lt;n} = X_{1:n-1}$ and $X_{&amp;gt;n} = X_{n+1:\infty}$.&lt;/p&gt;
&lt;p&gt;Now suppose the agent has a &lt;strong&gt;hypothesis space&lt;/strong&gt; $\mc{M}$, which is a set of probability distributions $\mu\in\mc{M}$. We call each $\mu$ a hypothesis. Let $\pi$ be a probability distribution over $\mc{M}$ (the prior). Then we have a mixture distribution&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(X_{1:n}) = \sum_{\mu\in\mc{M}} \pi(\mu)\cdot\mu(X_{1:n})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If we define the joint distribution&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(X_{1:n},\mu) = \pi(\mu)\cdot\mu(X_{1:n})\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;then we have the usual Bayesian quantities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data likelihood: $p(X_{1:n} \mid \mu) = \mu(X_{1:n})$.&lt;/li&gt;
&lt;li&gt;Hypothesis prior: $p(\mu) = \pi(\mu)$.&lt;/li&gt;
&lt;li&gt;Hypothesis posterior: $p(\mu \mid X_{1:n}) = \pi(\mu)\cdot\mu(X_{1:n})/p(X_{1:n})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can think of a finite data sequence $x_{1:n} \in \mc{X}^n$ as a partial observation that the agent updates its mixture weights on:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
w_\mu(x_{1:n}) = \pi(\mu)\frac{\mu(x_{1:n})}{p(x_{1:n})}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where the hypothesis posterior is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(\mu \mid x_{1:n}) = w_\mu(x_{1:n})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We also have a new quantity, the &lt;strong&gt;data posterior&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(X_{&amp;gt;n} \mid X_{1:n}=x_{1:n}) = \sum_{\mu\in\mc{M}} w_\mu(x_{1:n})\mu(X_{&amp;gt;n} \mid X_{1:n}=x_{1:n})\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which has the same form as the prior mixture, except that we reweighted by switching from $\pi(\mu)$ to $w_\mu(x_{1:n})$, and we conditionalized the hypotheses, i.e. switched from $\mu(X_{1:\infty})$ to $\mu(X_{&amp;gt;n} \mid X_{1:n}=x_{1:n})$.&lt;/p&gt;
&lt;p&gt;We can incorporate actions into this framework by specifying that all hypotheses in $\mu\in\mc{M}$ must be distributions over a combined observation and action stream. This stream would be a sequence $(x_1, a_1, x_2, a_2, \ldots)$ of alternating observation $x_t \in \mc{X}$ and action $a_t \in \mc{A}$ at every time step. Note that a hypothesis predicts the next observation $\mu(x_t \mid x_{&amp;lt;t}, a_{&amp;lt;t})$, but we don&amp;rsquo;t ask a hypothesis to predict a next action, i.e. $a_t$ given $x_{&amp;lt;t}, a_{&amp;lt;t}$, since that is the agent&amp;rsquo;s decision to make.&lt;/p&gt;




  &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;Free energy principle and Solomonoff induction&lt;/a&gt;



&lt;h2 id=&#34;hypotheses-vs-states-policies-vs-actions&#34;&gt;Hypotheses vs states; policies vs actions&lt;/h2&gt;
&lt;p&gt;There is a key distinction to make here: a hypothesis $\mu$ is itself a possible universe. $\mu$ is a distribution over all possible infinite data streams $\mc{X}^\infty$. $\mu$ can be arbitrarily complex, and consider all counterfactual latent states in the universe. $\mu$ may encode the dynamics of a time evolving system in its conditional probabilities $\mu(x_n \mid x_{&amp;lt;n})$. In this way $\mu$ is not a hidden state, but an entire possible universe.&lt;/p&gt;
&lt;p&gt;This is in contrast to the idea of a &lt;strong&gt;hidden state&lt;/strong&gt;, which is an unknown state of the universe &lt;strong&gt;at some point in time&lt;/strong&gt;. In the hidden state framework, the environment is defined by a known dynamics distribution $p(x_t, s_t \mid s_{t-1})$ or $p(x_t, s_t \mid s_{t-1}, a_{t-1})$ if we include actions. The only thing that is unknown is $s_{1:t}$. In this framework, knowing $s_{t-1}$ does not mean you know $s_t$ with certainty. In the mixture of hypotheses framework, if you know $\mu$, you know it for all time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: A hypothesis $\mu$ is static and true for all time, and a hidden state $s_t$ evolves and is true only at time $t$.&lt;/p&gt;
&lt;p&gt;We also need to make this distinction between policies and actions. A &lt;strong&gt;policy&lt;/strong&gt; $\pi$ (not to be confused with hypothesis prior above, but this is the conventional notation) is similar to $\mu$, in that it is a probability distribution over alternating observations and actions. $\pi(a_t \mid x_{&amp;lt;t}, a_{&amp;lt;t})$ specifies the agent&amp;rsquo;s action preferences given what it has already seen and done. If we combine an environment $\mu$ and a policy $\pi$, we can fully model the agent-environment interaction loop, i.e. the joint distribution over the space of combined sequences: $\mc{X}\times\mc{A}\times\mc{X}\times\mc{A}\times\ldots$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: A policy $\pi$ is static, i.e. a single agent choice that remains for all time, and an action $a_t$ is a choice made specifically at time $t$.&lt;/p&gt;
&lt;p&gt;However, it is possible to devise a setup where there is a set of possible policies $\Pi$, and the agent keeps &amp;ldquo;changing its mind&amp;rdquo; about which policy $\pi_t \in \Pi$ to use at time $t$. I find this formulation to be a bit redundant, because $\pi_t$ contains information about the agent&amp;rsquo;s preferences in all possible future situations, but if the agent changes its mind in the next step then that information is essentially overridden. Why not just have the agent choose an action $a_t$? It could make sense to impose a restriction that $\pi_t$ cannot evolve quickly over time, so that the policy represents a high-level choice about what to do in some time window. This is one avenue for formulating hierarchical decision making.&lt;/p&gt;
&lt;h1 id=&#34;free-energy-principle-and-time&#34;&gt;Free energy principle and time&lt;/h1&gt;
&lt;p&gt;This is where my understanding falls apart.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve reviewed two sources:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&#34;&gt;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;1-wikipedia&#34;&gt;1. Wikipedia&lt;/h2&gt;
&lt;p&gt;From the first (Wikipedia):&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210124131932.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
I&amp;rsquo;m reiterating Wikipedia&amp;rsquo;s notation here. Overwrite in your brain my usages of $\mu$ and $s$ above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mu\in R$ is the state of the agent at timestep (not to be confused with hypotheses).&lt;/li&gt;
&lt;li&gt;$a \in A$ is an action taken at every timestep.&lt;/li&gt;
&lt;li&gt;$s \in S$ is an observation at each timestep (not to be confused with environment states).&lt;/li&gt;
&lt;li&gt;$\psi \in \Psi$ is the hidden environment state at every timestep.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is the Bayesian inference being done here? In my exposition on Bayesian inference over time, the posteriors of interest are explicitly given. I&amp;rsquo;d like to know what posterior we are interested in approximating with variational free energy here.&lt;/li&gt;
&lt;li&gt;Is this joint minimization being done simultaneously over all timesteps, or is it done in a greedy fashion on every step?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;2-step-by-step-tutorial&#34;&gt;2. Step-by-Step Tutorial&lt;/h2&gt;
&lt;p&gt;From the second: &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The paper opens with an exposition that looks like it matches my own for time-less free energy minimization.&lt;/p&gt;
&lt;p&gt;Starting on page 16, we introduce policy $\pi$ (not to be confused with the prior). I&amp;rsquo;m confused about the relationship between the policy and the time evolution of the environment-agent loop. Does $\pi$ change over time, or is $\pi$ chosen up front and held fixed? Clearly it cannot be held fixed, because then the agent is not utilizing its free energy minimization to alter its behavior.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m also confused about the posterior approximation $q$. Now, $q(s \mid \pi)$ depends on the policy. Does this mean we run the free energy minimization for every $\pi$, each producing a different $q$? If that&amp;rsquo;s so, then why do we not write $q(s_t \mid o_{&amp;lt;t}, \pi)$ to indicate the dependency of $q$ on the observations $o_{&amp;lt;t}$ as well?&lt;/p&gt;
&lt;p&gt;Page 19 adds more confusion to the mix. We are introduced to a score function $G(\pi)$ for choosing policies $\pi$.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210124142608.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
First of all, I thought there is a joint distribution $p(o,s,\pi)$, implying that $p(\pi)$ is a prior which reflects the agent&amp;rsquo;s preferences over policies. So which is it? Does the agent use $G(\pi)$ or $p(\pi)$ to choose its policy? It&amp;rsquo;s also not clear if $\pi$ is chosen once and held fixed for all time, or if the policy changes over time.&lt;/p&gt;
&lt;p&gt;Second, and more perplexing, is that $G(\pi)$ is an expectation over $q(o,s\mid \pi)$, remember that $q$ is an approximate posterior over hidden states $s$. How can $q$ also be a distribution over observations $o$? Trying to make $q$ a joint distribution over $x$ and $h$ in my time-less free energy exposition above doesn&amp;rsquo;t make sense to me.&lt;/p&gt;
&lt;h1 id=&#34;my-open-questions&#34;&gt;My Open Questions&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Is the agent assumed to have a dynamics model of the environment where all that&amp;rsquo;s unknown to the agent is the environment state? that seems unrealistic. if the agent doesn&amp;rsquo;t know the &amp;ldquo;true&amp;rdquo; dynamics model, by what mechanism would the agent improve its dynamics model? The Bayesian posterior approximation is for estimating the effect of its actions on environment state, but this doesn&amp;rsquo;t address how the agent learns about the relationship between action and state.&lt;/li&gt;
&lt;li&gt;How are time and actions incorporated? I understand the time-less variational free energy formulation that I explained above. What I don&amp;rsquo;t understand is what this looks like when applied to an agent-environment interaction loop over time.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>