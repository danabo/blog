<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning on Dan&#39;s Notepad</title>
    <link>https://danabo.github.io/blog/tags/machine-learning/</link>
    <description>Recent content in machine learning on Dan&#39;s Notepad</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Â©2021 Daniel Abolafia.</copyright>
    <lastBuildDate>Tue, 13 Apr 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://danabo.github.io/blog/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Modular Neural Networks</title>
      <link>https://danabo.github.io/blog/posts/modular-neural-networks/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/modular-neural-networks/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\s}{\sigma}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\H}{\mb{H}}&lt;br&gt;
\newcommand{\I}{\mb{I}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[2]{#1_{\mid #2}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;I wrote up these notes in preparation for my guest lecture in Tom Dean&amp;rsquo;s Stanford course, &lt;a href=&#34;https://web.stanford.edu/class/cs379c/&#34;target=&#34;_blank&#34;&gt;CS379C: Computational Models of the Neocortex&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Selected papers&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.04227&#34;target=&#34;_blank&#34;&gt;Towards Modular Algorithm Induction&lt;/a&gt; (Abolafia et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Modular Networks: Learning to Decompose Neural Computation&lt;/a&gt; (Kirsch et al.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;what&#34;&gt;What?&lt;/h1&gt;
&lt;p&gt;What do the phrases &amp;ldquo;modular architectures&amp;rdquo; and &amp;ldquo;learning modular structures&amp;rdquo; mean?&lt;/p&gt;
&lt;p&gt;In programming, a module is a reusable function. Modularity is a design principle, where code is composed of smaller functions which have well defined behavior in isolation, so that the system can be understood by looking at its parts (i.e. &lt;a href=&#34;https://en.wikipedia.org/wiki/Reductionism#In_science&#34;target=&#34;_blank&#34;&gt;reduction&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Modular code satisfies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Isolation&lt;/strong&gt;: The internal behavior of one module doesn&amp;rsquo;t affect other modules.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reusability&lt;/strong&gt;: The same module applied in different circumstances, potentially given different kinds of data that follow the same pattern (think &lt;a href=&#34;https://en.wikipedia.org/wiki/Generic_programming&#34;target=&#34;_blank&#34;&gt;generics&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Abstract_type&#34;target=&#34;_blank&#34;&gt;abstract types&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the context of machine learning, a modular neural architecture is a type of neural network composed of smaller neural modules. If data can be said to contain modular structure (e.g. see &lt;a href=&#34;https://arxiv.org/abs/1902.07181&#34;target=&#34;_blank&#34;&gt;Andreas 2019&lt;/a&gt;), then one goal of modular neural networks is to recover that latent structure.&lt;/p&gt;
&lt;p&gt;Pictorial examples of modular neural networks:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210413141842.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;[Kirsch et al.](https://arxiv.org/abs/1811.05249)&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210413141514.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;[Chang et al.](https://arxiv.org/abs/1807.04640)&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210413141932.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;[Abolafia et al.](https://arxiv.org/abs/2003.04227)&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;why&#34;&gt;Why?&lt;/h1&gt;
&lt;p&gt;Why have modular neural architectures? Is it beneficial for program synthesis? Is it beneficial for machine learning in general?&lt;/p&gt;
&lt;h2 id=&#34;data-invariances&#34;&gt;Data invariances&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.06765&#34;target=&#34;_blank&#34;&gt;Modularity Matters: Learning Invariant Relational Reasoning Tasks&lt;/a&gt;  (Jo et al.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When a dataset has a large number of invariances, a machine learning model must learn to associate a large number of seemingly unrelated patterns with one another, which may exacerbate the interference problem &amp;hellip; One natural way to combat the interference problem is to allow for specialized sub-modules in our architecture. Once we modularize, we reduce the amount of interference that can occur between features in our model. These specialized modules can now learn highly discriminative yet invariant representations while not interfering with each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;strong-generalization&#34;&gt;Strong generalization&lt;/h2&gt;
&lt;p&gt;Strong generalization means getting the right answer for an input/task that is very different from the training regime. Sometimes this is called zero-shot learning. Humans seem to be able to this. How does it work?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.07088&#34;target=&#34;_blank&#34;&gt;Learning Modular Neural Network Policies for Multi-Task and Multi-Robot Transfer&lt;/a&gt; (Devin et al.)&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210413144242.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;The robot and task networks are trained end-to-end on different robot-task combinations, with some held out. For example, during training this system does not encounter robot 2 combined with task 2, but does encounter robot 2 and task 2 separately in different situations. At test time, the system has to perform well when robot 2 is combined with task 2.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.01796&#34;target=&#34;_blank&#34;&gt;Modular Multitask Reinforcement Learning with Policy Sketches&lt;/a&gt; (Andreas 2016)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The modular structure of our approach, which associates every high-level action symbol with a discrete subpolicy,naturally induces a library of interpretable policy fragments that are easily recombined. This makes it possible to evaluate our approach under a variety of different data conditions: (1) learning the full collection of tasks jointly via reinforcement, (2) in a zero-shot setting where a policy sketch is available for a held-out task, and (3) in a adaptation setting, where sketches are hidden and the agent must learn to adapt a pretrained policy to reuse high-level actions in a new task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;hellip; we have shown that it is possible to build agents that share behavior across tasks in order to achieve success in tasks with sparse and delayed rewards. This process induces an inventory of reusable and interpretable subpolicies which can be employed for zero-shot generalization when further sketches are available, and hierarchical reinforcement learning when they are not.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;parameter-sharing&#34;&gt;Parameter sharing&lt;/h2&gt;
&lt;p&gt;Convolutional and recurrent layers can be viewed as modular, in that they &amp;ldquo;stamp&amp;rdquo; a small neural network (with the same parameters) repeatedly in some pattern - repeated over space for CNNs, and repeated over time for RNNs.&lt;/p&gt;
&lt;h2 id=&#34;causal-learning&#34;&gt;Causal learning&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.10893&#34;target=&#34;_blank&#34;&gt;Recurrent Independent Mechanisms&lt;/a&gt; (Goyal et al.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Physical processes in the world often have a modular structure which human cognition appears toexploit, with complexity emerging through combinations of simpler subsystems. Machine learningseeks to uncover and use regularities in the physical world. Although these regularities manifestthemselves as statistical dependencies, they are ultimately due to dynamic processes governed bycausal physical phenomena.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The notion of independent or autonomous mechanisms has been influential in the field of causal inference. A complex generative model, temporal or not, can be thought of as the composition of independent mechanisms or âcausalâ modules. In the causality community, this is often considered a prerequisite for being able to perform localized interventions upon variables determined by such models (Pearl, 2009). It has been argued that the individual modules tend to remain robust or invariant even as other modules change, e.g., in the case of distribution shift (SchÃ¶lkopf et al., 2012; Peterset al., 2017). This independence is not between the random variables being processed but between the description or parametrization of the mechanisms: learning about one should not tell us anything about another, and adapting one should not require also adapting another. One may hypothesize that if a brain is able to solve multiple problems beyond a single i.i.d. (independent and identically distributed) task, they may exploit the existence of this kind of structure by learning independent mechanisms that can flexibly be reused, composed and re-purposed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An excerpt from Judea Pearl&amp;rsquo;s book &lt;a href=&#34;http://bayes.cs.ucla.edu/BOOK-2K/&#34;target=&#34;_blank&#34;&gt;Causality, 2nd ed.&lt;/a&gt;. Pearl refers to &amp;ldquo;mechanisms&amp;rdquo; as the nodes in a Bayesian network (e.g. depicted in figure 1.2), which are assumed to be modular: i.e. they are internally isolated, apart from causation traveling along their arrows, and they are reusable in the sense that the graph can be modified, which Pearl calls an intervention.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210413224425.png&#34; alt=&#34;&#34;&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The example reveals a stronger sense in which causal relationships are more stable than the corresponding probabilistic relationships, a sense that goes beyond their basic ontologicalâepistemological difference. The relationship, âTurning the sprinkler on would not affect the rain,â will remain invariant to changes in the mechanism that regulates how seasons affect sprinklers. In fact, it remains invariant to changes in all mechanisms shown in this causal graph. We thus see that causal relationships exhibit greater robustness to ontological changes as well; they are sensitive to a smaller set of mechanisms. More specifically, and in marked contrast to probabilistic relationships, causal relationships remain invariant to changes in the mechanism that governs the causal variables ($X_3$ in our example).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Regardless of what use is eventually made  of our âunderstandingâ of things, we surely would prefer an understanding in terms of durable relationships, transportable across situations, over those based on transitory relationships. The sense of âcomprehensibilityâ that accompanies an adequate explanation is a natural by-product of the transportability of (and hence of our familiarity with) the causal relationships used in the explanation. It is for reasons of stability that we regard the falling barometer as predicting but not explaining the rain; those predictions are not transportable to situations where the pressure surrounding the barometer is controlled by artificial means. True understanding enables predictions in such novel situations, where some mechanisms change and others are added. It thus seems reasonable to suggest that, in the final analysis, the explanatory account of causation is merely a variant of the manipulative account, albeit one where interventions are dormant. Accordingly, we may as well view our unsatiated quest for understanding âhow data is generatedâ or âhow things workâ as a quest for acquiring the ability to make predictions under a wider range of circumstances, including circumstances in which things are taken apart, reconfigured, or undergo spontaneous change.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;program-induction-and-synthesis&#34;&gt;Program induction and synthesis&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1804.00218&#34;target=&#34;_blank&#34;&gt;HOUDINI: Lifelong Learning as Program Synthesis&lt;/a&gt; (Valkov et al.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In contrast to standard methods for transfer learning in deep networks, which re-use the first few layers of the network, neural libraries have the potential to enable reuse of higher, more abstract levels of the network, in what could be called high-level transfer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;hellip; our results indicate that the modular representation used in HOUDINI allows it to transfer high-level concepts and avoid negative transfer. We demonstrate that HOUDINI offers greater transfer than progressive neural networks and traditional âlow-levelâ transfer, in which early network layers are inherited from previous tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.13404&#34;target=&#34;_blank&#34;&gt;Towards modular and programmable architecture search&lt;/a&gt; (Negrinho et al.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The building blocks of our search spaces are modules and hyper-parameters. Search spaces are created through the composition of modules and their interactions.Implementing a new module only requires dealing with aspects local to the module. Modules and hyperparameters can be reused across search spaces, and new search spaces can be written by combining existing search spaces.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;how&#34;&gt;How?&lt;/h1&gt;
&lt;p&gt;How can modularity be achieved? Two things need to be simultaneously learned:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The modules themselves.&lt;/li&gt;
&lt;li&gt;How the modules are to be connected together.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Key ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Routing&lt;/strong&gt;: How the modules are connected together.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt; vs &lt;strong&gt;static&lt;/strong&gt; routing: Static routing is fixed for all inputs, while dynamic routing is conditioned on a given input or context. A router is a function that outputs module routing given context.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Soft&lt;/strong&gt; vs &lt;strong&gt;hard&lt;/strong&gt; routing: Soft routing is a weighted sum across all choices, while hard routing is a single discrete choice. Soft routing is differentiable while hard routing is not.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples of soft routing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1410.5401&#34;target=&#34;_blank&#34;&gt;Neural Turing Machines&lt;/a&gt; (Graves et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/nature20101&#34;target=&#34;_blank&#34;&gt;Differentiable neural computers&lt;/a&gt; (Graves et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.08228&#34;target=&#34;_blank&#34;&gt;Neural GPUs Learn Algorithms&lt;/a&gt; (Kaiser et al.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples of hard routing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1807.04640&#34;target=&#34;_blank&#34;&gt;Automatically Composing Representation Transformations as a Means for Generalization&lt;/a&gt; (Chang et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.06392&#34;target=&#34;_blank&#34;&gt;Neural Random-Access Machines&lt;/a&gt; (Kurach et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.07275&#34;target=&#34;_blank&#34;&gt;Learning Simple Algorithms from Examples&lt;/a&gt; (Zaremba et al.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is there something in between soft and hard routing?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1701.06538&#34;target=&#34;_blank&#34;&gt;Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer&lt;/a&gt; (Shazeer et al.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Modular Networks: Learning to Decompose Neural Computation&lt;/a&gt; (Kirsch et al.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;module-routing-in-detail&#34;&gt;Module routing in detail&lt;/h2&gt;
&lt;p&gt;Following the setup in &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;In the case of soft routing, think of module weights as probabilities. Instead of summing module outputs, sum probabilities of each possible output. This leads to a natural correspondence between soft and hard routing, where hard routing is sampled from the probability distribution.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt;, the choices are organized into layers, where for each layer $l$ a subset of $M$ modules are selected and their outputs are summed. In &lt;a href=&#34;https://arxiv.org/abs/2003.04227&#34;target=&#34;_blank&#34;&gt;Abolafia et al.&lt;/a&gt;, modules are connected into arbitrary computation graphs.&lt;/p&gt;
&lt;p&gt;To keep things general, let $\mc{A}$ be a set of possible module routing choices, so that an element $a\in\mc{A}$ consists of all  choices needed for the architecture to produce an output (e.g. which modules to use in the computation graph and how they are connected).&lt;/p&gt;
&lt;p&gt;Given a routing choice $a\in\mc{A}$, the architecture output probability given input $x$ and parameters $\t$ is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y\mid x,a,\t)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Under the hood this output probability may be arrived at by combining the following components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A collection of differentiable functions (modules) $f_{\t\up{i}}\up{i} : \mc{Z}\up{i} \to \mc{Z}\up{j}$ where $\mc{Z}\up{i}$ and $\mc{Z}\up{j}$ are latent spaces (e.g. vectors in $\R^n$), and $\t\up{i}$ are the function parameters. Typically $\t = (\t\up{1},\t\up{2},\dots)$ and each module has independent and isolated parameters. The input $x$ may be initially encoded into a latent space, or some modules may have $\mc{Z} = \mc{X}$ and the data can be fed in directly.&lt;/li&gt;
&lt;li&gt;A function $g$ that takes routing specification $a$, modules $\set{f_i}$, and input $x$, and outputs hidden representation $h$ (also a real vector).&lt;/li&gt;
&lt;li&gt;A distribution &amp;ldquo;head&amp;rdquo; on output space $\mc{Y}$, s.t. $h$ represents the parameters of the distribution. For example, a Gaussian $\mc{N}(y \mid h)$ where $h$ encodes a vector of means and a covariance matrix.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Putting these three things together produces the probability distribution $p(y\mid x,a,\t)$. Given training target $y$, this probability is fully differentiable w.r.t. $\t$ (typically given by the architecture as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logit&#34;target=&#34;_blank&#34;&gt;logit&lt;/a&gt;, or log-probability). If routing choice $a$ were held fixed, we can train this architecture with standard supervised techniques, e.g. with SGD. Furthermore, if $a\up{k}$ can be provided by some external hard-coded system given training example $(x\up{k}, y\up{k})$, then we can use SGD to maximize the dataset log-probability&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\sum_{k=1}^N \log p(y\up{k}\mid x\up{k},a\up{k},\t)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is essentially the used in &lt;a href=&#34;https://arxiv.org/abs/1807.04640&#34;target=&#34;_blank&#34;&gt;Chang et al.&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1511.02799&#34;target=&#34;_blank&#34;&gt;Andreas et al.&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ideally, we&amp;rsquo;d like to learn the module routing $a\up{k}$ for each training example $(x\up{k}, y\up{k})$. That means learning a routing function, which produces a routing $a\up{k}$ given input $x\up{k}$. Assuming $\mc{A}$ is a discrete space (routing choices are discrete, e.g. which modules to pick and how to connect them), we cannot differentiate such a function. RL could be used, as in &lt;a href=&#34;https://arxiv.org/abs/2003.04227&#34;target=&#34;_blank&#34;&gt;Abolafia et al.&lt;/a&gt;, but &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt; provides a more general perspective that allows for differentiation in principle, though intractable in practice.&lt;/p&gt;
&lt;p&gt;Let the routing function output a probability distribution over $a\in\mc{A}$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(a \mid x, \p)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This router is comprised of a function $r_\p$, parameterized by parameters $\p$, that takes in $x$ and outputs a hidden representation $h&#39;$ that, like before, holds the parameters for some probability distribution on $\mc{A}$ (e.g. Gaussian). If $a$ decomposes into a set of independent routing choices, e.g. $a=(a_1, a_2, \dots)$, then $p(a \mid x, \p) = \prod_t p(a_t \mid z_t, \p)$ where $\set{z_t}$ are potentially intermediate outputs from various modules, and at least one $z_t$ equals $x$.&lt;/p&gt;
&lt;p&gt;This gives us a joint distribution on $y$ and $a$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y, a \mid x,\t,\p) = p(y\mid x,a,\t)p(a\mid x, \p)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;To find the probability of $y$ given $x$, independent of routing choice $a$, we marginalize out $a$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y \mid x, \t, \p) = \sum_{a\in\mc{A}} p(y\mid x,a,\t)p(a\mid x, \p)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;What this means in terms of training, is that if we are using naive SGD to maximize $\log p(y \mid x, \t, \p)$ w.r.t $\t$ and $\p$ jointly, then we simply sum over all possible routing choices in $\mc{A}$. In practice, this summation is intractable, since the number of routing decisions explodes as the number of modules is increased (as well as with other complexities like multi-input and multi-output modules).&lt;/p&gt;
&lt;p&gt;We can view the same optimization through the lense of the REINFORCE algorithm, which naturally leads to RL optimization, where $p(a\mid x, \p)$ is the policy, $a$ are actions, and $x$ are environment observations. The reward function is then $R(a\mid x) = p(y\mid x,a,\t)$ (or the log-probability of $y$ like in &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt;). Let the loss function $\mc{L} = p(y \mid x, \t, \p)$. Then the gradient $\nabla_\p \mc{L}$ is given using the &lt;a href=&#34;https://dallascard.github.io/the-reinforce-trick.html&#34;target=&#34;_blank&#34;&gt;&amp;ldquo;log-trick&amp;rdquo;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\nabla_\p \mc{L} &amp;amp;= \nabla_\p\E_{p(a\mid x, \p)}[p(y\mid x,a,\t)] \\&lt;br&gt;
&amp;amp;= \nabla_\p\E_{p(a\mid x, \p)}[R(a \mid x)] \\&lt;br&gt;
&amp;amp;= \E_{p(a\mid x, \p)}[\nabla_\p\log p(a\mid x, \p) R(a \mid x)]\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is the gradient as given in REINFORCE.&lt;/p&gt;
&lt;p&gt;For the experiments in &lt;a href=&#34;https://arxiv.org/abs/2003.04227&#34;target=&#34;_blank&#34;&gt;Abolafia et al.&lt;/a&gt;, I found that scaling up episode collection as much as possible using &lt;a href=&#34;https://arxiv.org/abs/1802.01561&#34;target=&#34;_blank&#34;&gt;IMPALA&lt;/a&gt; partially overcame the usual issues associated with RL: sparse reward, high variance gradients, and lack of stability. Stability is especially an issue when the modules are being trained at the same time, so that the reward being optimized is a moving target. We were not able to jointly learn module and routing, so in &lt;a href=&#34;https://arxiv.org/abs/2003.04227&#34;target=&#34;_blank&#34;&gt;Abolafia et al.&lt;/a&gt; we settled for hard coded modules and focused on getting the router to work.&lt;/p&gt;
&lt;h2 id=&#34;other-kinds-of-training&#34;&gt;Other kinds of training&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt; introduces a modified &lt;a href=&#34;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&#34;target=&#34;_blank&#34;&gt;EM algorithm&lt;/a&gt; for simultaneously training $\t$ and $\p$, where routing choice $a$ is the latent variable. As with EM, this is a two step iterative process, where joint probability of $a$ and $x$ is maximized with fixed $a$, and then $a$ is locally improved according to the current probability distribution. Rather than finding the argmax $a$, which is intractable, &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt; samples an i.i.d. batch from $p(a \mid x, \p)$ and updates $a$ only if a higher probability $a$ is found.&lt;/p&gt;
&lt;p&gt;This training algorithm can be viewed as performing policy gradient training (RL) with a &amp;ldquo;top-$K$&amp;rdquo; buffer, as described in &lt;a href=&#34;https://arxiv.org/abs/1801.03526&#34;target=&#34;_blank&#34;&gt;Neural Program Synthesis with Priority Queue Training&lt;/a&gt; (Abolafia 2018), where $K=1$.&lt;/p&gt;
&lt;p&gt;Here is the general case of this training procedure:&lt;/p&gt;
&lt;p&gt;Let $\tilde{A}_n$ be a length-$S$ i.i.d. sample from $p(a_n \mid x_n,\p)$.&lt;br&gt;
Let $A_n^*$ be a top-$K$ buffer for the $n$-th training example $(x\up{n},y\up{n})$. That means $A_n^*$ holds the best $a_n$ observed over the course of training, scored by $p(a_n \mid x_n,\p)$. Note that this is a moving target, since $\p$ is simultaneously changing, so the joint score of $A_n^*$ can decrease.&lt;br&gt;
Let $A_n = \tilde{A}_n \cup A_n^*$.&lt;/p&gt;
&lt;p&gt;Let $B \subseteq D$ be a minibatch. The Monte Carlo gradient approximation (ala REINFORCE) is:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\nabla_\p\mc{L} \approx \frac{1}{\abs{B}}\sum_{n\in B}\frac{1}{\abs{A_n}}\sum_{a_n \in A_n} \nabla_\p\log p(a\mid x, \p) R(a \mid x)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $R(a\mid x) = p(y\mid x,a,\t)$ or $\log p(y\mid x,a,\t)$.&lt;/p&gt;
&lt;p&gt;If $A_n^* = \emptyset$ this setup reduces to usual policy gradient training, and if $\tilde{A}_n = \emptyset$ and $\abs{A_n^*}=1$ (i.e. $K=1$) this reduces to the EM algorithm (Algorithm 1) in &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.03526&#34;target=&#34;_blank&#34;&gt;Abolafia 2018&lt;/a&gt; is similar (where $K\geq 1$), except that the reward function is assumed to be fixed through out training, so the top-$K$ buffer $A_n^*$ cannot diminish in total score over time.&lt;/p&gt;
&lt;h3 id=&#34;information-theory&#34;&gt;Information theory&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt; plots the quantities $H_a$ and $H_b$ as diagnostic tools and measures of &amp;ldquo;module collapse&amp;rdquo; and training convergence. I used the same quantities in my own experiments and found them to be similarly helpful.&lt;/p&gt;
&lt;p&gt;There is theoretical justification for these quantities, and they may even be used as regularizers.&lt;/p&gt;
&lt;p&gt;Let $A$ be the routing random variable and let $X$ be the input random variable, distributed jointly by $p(a \mid x, \p)p(x)$ where $p(x)$ is the true and unknown input distribution.&lt;/p&gt;
&lt;p&gt;The mutual information between $A$ and $X$ decomposes:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\I(A, X) = \H(A) - \H(A \mid X)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We cannot explicitly compute these entropy terms because we do not have access to $p(x)$ or $p(a \mid \p)$.&lt;/p&gt;
&lt;p&gt;Assume we can only explicitly compute $p(a \mid x, \p)$. Let&amp;rsquo;s also suppose that we can explcitily compute $\H(A \mid X=x) = -\sum_{a\in\mc{A}} p(a \mid x, \p) \log p(a \mid x, \p)$. Though summing over $\mc{A}$ is still intractable, if we suppose that the routing distribution factors into independent choices which are themselves tractable to enumerate (like the choice for each layer in &lt;a href=&#34;https://arxiv.org/abs/1811.05249&#34;target=&#34;_blank&#34;&gt;Kirsch et al.&lt;/a&gt;): $\mc{A} = \mc{A}_1\times\mc{A}_2\times\dots$ and $a = (a_1, a_2, \dots) \in \mc{A}$ s.t.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(a \mid x, \p) = \prod_{l=1}^L p(a_l \mid x, \p_l)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;then we can tractably compute the conditional entropy as the sum of entropies of each choice:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\H(A \mid X=x) &amp;amp;= \sum_{l=1}^L \H(A_l \mid X=x) \\&lt;br&gt;
&amp;amp;= \sum_{l=1}^L \sum_{a_l\in\mc{A}_l} p(a_l \mid x, \p_l) \log p(a_l \mid x, \p_l)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Now we can do a Monte Carlo approximation of the entropy terms of interest using the existing dataset. The statistical properties of this MC estimation is discussed in &lt;a href=&#34;https://arxiv.org/abs/1905.06922&#34;target=&#34;_blank&#34;&gt;On Variational Bounds of Mutual Information&lt;/a&gt;. Let $B \subseteq D$ be a minibatch uniformly sampled from dataset $D$.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\H(A \mid X) &amp;amp;= -\E_{p(x)}[\H(Y\mid X=x)] \\&lt;br&gt;
&amp;amp;\approx \frac{1}{\abs{B}}\sum_{n \in B} \H(Y\mid X=x_n)&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Letting $q(a \mid \p)$ be a Monte Carlo approximation of the marginal distribution $p(a\mid \p)$, derived from&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(a \mid \p) &amp;amp;= \E_{p(x)}p(a \mid x, \p) \\ &amp;amp;\approx \frac{1}{\abs{B}}\sum_{n \in B}p(a \mid x, \p) \\&lt;br&gt;
&amp;amp;= q(a\mid \p)\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;we can approximate the marginal entropy,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\H(A) &amp;amp;= -\E_{p(a \mid \p)}[\log p(a \mid \p)] \\&lt;br&gt;
&amp;amp;\approx -\E_{q(a \mid \p)}[\log q(a\mid \p)]\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We can think of $\I$ as measuring the bijectivness of the mapping from $x$ to $a$, where $\H(A)$ measures surjectivity and $\H(A\mid X)$ measures anti-injectivity. See &lt;a href=&#34;http://zhat.io/articles/primer-shannon-information#expected-mutual-information&#34;&gt;http://zhat.io/articles/primer-shannon-information#expected-mutual-information&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;$\H(A)$ measures module use. If its maximized, all the modules are getting used equally often.&lt;br&gt;
$\H(A\mid X)$ measures convergence. If its maximized, then the router is completely confident that there is exactly one appropriate routing choice for any given input.&lt;/p&gt;
&lt;p&gt;The ideal is that all modules get used often and the router thinks there is one routing choice per input.&lt;/p&gt;
&lt;p&gt;$\H(A)$ is a typical RL regularizer (called entropy regularization, see &lt;a href=&#34;https://arxiv.org/abs/1602.01783&#34;target=&#34;_blank&#34;&gt;A3C&lt;/a&gt;). However maximizing $\I(A,X)$ (where $x$ are env states) is uncommon. It may be unnecessary since $p(a \mid x)$ naturally becomes peaky over the course of training.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian information theory</title>
      <link>https://danabo.github.io/blog/posts/bayesian-information-theory/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/bayesian-information-theory/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\mf}{\mathfrak}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\D}{\Delta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\par}[1]{\left(#1\right)}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
\newcommand{\dom}[2]{#1_{\mid #2}}&lt;br&gt;
\newcommand{\df}{\overset{\mathrm{def}}{=}}&lt;br&gt;
\newcommand{\M}{\mc{M}}&lt;br&gt;
\newcommand{\up}[1]{^{(#1)}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\newcommand{\H}{\Omega}$&lt;/p&gt;
&lt;p&gt;Shannon&amp;rsquo;s information theory defines quantity of information (e.g. &lt;a href=&#34;https://en.wikipedia.org/wiki/Information_content#Definition&#34;target=&#34;_blank&#34;&gt;self-information&lt;/a&gt; $-\lg p(x)$) in terms of probabilities. In the context of data compression, these probabilities are given a frequentist interpretation (Shannon makes this interpretation explicit in his &lt;a href=&#34;http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf&#34;target=&#34;_blank&#34;&gt;1948 paper&lt;/a&gt;). In 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/&#34;&gt;Deconstructing Bayesian Inference&lt;/a&gt;
  


, I introduced the idea of a subjective data distribution. If quantities of information are calculated using a subjective data distribution, what is their meaning? Below I will answer this question by building from the ground up a different notion of Bayesian inference.&lt;/p&gt;
&lt;p&gt;My thesis is that subjective (Bayesian) probabilities quantify non-determinism, rather than randomness (where non-determinism means that something takes on more than one value, i.e. is a set rather than a single value). Below I motivate the idea that quantity of information based on non-determinism can be interpreted as measuring the reduction in size (&amp;ldquo;narrowing down&amp;rdquo;) of a possibility space.&lt;/p&gt;
&lt;h1 id=&#34;information-and-finite-possibilities&#34;&gt;Information and finite possibilities&lt;/h1&gt;
&lt;p&gt;Following 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/&#34;&gt;Deconstructing Bayesian Inference&lt;/a&gt;
  


, let&amp;rsquo;s suppose an agent is predicting the continuation of a data string, and the agent&amp;rsquo;s prediction is not uniquely determined, i.e. non-deterministic. I will represent the agent&amp;rsquo;s prediction as a set of possible predictions, called the agent&amp;rsquo;s &lt;strong&gt;hypothesis set&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Formally, the agent receives an endless stream of data $\o$ drawn from the set $\X^\infty$, where $\X$ is some finite character set. In the examples below, let&amp;rsquo;s assume $\X = \set{0,1}$. Given some finite sequence $x\in\X^*$, a prediction is a continuation (not necessarily the correct continuation), i.e. an infinite sequence starting with prefix $x$.&lt;/p&gt;
&lt;p&gt;Let $\H \subseteq \X^\infty$ be the agent&amp;rsquo;s hypothesis set. When finite data $x$ is observed, we narrow down $\H$ to the subset of all sequences starting with $x$. This is called &lt;strong&gt;conditionalizing&lt;/strong&gt; (or &lt;strong&gt;restriction&lt;/strong&gt;). Denote $\dom{\H}{x} = \set{\o\in\H \mid x\sqsubset\o}$ as the subset of $\H$ consisting of sequences starting with the prefix $x$. The set $\dom{\H}{x}$ is $\H$ conditioned on $x$.&lt;/p&gt;
&lt;p&gt;For example, a rigid agent that only ever predicts $0$s no matter what has the following hypothesis set:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\H = \set{0000000000\dots}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Alternatively, consider:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\H = \set{0000000000\dots, 1111111111\dots}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Before observing anything, the agent&amp;rsquo;s prediction for the first timestep is not determined - it could be 0 or it could be 1. When the first bit is observed, be it a 0 or a 1, the agent&amp;rsquo;s predictions after that become fully determined: $\dom{\H}{0} = \set{0000000000\dots}$ and $\dom{\H}{1} = \set{1111111111\dots}$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider a more complex hypothesis set:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\H = \{&amp;amp;0000000000\dots,&lt;br&gt;
\\&amp;amp;0100000000\dots,&lt;br&gt;
\\&amp;amp;1000000000\dots,&lt;br&gt;
\\&amp;amp;1001111111\dots,&lt;br&gt;
\\&amp;amp;1010101010\dots,&lt;br&gt;
\\&amp;amp;1101100110\dots,&lt;br&gt;
\\&amp;amp;1110111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Here are the conditionalized sets on the shortest prefixes:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{0} = \{&amp;amp;0000000000\dots,&lt;br&gt;
\\&amp;amp;0100000000\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{1}  = \{&amp;amp;1000000000\dots,&lt;br&gt;
\\&amp;amp;1001111111\dots,&lt;br&gt;
\\&amp;amp;1010101010\dots,&lt;br&gt;
\\&amp;amp;1101100110\dots,&lt;br&gt;
\\&amp;amp;1110111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{00} &amp;amp;= \set{0000000000\dots}\\&lt;br&gt;
\dom{\H}{01} &amp;amp;= \set{0100000000\dots}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{10}  = \{&amp;amp;1000000000\dots,&lt;br&gt;
\\&amp;amp;1001111111\dots,&lt;br&gt;
\\&amp;amp;1010101010\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{11}  = \{&amp;amp;1101100110\dots,&lt;br&gt;
\\&amp;amp;1110111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{100}  = \{&amp;amp;1000000000\dots,&lt;br&gt;
\\&amp;amp;1001111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\dom{\H}{101}  = \{1010101010\dots\}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\dom{\H}{110}  = \{1101100110\dots\}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\dom{\H}{111}  = \{&amp;amp;1110111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;There are more sequences in $\H$ starting with $1$ than with $0$, and so $\dom{H}{0}$ is a relatively smaller portion of $\H$ than $\dom{H}{1}$ is - specifically $\dom{H}{0}$ is 1/4 of $\H$ and $\dom{H}{1}$ is 3/4 of $\H$.&lt;/p&gt;
&lt;p&gt;If the agent&amp;rsquo;s goal is to make a prediction, it has incentive to narrow down as much of $\H$ as possible to reduce prediction uncertainty, prior to making the prediction. Thus, for this particular $\H$, the agent prefers to observe $0$ than $1$ as the first bit. In other words, smaller subsets $\H$ are better because they also mean $\H$ was narrowed down by a greater amount.&lt;/p&gt;
&lt;p&gt;It can be easier to think in terms of maximizing the amount of &amp;ldquo;narrowing-down&amp;rdquo;. We can formally quantify it by counting the number of &amp;ldquo;halvings&amp;rdquo; of $\H$ a given subset is worth.&lt;/p&gt;
&lt;p&gt;For subset $A \subseteq \H$ (still assuming finite $\H$), define the &lt;strong&gt;information gain&lt;/strong&gt; of $A$ (or &lt;strong&gt;surprise&lt;/strong&gt;) as:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
h_{\H}(A) \df -\lg\par{\frac{\abs{A}}{\abs{\H}}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\lg$ is log base 2. In general $h(A)$ is a real number, so if $n \leq h(A) &amp;lt; n+1$, then $\abs{A}$ is smaller than $1/2^{-n}$ the size of $\H$, and no smaller than  $1/2^{-n-1}$ of $\H$. In the context of narrowing down $\H$ with data $x$, the quantity $h(\dom{\H}{x})$ tells us how many halvings of $\H$ the data $x$ gave us. When the goal is to be as certain as possible about predictions of the future (where prediction uncertainty is represented by the set $\dom{\H}{x}$), the larger $h(\dom{\H}{x})$ is the better.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s calculate possible information gains for the example above:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h_\H(\dom{\H}{0}) &amp;amp;= -\lg\par{\frac{2}{8}} = 2 \\&lt;br&gt;
h_\H(\dom{\H}{1}) &amp;amp;= -\lg\par{\frac{6}{8}} \approx 0.415 \\&lt;br&gt;
h_\H(\dom{\H}{00}) &amp;amp;= -\lg\par{\frac{1}{8}} = 3 \\&lt;br&gt;
h_\H(\dom{\H}{10}) &amp;amp;= -\lg\par{\frac{3}{8}} \approx 1.415 \\&lt;br&gt;
&amp;amp;\vdots&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Suppose we previously observed $1$, for an information gain of about $0.415$, and reduced the remaining hypothesis set to $\dom{\H}{1}$. Then if we observe $0$, the hypothesis set is further reduced to $\dom{\H}{10}$. Going from $\H$ to $\dom{\H}{10}$ is worth a total information gain of about $1.415$, but what about the relative information gain going from $\dom{\H}{1}$ to $\dom{\H}{10}$? This quantity is called &lt;strong&gt;conditional information gain&lt;/strong&gt;, defined as&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
h_\H(A \mid B) &amp;amp;\df h_\H(A\cap B) - h_\H(B) \\&lt;br&gt;
&amp;amp;= -\lg\par{\frac{\abs{A \cap B}}{\abs{B}}} \\&lt;br&gt;
&amp;amp;= h_B(A\cap B)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;So $h_\H(\dom{\H}{10} \mid \dom{\H}{1}) = h_\H(\dom{\H}{10}) - h_\H(\dom{\H}{1}) \approx 1.415 - 0.415 = 1$. Conditional information gain is just the information gain starting from a different hypothesis set, so $h_\H(\dom{\H}{10} \mid \dom{\H}{1}) = h_{\dom{\H}{1}}(\dom{\H}{10}) = -\lg(3/6) = 1$, which is the number of halvings it takes to get from $\dom{\H}{1}$ to $\dom{\H}{10}$.&lt;/p&gt;
&lt;p&gt;The agent would like $h(\dom{\H}{x})$ to be maximized given a pre-defined $\H$, but the agent may not have any control over this quantity, unless the agent can take actions that affect what data $x$ it observes. However, if the agent gets to choose $\H$, then would the agent choose a very small set to begin with so that it does not need to be narrowed down very much?&lt;/p&gt;
&lt;p&gt;There is a problem. Take again as an example the rigid agent: $\H = \set{0000000000\dots}$. If only $0$s are ever observed, then this is a great hypothesis set, because the agent will be maximally certain about its prediction of future $0$s, and the agent will be right. But suppose the agent is wrong, e.g. the agent observes data $x = 001$. Then $\dom{\H}{001} = \set{}$ is the empty set. The agent can no longer make any prediction! If we quantify this narrowing-down, we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
h_\H(\dom{\H}{001}) = -\lg\par{\frac{0}{1}} = \infty\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ve maximized the amount of narrowing-down - it&amp;rsquo;s infinite. But at the same time this defeats the actual goal of being maximally certain about predictions. Having an empty hypothesis set is a degenerate state. Clearly, too much information gain is bad. Is there an ideal trade-off?&lt;/p&gt;
&lt;h2 id=&#34;compound-hypotheses&#34;&gt;Compound hypotheses&lt;/h2&gt;
&lt;p&gt;Consider the following hypothesis set:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\H = \{&amp;amp;0000000000\dots,&lt;br&gt;
\\&amp;amp;0100000000\dots,&lt;br&gt;
\\&amp;amp;0010000000\dots,&lt;br&gt;
\\&amp;amp;0110000000\dots,&lt;br&gt;
\\&amp;amp;1001111111\dots,&lt;br&gt;
\\&amp;amp;1101111111\dots,&lt;br&gt;
\\&amp;amp;1011111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The first symbol in these sequences fully determines the long-run behavior of the sequences, i.e. sequences starting with 0 end with 0s, and sequences starting with 1 end with 1s. However, the 2nd and 3rd symbols are not determined by the 1st. Perhaps it would make sense to not care about predicting them. In that case, we are not so interested in narrowing down $\H$ to one sequence, as we are in narrowing down $\H$ into a particular long-run pattern.&lt;/p&gt;
&lt;p&gt;$\newcommand{\h}{\mc{H}}$$\newcommand{\tr}{\rightarrowtail}$We can formally represent what we care about and don&amp;rsquo;t care about predicting, by partitioning $\H$. In this example, suppose we make the following partition:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mf{H} = \set{\h_1, \h_2} = \{&lt;br&gt;
\{&amp;amp;0000000000\dots,&lt;br&gt;
\\&amp;amp;0100000000\dots,&lt;br&gt;
\\&amp;amp;0010000000\dots,&lt;br&gt;
\\&amp;amp;0110000000\dots\},&lt;br&gt;
\\\{&amp;amp;1001111111\dots,&lt;br&gt;
\\&amp;amp;1101111111\dots,&lt;br&gt;
\\&amp;amp;1011111111\dots,&lt;br&gt;
\\&amp;amp;1111111111\dots\}\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\h_1$ contains only sequences ending in 0s, and $\h_2$ contains only sequences ending in 1s.&lt;/p&gt;
&lt;p&gt;In general, for a partition $\mf{H}$ of $\H$, call each $\h\in\mf{H}$ a &lt;strong&gt;compound hypothesis&lt;/strong&gt;, indicating that its a set of &lt;strong&gt;primitive hypotheses&lt;/strong&gt; (data continuations). As we shall see, compound hypotheses correspond closely to the hypotheses-as-data-distributions formulation which we saw in 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/&#34;&gt;Deconstructing Bayesian Inference&lt;/a&gt;
  


.&lt;/p&gt;
&lt;p&gt;For some partition $\mf{H}$ of $\H$, let $\dom{\mf{H}}{x} = \set{\dom{\h}{x} \mid \h \in \mf{H}}$ be the partition of $\dom{\H}{x}$ consisting of the parts in $\mf{H}$ which have each been conditionalized on $x$ (narrowed down to the sequences starting with $x$).&lt;/p&gt;
&lt;p&gt;In our example, $\dom{\mf{H}}{0} = \set{\h_1, \set{}}$ and $\dom{\mf{H}}{1} = \set{\set{}, \h_2}$. Let&amp;rsquo;s consider empty compound hypotheses to be eliminated. Then in either scenario, observing just the 1st symbol narrows down $\mf{H}$ to exactly one compound hypothesis, analogous to our original goal of narrowing down $\H$ to one hypothesis. The remaining compound hypothesis (either $\h_1$ or $\h_2$) is uncertain about what the 2nd and 3rd symbols will be, but certain about all symbols after that.&lt;/p&gt;
&lt;h2 id=&#34;information-gain&#34;&gt;Information Gain&lt;/h2&gt;
&lt;p&gt;Earlier, agent&amp;rsquo;s goal was to gain maximum prediction certainty by narrowing down $\H$ as much as possible. Now with partition $\mf{H}$, the agent&amp;rsquo;s goal is to narrow down $\mf{H}$ as much as possible, ideally reducing all of the parts but one to empty sets. However, if no compound hypothesis $\dom{\h}{x}\in\dom{\mf{H}}{x}$ is empty, is there still a sense in which the agent narrowed its compound hypotheses down? This question can be answered by considering information gain quantities.&lt;/p&gt;
&lt;p&gt;$\H \tr \dom{\H}{x}$ is the total information gained, and is quantified by $h_\H(\dom{\H}{x})$. This is the number of halvings the hypothesis set $\H$ is reduced by due to observing $x$. This quantity does not depend on the choice of partition $\mf{H}$.&lt;/p&gt;
&lt;p&gt;$\h \tr \dom{\h}{x}$ is the information gained within compound hypothesis $\h \in \mf{H}$, and is quantified by $h_\H(\dom{\h}{x} \mid \h) = h_\h(\dom{\h}{x})$. This is the information gained where $\h$ is treated as its own hypothesis set. This quantity only depends on the given $\h$, and not the other parts in $\mf{H}$. For each $\h\in\mf{H}$ there is an information gain $\h \tr \dom{\h}{x}$. Since we are considering $\h$ to be a set of sequences that the agent doesn&amp;rsquo;t care about distinguishing, reductions in $\h$ are essentially wasted information gain. If we regard variation of sequences within $\h$ to be noise, then this quantity measures information gained about that noise.&lt;/p&gt;
&lt;p&gt;Let $\o\in\H$ be the full data sequence, of which only a finite prefix $x \sqsubset \o$ has been observed. Call $\o$ the &lt;strong&gt;true hypothesis&lt;/strong&gt;. Likewise, for partition $\mf{H}$ there is exactly one compound hypothesis $\h\in\mf{H}$ containing $\o$. Call this the &lt;strong&gt;true compound hypothesis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Does observing $x$ tell the agent anything about which compound hypothesis $\h\in\mf{H}$ is true (contains $\o$)? Consider first the amount of information gain needed to reach certainty: $\H\tr\h$, i.e. reduce all but one compound hypotheses to the empty set (again, we don&amp;rsquo;t care about the size of the remaining compound hypothesis). This is quantified by $h_\H(\h)$. When $x$ is observed, $\H$ becomes $\dom{\H}{x}$ and $\h$ becomes $\dom{\h}{x}$. At that point, the information gain needed to achieve certainty that the same compound hypothesis true is $\dom{\H}{x}\tr\dom{\h}{x}$, quantified by $h_{\dom{\H}{x}}(\dom{\h}{x})$.&lt;/p&gt;
&lt;p&gt;We haven&amp;rsquo;t achieved compound hypothesis certainty, but the quantity of information gain needed to do so has changed:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\D_x\up{\h} = h_\H(\h) - h_{\dom{\H}{x}}(\dom{\h}{x})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s confirm that the sign is correct. If $h_{\dom{\H}{x}}(\dom{\h}{x}) &amp;gt; h_\H(\h)$ then we need more bits to achieve $\dom{\H}{x}\tr\dom{\h}{x}$ than to achieve $\H\tr\h$, i.e. the task of narrowing down to this compound hypothesis has gotten harder. In that case, $\D_x\up{\h}$ is negative. If, on the other hand, $h_\H(\h) &amp;gt; h_{\dom{\H}{x}}(\dom{\h}{x})$ then this compound hypothesis has become a larger portion of the remaining $\dom{\H}{x}$ than it was before observing $x$, so the task of narrowing down to this compound hypothesis has gotten easier. In that case, $\D_x\up{\h}$ is positive.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210409163721.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
Note that $\D_x\up{\h}$ is upper bounded. If the agent has succeeded in ruling out all other compound hypotheses so that $\dom{\h}{x} = \dom{\H}{x}$, then $h_{\dom{\H}{x}}(\dom{\h}{x}) = 0$ and $\D_x\up{\h} = h_\H(\h)$, which is the maximum amount of information that can be gained about whether $\h$ is true.&lt;/p&gt;
&lt;p&gt;If $\h$ is known to be false, i.e. $\dom{\h}{x} = \set{}$, then $h_{\dom{\H}{x}}(\dom{\h}{x}) = \infty$ and so $\D_x\up{\h} = -\infty$. Thus $\D_x\up{\h}$ is not lower bounded, i.e. there is a maximum amount of information to be gained about whether a compound hypothesis is true, but an infinite amount of information to lose. For example, if $\h$ is true but $x$ is very misleading, then $\D_x\up{\h}$ will be very negative. In the long run, if the agent observes enough data, $\o_{1:n}$ for large $n$, then $\D_{\o_{1:n}}\up{\h}$ will go up and eventually converge to $h_\H(\h)$. The misleading initial data caused the agent to lose information, in the sense that even more information needs to be gained to achieve the same certainty that $\h$ is true.&lt;/p&gt;
&lt;p&gt;Also note that $\D_x\up{\h}$ is a total change given the entire data sequence $x$. We can also quantify the change due to a single timestep $x_n$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\D_{x_n}\up{\dom{\h}{x_{&amp;lt;n}}} = h_{\dom{\H}{x_{&amp;lt;n}}}(\dom{\h}{x_{&amp;lt;n}}) - h_{\dom{\H}{x_{1:n}}}(\dom{\h}{x_{1:n}})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then the total change is the sum of changes for each timestep:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\D_x\up{\h} = \sum_{i=1}^\abs{x} \D_{x_i}\up{\dom{\h}{x_{&amp;lt;i}}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;It may even be of some interest to plot incremental information gains over time for each compound hypothesis $\h$. What we will observe is that in the long run is that $\D_x\up{\h}$ asymptotically converges to $h_\H(\h)$ for the true compound hypothesis $\h$, and the incremental change $\D_{x_n}\up{\dom{\h}{x_{&amp;lt;n}}}$ gets smaller and smaller over time for that same $\h$. Meanwhile, for all other compound hypotheses $\h&#39;$ (the false ones), $\D_x\up{\h&#39;}$ diverges to $-\infty$ and the incremental change $\D_{x_n}\up{\dom{\h&#39;}{x_{&amp;lt;n}}}$ goes negative and grows larger and larger in magnitude. For &amp;ldquo;misleading&amp;rdquo; data, the short-run behavior of these plots may be oscillatory before long-run behavior takes over.&lt;/p&gt;
&lt;h3 id=&#34;useful-identities&#34;&gt;Useful Identities&lt;/h3&gt;
&lt;p&gt;Doing some algebraic manipulation, we get:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\D_x\up{\h} &amp;amp;= h_\H(\h) - h_{\dom{\H}{x}}(\dom{\h}{x}) \\&lt;br&gt;
&amp;amp;= -\lg\par{\frac{\abs{\h}}{\abs{\H}}} + \lg\par{\frac{\abs{\dom{\h}{x}}}{\abs{\dom{\H}{x}}}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\abs{\dom{\h}{x}}/\abs{\dom{\H}{x}}}{\abs{\h}/\abs{\H}}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{f_x\up{\h}}{f\up{\h}}}\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $f\up{\h} = \abs{\h}/\abs{\H}$ is the fraction of predictions that $\h$ takes up, and $f_x\up{\h} = \abs{\dom{\h}{x}}/\abs{\dom{\H}{x}}$ is that fraction after $x$ is observed. This gives us an interpretation for the quantity $\D_x\up{\h}$: the number of doublings it takes to go from $f\up{\h}$ to $f_x\up{\h}$. E.g. if $\h$ is a quarter the size of $\H$ and $\dom{\h}{x}$ is half the size of $\dom{\H}{x}$, then $\D_x\up{\h} = \lg\frac{1/2}{1/4} = \lg 2 = 1$. That means the agent has one less bit to gain about whether $\dom{\h}{x}$ is true, and has in that sense gained one bit of information.&lt;/p&gt;
&lt;p&gt;Doing even more algebra, we get another useful identity:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\D_x\up{\h} &amp;amp;= \lg\par{\frac{\abs{\dom{\h}{x}}/\abs{\dom{\H}{x}}}{\abs{\h}/\abs{\H}}} \\&lt;br&gt;
&amp;amp;= -\lg\par{\frac{\abs{\dom{\H}{x}}}{\abs{\H}}} + \lg\par{\frac{\abs{\dom{\h}{x}}}{\abs{\h}}} \\&lt;br&gt;
&amp;amp;= h_\H(\dom{\H}{x}) - h_\h(\dom{\h}{x})&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Thus, $h_\H(\dom{\H}{x}) = h_\h(\dom{\h}{x}) + \D_x\up{\h}$, for all $\h\in\mf{H}$. That is to say, the total information gain $h_\H(\dom{\H}{x})$ can be decomposed as the sum of information gained within a given compound hypothesis $\h$ (information gained about noise, i.e. what we don&amp;rsquo;t care about predicting), plus the information gained about whether $\h$ is true. Total info gain $h_\H(\dom{\H}{x})$ decomposes similarly for every $\h\in\mf{H}$.&lt;/p&gt;
&lt;h2 id=&#34;other-information-quantities&#34;&gt;Other Information Quantities&lt;/h2&gt;
&lt;p&gt;See my &lt;a href=&#34;http://zhat.io/articles/primer-shannon-information&#34;target=&#34;_blank&#34;&gt;primer to Shannon&amp;rsquo;s information theory&lt;/a&gt; for more intuition about the interpretation of these quantities, and specifically the sections on&lt;br&gt;
&lt;a href=&#34;http://zhat.io/articles/primer-shannon-information#mutual-information&#34;target=&#34;_blank&#34;&gt;mutual information&lt;/a&gt; and &lt;a href=&#34;http://zhat.io/articles/primer-shannon-information#entropy&#34;target=&#34;_blank&#34;&gt;entropy&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For sets $A,B\subseteq\H$, the quantity $i_\H(A, B)$ is called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Pointwise_mutual_information&#34;target=&#34;_blank&#34;&gt;&lt;strong&gt;pointwise mutual information&lt;/strong&gt;&lt;/a&gt; (PMI) between $A$ and $B$, defined by&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
i_\H(A,B) &amp;amp;\df h_\H(A) - h_\O(A \mid B) \\&lt;br&gt;
&amp;amp;= -\lg\par{\frac{\abs{A}}{\abs{\H}}} + \lg\par{\frac{\abs{A\cap B}}{\abs{B}}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\abs{A\cap B}}{\abs{A}\abs{B}}\abs{\H}} \\&lt;br&gt;
&amp;amp;= \lg\par{\frac{\abs{A\cap B}}{\abs{A}\abs{B}}} - \lg\par{\frac{1}{\abs{\H}}}\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Note that PMI is symmetric, so $i_\H(A, B) = i_\H(B, A)$.&lt;/p&gt;
&lt;p&gt;Notice that $\D_x\up{\h} = i_\H(\h, \dom{\H}{x})$. Using our intuition about narrowing down compound hypotheses from before, we can interpret the meaning of $i_\H(A, B)$ in general.&lt;/p&gt;
&lt;p&gt;Let&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\frac{\H \tr \H&#39;}{U \tr U&#39;}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;denote the statement &amp;ldquo;$\H$ is narrowed down to $\H&#39;$ while $U$ is narrowed down to $U&#39;$&amp;rdquo;, with the assumption that $U \subseteq \H$ and $U&#39;\subseteq\H&#39;$. So the idea of gaining information about whether compound hypothesis $\h$ is true can be written succinctly as $\frac{\H \tr \dom{\H}{x}}{\h \tr \dom{\h}{x}}$.&lt;/p&gt;
&lt;p&gt;In general, $i_\H(A, B)$ quantifies $\frac{\H \tr B}{A \tr (A \cap B)}$, and is the number of doublings achieved from fraction $f = \abs{A}/\abs{\H}$ to fraction $f&#39;=\abs{A\cap B}/\abs{B}$, i.e. $\lg(f&#39;/f)$.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210409163741.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210409163757.png&#34; alt=&#34;&#34;&gt;
&lt;/p&gt;
&lt;h3 id=&#34;entropy&#34;&gt;Entropy&lt;/h3&gt;
&lt;p&gt;It is useful to have a single quantity representing the state of $\dom{\mf{H}}{x}$.&lt;/p&gt;
&lt;p&gt;Let $\mf{A}$ be some partition of $\H$. Define &lt;strong&gt;entropy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mb{H}(\mf{A}) \df \sum_{A\in\mf{A}} \frac{\abs{A}}{\abs{\H}} h_\H(A)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This quantity doesn&amp;rsquo;t require $\O$ to be explicitly specified because it determined by the argument, i.e. $\O = \bigcup \mf{A}$.&lt;/p&gt;
&lt;p&gt;Let $\mf{H}$ be a compound hypothesis set. Then $\mb{H}(\mf{H})$ quantifies roughly how much of a difference there is in information that can be gained about whether each compound hypothesis $\h\in\mf{H}$ is true. Specifically, it is the expected information gain across $\mf{H}$, though expectations don&amp;rsquo;t have the same meaning here because these &amp;ldquo;probabilities&amp;rdquo; don&amp;rsquo;t denote randomness. $\mb{H}(\mf{H})$ is maximized if $h_\H(\h) = h_\H(\h&#39;)$ for all $\h,\h&#39;\in\mf{H}$, and $\mb{H}(\mf{H})$ is 0 if one $\h\in\mf{H}$ is non-empty while all other compound hypotheses are empty. If we observe $x$, then high $\mb{H}(\dom{\mf{H}}{x})$ indicates high uncertainty about which compound hypothesis is true, and small $\mb{H}(\dom{\mf{H}}{x})$ indicates high certainty about which compound hypothesis is true.&lt;/p&gt;
&lt;h3 id=&#34;mutual-information&#34;&gt;Mutual Information&lt;/h3&gt;
&lt;p&gt;Let $\mf{A}$ and $\mf{B}$ be partitions of $\H$. Define &lt;strong&gt;mutual information&lt;/strong&gt; (MI)&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mb{I}(\mf{A}, \mf{B}) \df \sum_{A\in\mf{A}}\sum_{B\in\mf{B}} \frac{\abs{A\cap B}}{\abs{\H}} i_\H(A, B)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is the expected pointwise mutual information between the two partitions, which quantifies roughly how much redundancy there is between them.&lt;/p&gt;
&lt;p&gt;MI plays the following role in Bayesian information gain: Let $\mf{X}_n = \set{\dom{\H}{x} \mid x\in\X^n}$ be the partition of $\H$ consisting of all possible conditionalizations $\dom{\H}{x}$ for each possible length-$n$ data sequence $x\in\X^n$. Then $\mb{I}(\mf{H}, \mf{X}_n)$ quantifies how much information could be gained (in expectation) about which compound hypothesis is true given a (not yet observed) length-$n$ data sequence. If MI is minimized, $\mb{I}(\mf{H}, \mf{X}_n)=0$, then we expect that $\D_x\up{\h} = i_\H(\h, \dom{\H}{x})=0$ for $\h\in\mf{H}$ and $\dom{\H}{x}\in\mf{X}_n$. This would indicate that the partitions $\mf{H}$ and $\mf{X}_n$ are orthogonal, in a sense. On the other hand, MI is maximized when $\mb{I}(\mf{H}, \mf{X}_n)=\min\set{\mb{H}(\mf{H}), \mb{H}(\mf{X}_n)}$, and indicates that each $x\in\X^n$ will narrow down some $\h\in\mf{H}$ to empty sets (with either one remaining compound hypothesis which is narrowed down, or multiple remaining compound hypotheses which are not narrowed down at all), i.e. the partitions are parallel, in a sense.&lt;/p&gt;
&lt;h1 id=&#34;infinite-possibilities&#34;&gt;Infinite possibilities&lt;/h1&gt;
&lt;p&gt;In practice agents would want to have a large enough hypothesis set to be able to make predictions in all circumstances. That is to say, given any finite observation $x\in\X^*$, it is desirable for at least one sequence $\o\in\H$ to begin with $x$, denoted $x \sqsubset \o$. Then, $\dom{\H}{x}$ is non-empty for all $x$ and so the agent always has at least one prediction to make.&lt;/p&gt;
&lt;p&gt;Clearly such an $\H$ cannot be finite because $\X^*$ is infinite, i.e. there is at least one $\o\in\H$ for every $x\in\X^*$ (Simple proof: Suppose $\H$ were finite. Construct finite $x$ s.t. $x\not\sqsubset\o$ for all $\o\in\H$).  (Note that such an $\H$ need not be equal to $\X^\infty$. For example, $\H = \set{x`00000\dots \mid x\in\X^*}$ where $x`00000\dots$ is $x$ appended with infinite $0$s. This $\H$ does not include sequences with other limiting behavior, e.g. the binary digits of Pi.)&lt;/p&gt;
&lt;p&gt;However, $\H$ can be too big. Suppose $\H=\X^\infty$. Then $\dom{\H}{x} = \X^\infty$ for all $x\in\X^*$. This hypothesis set can never be narrowed down to anything, i.e. there is never information gain. An agent with this hypothesis set remains maximally uncertain always, and so it is not useful.&lt;/p&gt;
&lt;p&gt;Even if $\H$ is a strict subset of $\X^\infty$ but contains every finite data sequence (so that $\dom{\H}{x}$ is non-empty for all $x$), it&amp;rsquo;s usefulness is still dubious. In general $\abs{\dom{\H}{x}}=\infty$ for all $x$ (the cardinality of $\dom{\H}{x}$ is always infinite; disregarding different sizes of infinity), so we cannot compare to what extent we&amp;rsquo;ve narrowed down $\H$ further by observing $x$ rather than $y$. That is to say, $\abs{\dom{\H}{x}}/\abs{\H} = \infty/\infty$ is indeterminate. Furthermore, $\dom{\H}{x}$ will contain every finite data sequence starting with $x$, so there is no tangible sense in which the agent&amp;rsquo;s predictions at finite time are narrowed down.&lt;/p&gt;
&lt;p&gt;The problem of measuring narrowing-down of infinite possibility sets is resolved by choosing a measure $\mu$ on $\H$. However a new problem arises: how to choose $\mu$. My purpose in presenting Bayesian information theory as narrowing down possibility spaces is to give meaning to probability values. Now, we&amp;rsquo;ve reintroduced an arbitrary measure $\mu$.&lt;/p&gt;
&lt;p&gt;There is a sort of middle ground that also serves as a bridge between the usual probabilistic conception of Bayesian inference I introduced in 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/&#34;&gt;Deconstructing Bayesian Inference&lt;/a&gt;
  


 and the hypothesis set conception, via algorithmic information theory.&lt;/p&gt;
&lt;h2 id=&#34;algorithmic-randomness&#34;&gt;Algorithmic Randomness&lt;/h2&gt;
&lt;p&gt;In 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/&#34;&gt;Deconstructing Bayesian Inference&lt;/a&gt;
  


, I defined Bayesian inference as a manipulation of probability measures on $\X^\infty$. Given a set $\M$ of measures $\mu$ on $\X^\infty$, and a prior $p$ on $\M$, what is the corresponding &amp;ldquo;possibility space narrowing-down&amp;rdquo; perspective?&lt;/p&gt;
&lt;p&gt;I supposed that hypothesis probabilities $\mu(\o_{1:n})$ for $\mu\in\M$ represented irreducible uncertainty about the world, whereas subjective probabilities $p(\o_{1:n})$ represented a mixture of reducible and irreducible uncertainty, where knowing which hypothesis $\mu$ is true maximally reduces your uncertainty.&lt;/p&gt;
&lt;p&gt;I also introduced a distinction between randomness and non-determinism. Randomness can be defined as incompressibility, and non-determinism refers to the output of a mathematical construct (such as a function) being not uniquely determined by givens (such as an input). Probabilities can measure both, which can lead to confusion.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s suppose here that hypothesis probabilities always measure randomness. Assuming $\mu\in\M$ is computable, we can precisely define what it means for an infinite sequence $\o\in\X^\infty$ to &amp;ldquo;look like&amp;rdquo; a typical sequence randomly drawn from $\mu$. Call such &amp;ldquo;typical looking&amp;rdquo; sequences &lt;strong&gt;$\mu$-typical&lt;/strong&gt; (or &lt;strong&gt;$\mu$-random&lt;/strong&gt;). Formally, $\o\in\X^\infty$ is $\mu$-typical iff the optimal compression rate of $\o$ (via monotone algorithmic complexity $Km$) is achieved with arithmetic coding using $\mu$. (See &lt;a href=&#34;https://www.springer.com/gp/book/9781489984456&#34;target=&#34;_blank&#34;&gt;Li &amp;amp; VitÃ¡nyi&lt;/a&gt;, 3rd edition, theorem 4.5.3 on page 318, which states, $\sup_n\left\{\lg(1/\mu(\o_{1:n})) - Km(\o_{1:n})\right\} &amp;lt; \infty$.)&lt;/p&gt;
&lt;p&gt;Let $\h\up{\mu}\subseteq\X^\infty$ be the set of all infinite sequences which are $\mu$-typical, called the &lt;strong&gt;$\mu$-typical set&lt;/strong&gt;. The $\mu$-probability of drawing a $\mu$-typical sequence is 1, i.e. $\mu(\h\up{\mu})=1$, and the $\mu$-probability of drawing a $\mu$-atypical sequence is 0. You can think of $\h\up{\mu}$ as $\X^\infty$ with a $\mu$-measure 0 subset subtracted from it (specifically $\h\up{\mu}$ is the smallest constructable $\mu$-measure 1 subset of $\X^\infty$).&lt;/p&gt;
&lt;p&gt;The prior $p$ on $\M$ induces a subjective data distribution: $p(\o_{1:n}) = \sum_{\mu\in\M} p(\mu)\mu(\o_{1:n})$. So long as $p$ is computable, there is a $p$-typical set $\h\up{p}$.&lt;/p&gt;
&lt;p&gt;Let $\H = \h\up{p}$ and $\mf{H} = \set{\h\up{\mu} \mid \mu\in\M}$, where $\H$ is an agent&amp;rsquo;s hypothesis set (set of sequence predictions) and $\mf{H}$ is a set of compound hypotheses, where $\bigcup \mf{H} = \H$. I&amp;rsquo;m dropping the requirement that $\mf{H}$ be a proper partition of $\O$ (i.e. all $\h\in\mf{H}$ are mutually disjoint), in which case we call $\mf{H}$ a cover of $\O$.&lt;/p&gt;
&lt;p&gt;Now, Bayesian inference with hypothesis set (of measures) $\M$ and prior $p$, and Shannon quantities using these measures, corresponds to Bayesian inference with $\H$ and $\mf{H}$ as I outlined above. The restriction $\dom{\H}{x}$ is the typical set for the conditional measure $p(\cdot \mid x)$, and likewise for $\h\up{\mu}\in\mf{H}$, the restriction $\dom{\h\up{\mu}}{x}$ is the typical set for the conditional measure $\mu(\cdot \mid x)$.&lt;/p&gt;
&lt;p&gt;The prior $p(\mu)$ is simply the relative size of $\h\up{\mu}$ within $\H$, given by $p(\h\up{\mu})$. The prior encodes how much information we would gain if all other hypotheses were ruled out: $\H\tr\h\up{\mu}$, quantified by $h_p(\h\up{\mu}) = -\lg p(\h\up{\mu})$.&lt;/p&gt;
&lt;p&gt;The posterior $p(\mu\mid x)$ is simply the relative size of $\dom{\h\up{\mu}}{x}$ within $\dom{\H}{x}$, given by $p(\dom{\h\up{\mu}}{x}) = p(\h\up{\mu} \cap \dom{\H}{x})$. The posterior encodes how much information we would gain if all other hypotheses were ruled out (after observing $x$): $\dom{\H}{x}\tr\dom{\h\up{\mu}}{x}$, quantified by $h_p(\h\up{\mu} \mid \dom{\H}{x}) = -\lg p(\dom{\h\up{\mu}}{x})/p(\dom{\H}{x})$.&lt;/p&gt;
&lt;p&gt;Furthermore, $\H\tr\dom{\H}{x}$ is quantified by $h_p(\dom{\H}{x}) = -\lg p(\dom{\H}{x}) = -\lg p(x)$, and $\h\up{\mu}\tr\dom{\h\up{\mu}}{x}$ is quantified by $h_\mu(\dom{\h\up{\mu}}{x}) = -\lg \mu(\dom{\h\up{\mu}}{x}) = -\lg \mu(x)$.&lt;/p&gt;
&lt;p&gt;The mysterious &amp;ldquo;information gained about whether hypothesis $\mu$ is true&amp;rdquo; becomes $\frac{\H \tr \dom{\H}{x}}{\h\up{\mu} \tr \dom{\h\up{\mu}}{x}}$, quantified by $i_p(\h\up{\mu}, \dom{\H}{x}) = \lg\frac{p(\dom{\h\up{\mu}}{x})}{p(\h\up{\mu})p(\dom{\H}{x})} = \lg\frac{p(\mu,x)}{p(\mu)p(x)}$ which is the pointwise mutual information between hypothesis $\mu$ and data $x$.&lt;/p&gt;
&lt;p&gt;Finally, the information gained about which hypothesis is true is summarized by the quantity&lt;/p&gt;
&lt;p&gt;$$\mb{I}_p(\mf{H}, \mf{X}_\abs{x}) = \sum_{\h\up{\mu}\in\mf{H}}\sum_{\dom{\H}{x}} \mu(\dom{\h}{x}) i_p(\h\up{\mu}, \dom{\H}{x}) = \mb{E}_p\left[i(H,X_{1:\abs{x}})\right] = I(H, X_{1:\abs{x}})\,,$$&lt;/p&gt;
&lt;p&gt;where $H$ is the random variable corresponding to choice of hypothesis $\mu$ sampled from $p(\mu)$ and $X_{1:\abs{x}}$ is the random variable corresponding to choice of length-$\abs{x}$ data sampled from $p(x)$. This quantity is sometimes called &lt;strong&gt;Bayesian surprise&lt;/strong&gt; (see &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2860069/&#34;target=&#34;_blank&#34;&gt;ref 1&lt;/a&gt; and &lt;a href=&#34;https://www.fil.ion.ucl.ac.uk/~karl/The%20free-energy%20principle%20A%20unified%20brain%20theory.pdf&#34;target=&#34;_blank&#34;&gt;ref 2&lt;/a&gt;), though more commonly &amp;ldquo;surprise&amp;rdquo; refers to the total info gain $h_\mu(\dom{\H}{x})$ (&lt;a href=&#34;https://en.wikipedia.org/wiki/Information_content&#34;target=&#34;_blank&#34;&gt;ref 1&lt;/a&gt;, &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;ref 2&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;posterior-consistency&#34;&gt;Posterior consistency&lt;/h3&gt;
&lt;p&gt;When $\mf{H}$ is a strict partition, this is a special (and desirable) case in Bayesian inference, called &lt;strong&gt;posterior consistency&lt;/strong&gt;. Informally, posterior consistency is when posterior probabilities converge to one-hot (or Dirac delta) distributions (almost surely). Posterior consistency is a property of a particular set of hypotheses $\M$, and is invariant to prior probabilities (assuming suppose on $\M$).&lt;/p&gt;
&lt;p&gt;If $\M$ has the posterior consistency property, then the corresponding $\mf{H}$ will be &lt;em&gt;nearly&lt;/em&gt; a partition, where $\mu(\h\up{\mu}\cap\h\up{\nu}))=\nu(\h\up{\mu}\cap\h\up{\nu}))=0$, for $\mu,\nu\in\M$.&lt;/p&gt;
&lt;p&gt;Posterior inconsistency in $\M$ corresponds to $\mf{H}$ which has overlapping compound hypotheses (of non-zero measure).&lt;/p&gt;
&lt;p&gt;Why do we want $\mf{H}$ to be a partition? So then information gained about one hypothesis $\mu$, corresponding to compound hypothesis $\h\up{\mu}$, necessarily implies information loss about all other hypotheses. If compound hypotheses overlap, then you can become more certain about multiple hypotheses at the same time, and in the infinite data limit, many hypotheses may remain (and we have not achieved our goal of prediction certainty by narrowing down to one hypothesis).&lt;/p&gt;
&lt;h3 id=&#34;posterior-convergence&#34;&gt;Posterior convergence&lt;/h3&gt;
&lt;p&gt;For any typical set $\h\up{\mu}$, there are actually infinitely many computable measures with the same typical set. Why is that? For any $\mu$, a new measure $\mu&#39;$ can be constructed that assigns different probabilities than $\mu$ to finite sequences, e.g. $\mu&#39;(\o_{1:n}) \neq \mu(\o_{1:n})$, while preserving the limiting compression rates:&lt;/p&gt;
&lt;p&gt;$$\lim_{n\to\infty} \frac{1}{n}\lg\par{\frac{1}{\mu(\o_{1:n})}} = \lim_{n\to\infty} \frac{1}{n}\lg\par{\frac{1}{\mu&#39;(\o_{1:n})}}$$&lt;/p&gt;
&lt;p&gt;A difference in probabilities of finite sequence $\o_{1:n}$ corresponds to a constant offset in compression length (according to Shannon):&lt;/p&gt;
&lt;p&gt;$$\abs{\lg\par{\frac{1}{\mu(\o_{1:n})}} - \lg\par{\frac{1}{\mu&#39;(\o_{1:n})}}} = C &amp;lt; \infty$$&lt;/p&gt;
&lt;p&gt;Any finite difference becomes negligible as $n\to\infty$. $\mu$ is said to have &lt;strong&gt;posterior convergence&lt;/strong&gt; to $\mu&#39;$.&lt;/p&gt;
&lt;p&gt;This is a strange predicament, because it implies that there are infinitely many probability measures that essentially encode the same randomness. That is to say, the measurement of randomness of finite sequences is not uniquely determined. This actually falls in line with the results of algorithmic information theory, where optimal compression length (Kolmogorov complexity) depends on choice of programming language (universal Turing machine), and is also arbitrary for that reason.&lt;/p&gt;
&lt;p&gt;It is the infinite sequences which have a unique quantity of randomness, in a sense. Non-computable infinite sequences have a compressed length that is also infinite, but often a finite compression rate: $\limsup_{n\to\infty}\frac{Km(\o_{1:n})}{n}$. Moreover, $\o$ has a unique limiting data posterior, i.e.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lim_{n\to\infty} \mu(\o_n \mid \o_{&amp;lt;n}) - \mu&#39;(\o_n \mid \o_{&amp;lt;n}) = 0\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;(almost surely), for any two measures $\mu$ and $\mu&#39;$ which have the same typical set $\h\up{\mu}$. Solomonoff&amp;rsquo;s universal data distribution $\xi$ (the mixture of all semicomputable semimeasures, see 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/#solomonoff-induction&#34;&gt;Deconstructing Bayesian Inference#solomonoff-induction&lt;/a&gt;
  


) has posterior convergence to all such $\mu$. In fact, Solomonoff&amp;rsquo;s mixture can be used as a test for whether $\o$ is $\mu$-typical by observing if $\lim_{n\to\infty} \xi(\o_n \mid \o_{&amp;lt;n}) - \mu(\o_n \mid \o_{&amp;lt;n}) = 0$ (almost surely).&lt;/p&gt;
&lt;p&gt;The takeaway here is that if compound hypothesis $\h$ is the typical set for some measure $\mu$, then if we don&amp;rsquo;t specify any particular measure, the limiting information gain $h_\H(\dom{\h}{x&amp;rsquo;y} \mid \dom{\h}{x})$ as $\abs{x}\to\infty$ converges to something unique. So if we have infinite hypothesis sets and don&amp;rsquo;t want to arbitrarily choose a measure, not all hope is lost.&lt;/p&gt;
&lt;h1 id=&#34;shannon-equivalence&#34;&gt;Shannon Equivalence&lt;/h1&gt;
&lt;p&gt;I defined the quantities of information above using the set cardinality function $\abs{\cdot}$ to measure the sizes of sets (called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Counting_measure&#34;target=&#34;_blank&#34;&gt;counting measure&lt;/a&gt;). In general, the size of a set can be defined with a &lt;strong&gt;measure&lt;/strong&gt;, which is a function from subsets to non-negative real numbers. So a measure $\mu$ on $\H$ has the type signature $\mu : 2^\H \to \mb{R}_{\geq 0}$ (though technically we need to restrict ourselves to &lt;em&gt;measurable&lt;/em&gt; subsets of $\H$, see my &lt;a href=&#34;http://zhat.io/articles/primer-probability-theory#primer-to-measure-theory&#34;target=&#34;_blank&#34;&gt;primer to measure theory&lt;/a&gt; for details). Furthermore, if we choose measure $\mu$ s.t. $\mu(\H) = 1$, then $\mu$ is called a &lt;strong&gt;probability measure&lt;/strong&gt; (or a &lt;strong&gt;normalized measure&lt;/strong&gt;). See my &lt;a href=&#34;http://zhat.io/articles/primer-probability-theory#definitions&#34;target=&#34;_blank&#34;&gt;primer to probability theory&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;If we replace $\abs{\cdot}$ with probability measure $\mu(\cdot)$ everywhere in the quantities of information defined above, then we get the usual Shannon definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$h_\mu(A) = -\lg \mu(A)$&lt;/li&gt;
&lt;li&gt;$h_\mu(A \mid B) = -\lg \par{\frac{\mu(A \cap B)}{\mu(B)}}$&lt;/li&gt;
&lt;li&gt;$i_\mu(A, B) = \lg\par{\frac{\mu(A\cap B)}{\mu(A)\mu(B)}}$&lt;/li&gt;
&lt;li&gt;$\mb{H}_\mu(\mf{A}) = \sum_{A\in\mf{A}} \mu(A) h_\mu(A)$&lt;/li&gt;
&lt;li&gt;$\mb{H}_\mu(\mf{A}\mid B) = \sum_{A\in\mf{A}} \mu(A\mid B) h_\mu(A\mid B)$&lt;/li&gt;
&lt;li&gt;$\mb{H}_\mu(\mf{A}\mid\mf{B}) = -\sum_{A\in\mf{A}}\sum_{B\in\mf{B}} \mu(A\cap B) h_\mu(A\mid B)$&lt;/li&gt;
&lt;li&gt;$\mb{I}_\mu(\mf{A}, \mf{B}) = \sum_{A\in\mf{A}}\sum_{B\in\mf{B}} \mu(A\cap B) i_\mu(A, B)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $A,B\subseteq\H$ and $\mf{A},\mf{B}$ are two partitions of $\H$. Note that $\abs{\H}$ disappears because it becomes $\mu(\H) = 1$.&lt;/p&gt;
&lt;h2 id=&#34;optimal-compression&#34;&gt;Optimal Compression&lt;/h2&gt;
&lt;p&gt;Now we see that Bayesian information theory is mathematically equivalent to Shannon&amp;rsquo;s information theory, where a probability measure $\mu$ is used to measure the sizes of hypothesis sets (sets of predictions).&lt;/p&gt;
&lt;p&gt;However, what is the connection between narrowing down hypothesis sets and optimal compression? Given probability measure $\mu$ on $\H$, the $\mu$-probability of finite observation $x\in\X^*$ is $\mu(\dom{\H}{x})$. (We can abuse notation and write $\mu(x)$ where $x$ is shorthand for $\dom{\H}{x}$ when given as the argument to $\mu$.) Then is $h_\mu(\dom{\H}{x})$ the optimal compressed length of $x$?&lt;/p&gt;
&lt;p&gt;For finite strings, optimal compression isn&amp;rsquo;t a well defined notion. According to algorithmic information theory, we use the shortest program that outputs $x$ as the compressed representation of $x$, but the length of that shortest program depends on our arbitrary choice of programming language. We can achieve an encoded length of approximately $h_\mu(\dom{\H}{x})$ by using &lt;a href=&#34;https://en.wikipedia.org/wiki/Arithmetic_coding&#34;target=&#34;_blank&#34;&gt;arithmetic coding&lt;/a&gt;, with $\mu$ as the provided measure (ignoring the length of the arithmetic decoder program itself).&lt;/p&gt;
&lt;p&gt;Shannon&amp;rsquo;s information theory operates in the domain of random data, and provides optimal code lengths &lt;em&gt;in expectation&lt;/em&gt;. In the Bayesian information theory I&amp;rsquo;ve outlined above, we are not working with randomness, but non-determinism (the agent&amp;rsquo;s predictions are not uniquely determined). However, as the data length goes to infinity, these two conceptions of probability become intertwined.&lt;/p&gt;
&lt;p&gt;Let $\o\in\X^\infty$ be an infinite sequence, and an agent observed the finite prefix $\o_{1:n}$. If the agent has a hypothesis set $\H$ with probability measure $\mu$, the agent can use arithmetic coding w.r.t. $\mu$ to achieve compressed length $-\lg \mu(\o_{1:n})$, and limiting compression rate&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\limsup_{n\to\infty} \frac{1}{n}\lg\par{\frac{1}{\mu(\o_{1:n})}}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If $\o$ is $\mu$-typical, than this will be the optimal compression rate achievable (according to algorithmic information theory). However, if $\o$ is not $\mu$-typical then the agent&amp;rsquo;s compression rate will be worse than the optimum.&lt;/p&gt;
&lt;p&gt;Let $\X = \set{0,1}$. For each bit of data $\o_n$, the approximate compressed length of that bit is $-\lg\mu(\o_n \mid \o_{&amp;lt;n})$. So if the agent gains more than 1 bit of information from $\o_n$, arithmetic coding w.r.t. $\mu$ will actually assign more than one bit to $\o_n$ in the compressed representation. If the agent&amp;rsquo;s info gain remains high in the long run, this &amp;ldquo;compression&amp;rdquo; of $\o$ will end up being longer than $\o$ itself (specifically, the compression of $\o_{1:n}$ will be longer than $n$ as $n\to\infty$). This gives us a precise sense about whether the agent is doing a good job at predicting the part of $\o$ that can be predicted: If the agent&amp;rsquo;s $\mu$-compression rate is better than the length of the data itself then the agent is predicting the data at least better than random.&lt;/p&gt;
&lt;p&gt;A mixture distribution can be viewed as a hedge against bad compression. Suppose $\o$ is not $\mu$-typical. If instead of using $\mu$ to predict $\o$, we had a set of distributions $\M$ of which $\mu$ is a member, and we use the mixture $p = \sum_{\nu\in\M} w_\nu \nu$ to predict $\o$. If $\o$ is typical w.r.t. at least one $\nu\in\M$, then $\o$ is also $p$-typical. The difference between using $\nu$ and $p$ to compress $\o$ is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\limsup_{n\to\infty} \lg\par{\frac{1}{p(\o_{1:n})}} - \lg\par{\frac{1}{\nu(\o_{1:n})}} \,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is a constant cost in compression length, and becomes negligible in the long run as $n\to\infty$, i.e. the compression rate using $p$ and $\nu$ is the same. So using a mixture to compress $\o$ may incur additional cost (extra bits) initially, in the long run it is no worse than using the &amp;ldquo;true&amp;rdquo; hypothesis $\nu$ (there may be more than one &amp;ldquo;true&amp;rdquo; hypothesis in $\M$). A good strategy is then to make $\M$ as large as possible. This is the premise behind Solomonoff induction, where $\M$ is the set of all semicomputable semimeasures.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deconstructing Bayesian Inference</title>
      <link>https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\z}{\zeta}&lt;br&gt;
\newcommand{\l}{\lambda}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\H}{\mc{H}}&lt;br&gt;
\newcommand{\P}{\mc{P}}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}&lt;br&gt;
$$&lt;/p&gt;
&lt;h1 id=&#34;constructing-bayesian-inference&#34;&gt;Constructing Bayesian Inference&lt;/h1&gt;
&lt;p&gt;Before deconstructing Bayesian inference, I will present the general definition. At any point, feel free to look at the &lt;a href=&#34;#use-cases&#34;&gt;#Use Cases&lt;/a&gt; section for examples of Bayesian inference to use as intuition pumps.&lt;/p&gt;
&lt;p&gt;An &amp;ldquo;agent&amp;rdquo; here refers to a physical entity that tries to predict the future. An agent can be a robot or a biological organism. Either way, the agent receives a stream of sensory data over the course of its life. The agent&amp;rsquo;s goal is to predict the future of this stream, in some capacity. Predictions about high-level objects and states of the world are made indirectly by predicting their effect on the sensory stream. That is to say, all prediction can be rolled into predicting the sensory stream. Though the agent may also act to influence the future of its sensory stream, for simplicity, I will consider only prediction and not actions.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s work in discrete time.&lt;br&gt;
Let $\X$ be the set of possible observations at each time-step.&lt;br&gt;
The agent&amp;rsquo;s sensory stream is a sequence $(x_1, x_2, x_3, \dots) \in \X^\infty$. I&amp;rsquo;m assuming the sensory stream is infinite, i.e. the agent lives forever. This is not a reasonable assumption but it&amp;rsquo;s workable. The alternative (assuming the stream is finite) is more complicated because it requires the agent to predict the end time of its own stream (presumably when it will die).&lt;/p&gt;
&lt;p&gt;Let $\o\in\X^\infty$.&lt;br&gt;
$\o_i$ denotes the $i$-th binary value in $\o$.&lt;br&gt;
$\o_{(i_1,i_2,\dots)}$ denotes the sub-sequence of $\o$ indexed by the sequence $(i_1,i_2,\dots)$.&lt;br&gt;
$\o_{i:j} = \o_{(i,i+1,\dots,j-1,j)}$ denotes the slice of $\o$ starting from $i$ and ending at $j$ (inclusive).&lt;br&gt;
$\o_{&amp;lt;i} = \o_{1:i-1}$ denotes the slice up to position $i-1$.&lt;br&gt;
$\o_{\leq i} = \o_{1:i}$ includes $i$.&lt;br&gt;
$\o_{&amp;gt;i} = \o_{i+1:\infty}$ denotes the unbounded slice starting at position $i+1$.&lt;/p&gt;
&lt;p&gt;Let $\X^* = \X^0 \cup \X^1 \cup \X^2 \cup \X^3 \cup \dots$ be the union of all finite cartesian products of $\X$, i.e. the set of all finite sequences of any length.&lt;br&gt;
Let $x,y\in\X^*$ be finite sequences.&lt;br&gt;
Denote $\abs{x}$ as the length of $x$.&lt;br&gt;
Denote $x`y = (x_1, \dots, x_\abs{x}, y_1, \dots, y_\abs{y})$ as the sequence concatenation of $x$ and $y$.&lt;br&gt;
Denote $x \sqsubset y$ to mean that $x = y_{1:\abs{x}}$, i.e. $x$ is a prefix of $y$.&lt;/p&gt;
&lt;h2 id=&#34;defining-a-hypothesis&#34;&gt;Defining A Hypothesis&lt;/h2&gt;
&lt;p&gt;As the agent gains sensory data, the agent will try to explain that data with various hypotheses. What constitutes a hypothesis is perhaps an open problem in epistemology. I will give the standard idea of &amp;ldquo;hypothesis&amp;rdquo; within Bayesian epistemology.&lt;/p&gt;
&lt;p&gt;A hypothesis is a &lt;strong&gt;probability measure&lt;/strong&gt;, $\mu$, over all possible sensory streams, $\X^\infty$. That means we can compute the $\mu$-probability of any set of infinite sequences, i.e. $\mu(A)$ for $A \subseteq \X^\infty$. Typically there is a set of hypotheses $\H$ under consideration, i.e. $\H$ is a set of different $\mu$.&lt;/p&gt;
&lt;p&gt;For any $\o\in\X^\infty$, the prefix $\o_{1:n}$ is a partial sensory sequence. The marginal $\mu$-probability of $\o_{1:n}$ is $\mu(\Gamma_{\o_{1:n}})$ where $\Gamma_{\o_{1:n}} = \set{\z\in\X^\infty \mid \o_{1:n}\sqsubset\z}$ is the set of all infinite sequences starting with $\o_{1:n}$, called a &lt;strong&gt;cylinder set&lt;/strong&gt;. For notational simplicity, I will write  $\mu(x)$ to denote $\mu(\Gamma_{x})$ when $x$ is a finite sequence, so $\mu(\o_{1:n})$ denotes $\mu(\Gamma_{\o_{1:n}})$.&lt;/p&gt;
&lt;p&gt;The quantity $\mu(\o_n \mid \o_{&amp;lt;n}) = \mu(\o_{1:n})/\mu(\o_{&amp;lt;n}) = \mu(\o_{1:n})/\mu(\o_{1:n-1})$ is the conditional $\mu$-probability of $\o_n$ given $\o_{&amp;lt;n}$ was already observed.&lt;/p&gt;
&lt;p&gt;$\mu$ defined in this way will guarantee that the following holds:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(\o_{&amp;lt;n}) = \int_\X \mu(\o_{&amp;lt;n}`\chi)d\chi\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;(replace integral with sum for countable $\X$.)&lt;/p&gt;
&lt;p&gt;Note that for most $\o\in\X^\infty$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lim_{n\to\infty} \mu(\o_{1:n}) = 0\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is similar to how a probability distribution on the real unit interval may assign 0 probability to any particular real number. Here $\o_{1:n}$ acts like an interval, which has probability mass, and the full sequence $\o$ acts like a single point, which does not have probability mass.&lt;/p&gt;
&lt;p&gt;A hypothesis $\mu$ ideally represents some primitive way the world might be. We might say a hypothesis probability $\mu(\o_{1:n})$ is &lt;em&gt;objective&lt;/em&gt; in some sense. Hypotheses that assign probabilities to outcomes are supposing the universe has inherent randomness that cannot be known before hand (we could also have deterministic hypotheses that assign probabilities of either 0 or 1).&lt;/p&gt;
&lt;p&gt;Bayesian inference distinguishes between two kinds of uncertainty: that due to inherent randomness in the universe, and that due to lack of knowledge of the agent. This distinction between kinds of uncertainty may not be precise (and I&amp;rsquo;m not sure I agree that such a distinction should be made), but this is typically how hypothesis probabilities are conceptually distinguished from subjective data probabilities (which I&amp;rsquo;ll get to in a moment).&lt;/p&gt;
&lt;p&gt;In this framing, the goal of Bayesian inference is to calculate one&amp;rsquo;s certainty (or uncertainty) that each hypothesis being considered explains the observed data. This kind of uncertainty is represented with probabilities just like irreducible uncertainty, but the former is in principle reducible to certainty. As the agent observes data, these probabilities change, and hence the agent&amp;rsquo;s uncertainty about each hypothesis changes over time (potentially approaching certainty).&lt;/p&gt;
&lt;h2 id=&#34;defining-bayesian-inference&#34;&gt;Defining Bayesian Inference&lt;/h2&gt;
&lt;p&gt;Let $\X^\infty$ be the set of all possible observations (infinite data sequences).&lt;br&gt;
Let $\H$ be a set of hypotheses (probability measures on $\X^\infty$).&lt;/p&gt;
&lt;p&gt;I will induce a joint probability distribution on $\H$ and $\X^\infty$.&lt;br&gt;
Let $p(\mu)$ be the &lt;strong&gt;prior probability&lt;/strong&gt; of $\mu\in\H$.&lt;br&gt;
Let $p(\o_{1:n} \mid \mu) = \mu(\o_{1:n})$ be the &lt;strong&gt;data probability&lt;/strong&gt; of $\o_{1:n}$ under hypothesis $\mu$. This is read as, &amp;ldquo;the probability of data $\o_{1:n}$ given hypothesis $\mu$.&amp;rdquo; Here, &amp;ldquo;giving the hypothesis&amp;rdquo; is equivalent to specifying what data distribution should be used to calculate the probability of $\o_{1:n}$.&lt;/p&gt;
&lt;p&gt;$p(\mu,\o_{1:n}) = p(\mu)p(\o_{1:n} \mid \mu) = p(\mu)\mu(\o_{1:n})$ is the joint probability of $\mu$ and $\o_{1:n}$. From here, we can compute any other quantity of interest.&lt;/p&gt;
&lt;p&gt;$$p(\o_{1:n}) = \int_\H p(\mu,\o_{1:n})d\mu = \int_\H p(\mu)\mu(\o_{1:n})d\mu$$&lt;/p&gt;
&lt;p&gt;is called the &lt;strong&gt;subjective (Bayesian) data probability&lt;/strong&gt; of $\o_{1:n}$ (replace integrals with sums if $\H$ is discrete). $p(\o_{1:n})$ does not condition on a hypothesis, and is the sum/integral over every hypothesis probability $\mu(\o_{1:n})$ weighted by the prior $p(\mu)$. You can think of $p(\o_{1:n})$ as a weighted-average over hypothesis probabilities.&lt;/p&gt;
&lt;p&gt;If a hypothesis probability $\mu(\o_{1:n})$ is &lt;em&gt;objective&lt;/em&gt; in some sense (a hypothesis represents the way the world actually might be), then a prior probability $p(\mu)$ represents the agent&amp;rsquo;s state of certainty/uncertainty about $\mu$ being true (how the world actually is). If the agent knew that $\mu$ was the case, then the agent would just use $\mu$ to predict the future sensory data, where $\mu$-probabilities are due to inherent randomness in the data. However, since the agent has uncertainty about which hypothesis is the case, $p(\o_{1:n})$ represents the agent&amp;rsquo;s total uncertainty about $\o_{1:n}$ occurring, which is a combination of inherent randomness in the data and the agent&amp;rsquo;s lack of knowledge about which hypothesis is true.&lt;/p&gt;
&lt;p&gt;Another quantity of interest:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(\mu\mid\o_{1:n}) &amp;amp;= p(\mu,\o_{1:n})/p(\o_{1:n}) \\ \\&lt;br&gt;
&amp;amp;= \frac{p(\mu)p(\o_{1:n}\mid\mu)}{p(\o_{1:n})}\\ \\&lt;br&gt;
&amp;amp;= p(\mu)\frac{\mu(\o_{1:n})}{p(\o_{1:n})}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is called the &lt;strong&gt;hypothesis posterior probability&lt;/strong&gt; (or more commonly, just &amp;ldquo;&lt;strong&gt;posterior&lt;/strong&gt;&amp;quot;) of hypothesis $\mu$ given data $\o_{1:n}$. This quantity represents the agent&amp;rsquo;s certainty/uncertainty about hypothesis $\mu$ after observing $\o_{1:n}$. If we are viewing $p(\mu)$ as a weight, then the weight on $\mu$ updates to $p(\mu\mid\o_{1:n})$ when $\o_{1:n}$ is observed. In this sense, the agent updates its knowledge (confidence) about the hypotheses in $\H$ when data is observed. The last equation is a prescription for an update rule on these uncertainty weights on $\mu$, taking the form $w&#39; = \gamma w$, where $\gamma = \mu(\o_{1:n})/p(\o_{1:n})$ is a multiplicative adjustment on the prior weight $w$.&lt;/p&gt;
&lt;p&gt;Observing additional data amounts to appending $\o_{n+1:m}$ to $\o_{1:n}$, and the weight on $\mu$ updates again to $p(\mu\mid\o_{1:m})$. This iterative process of observing more data and updating can go on forever, so long as the data space $\X^\infty$ consists of infinite sequences.&lt;/p&gt;
&lt;p&gt;As a side note, you should recognize $p(\mu\mid\o_{1:n})=p(\o_{1:n}\mid\mu)p(\mu)/p(\o_{1:n})$ as the classic form of &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem#Statement_of_theorem&#34;target=&#34;_blank&#34;&gt;Bayes rule&lt;/a&gt; (typically used to explain Bayesian inference). Usually, it is written like this: $p(H \mid D) = p(D \mid H)p(H)/p(D)$, where $H$ is a random variable for hypotheses (often parameter $\Theta$ is used in place of $H$), and $D$ is a random variable for datasets. In my opinion this form hides the sequential nature of Bayesian inference and is pedagogically confusing for that reason.&lt;/p&gt;
&lt;p&gt;In general, the agent wants to predict what sensory data will occur after $\o_{\leq n}$ has been observed. For that, we need the &lt;strong&gt;data posterior probability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(\o_{&amp;gt;n} \mid \o_{\leq n}) &amp;amp;= \int_\H p(\mu, \o_{&amp;gt;n} \mid \o_{\leq n})d\mu \\&lt;br&gt;
&amp;amp;= \int_\H p(\o_{&amp;gt;n} \mid \mu, \o_{\leq n})p(\mu\mid\o_{\leq n})d\mu \\&lt;br&gt;
&amp;amp;= \int_\H p(\mu\mid\o_{\leq n})p(\o_{&amp;gt;n} \mid \o_{\leq n},\mu)d\mu \\&lt;br&gt;
&amp;amp;= \int_\H p(\mu\mid\o_{\leq n})\mu(\o_{&amp;gt;n} \mid \o_{\leq n})d\mu \\&lt;br&gt;
&amp;amp;= \int_\H p(\mu)\frac{\mu(\o_{\leq n})}{p(\o_{\leq n})}\mu(\o_{&amp;gt;n} \mid \o_{\leq n})d\mu\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Like the subjective data probability $p(\o_{\leq n})$, the data posterior probability $p(\o_{&amp;gt;n} \mid \o_{\leq n})$ is subjective, in the sense that it reflects the agent&amp;rsquo;s uncertainty about the outcome, as well as any irreducible randomness. These two quantities have the same form: $p(\o_{\leq n})$ averages over $p(\mu)\mu(\o_{\leq n})$, and $p(\o_{&amp;gt;n} \mid \o_{\leq n})$ averages over $p(\mu\mid\o_{\leq n})\mu(\o_{&amp;gt;n} \mid \o_{\leq n})$. Either way, $p(\o_{\leq n})$ and $p(\o_{&amp;gt;n} \mid \o_{\leq n})$ are both weighted averages over all hypothesis probabilities, i.e. the inherent randomness of each hypothesis is weighted by the agent&amp;rsquo;s uncertainty about that hypothesis. The only difference is that the posterior conditions everything on the observed $\o_{\leq n}$.&lt;/p&gt;
&lt;h1 id=&#34;deconstructing-bayesian-inference&#34;&gt;Deconstructing Bayesian inference&lt;/h1&gt;
&lt;p&gt;To review, we assumed the agent has a set of hypotheses $\H$, where each hypothesis $\mu\in\H$ is a probability measure on infinite data sequences $\X^\infty$. For partial observation $\o_{1:n}$, each hypothesis assigns probability $\mu(\o_{1:n})$. Putting a prior on $\H$ allows us to compute $p(\o_{1:n})$, which is the &amp;ldquo;average&amp;rdquo; $\mu$-probability of $\o_{1:n}$. The resulting distribution $p$ is called the subjective data distribution.&lt;/p&gt;
&lt;h2 id=&#34;removing-hypotheses&#34;&gt;Removing hypotheses&lt;/h2&gt;
&lt;p&gt;Question: What is the difference between a hypothesis $\mu$ and a subjective data distribution $p$? Do we need hypotheses at all?&lt;/p&gt;
&lt;p&gt;They appear to have the same form: $\mu(\o_{1:n})$ vs $p(\o_{1:n})$. Above I gave a hand-wavy interpretational difference between the two. However, it is the case that hypotheses are not strictly necessary for predicting the continuation of a data sequence (equivalent to the singleton hypothesis space $\H = \set{p}$).&lt;/p&gt;
&lt;p&gt;The subjective data distribution $p$ can be defined directly by specifying the distribution $p(x_n \mid x_{&amp;lt;n})$ on $x_n\in\X$ for all finite $x_{&amp;lt;n} \in \X^*$. That is because for all $x_{1:n}\in\X$, the probability factorizes: $p(x_{1:n}) = p(x_{n}\mid x_{&amp;lt;n})p(x_{n-1}\mid x_{&amp;lt;n-1})\dots p(x_2\mid x_{&amp;lt;2})p(x_1)$. This amounts to filling in values for each node in a tree. For example, if $\X=\set{0,1}$, then we have a binary tree:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210317161633.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Each node in the tree is assigned a conditional probability distribution, i.e. for each node at position $x_{&amp;lt;n}$ (where $x_{&amp;lt;n} = (x_1, x_2,\dots,x_{n-1})$ encodes a path from the root), the probability $p(\chi \mid \dots)$ is specified for each $\chi\in\X$.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Each node&amp;rsquo;s distribution, $p(\chi \mid \dots)$, can be chosen independently from every other node (there are no constraints on these conditional distributions). Therefore, since the entire tree uniquely determines $p(x_{&amp;lt;n})$ (by walking down the edges of the tree corresponding to the observed data sequence $x_{&amp;lt;n}$ and multiplying the conditional probabilities at each node in the path, i.e. $p(x_i \mid x_{&amp;lt;i})$ at node $x_{&amp;lt;i}$), a Bayesian agent is free to choose any arbitrary conditional predictions it wants.&lt;/p&gt;
&lt;p&gt;Clearly, if we are only concerned with data predictions of the form $p(\o_{n:m}\mid \o_{&amp;lt;n})$, then we do not actually need hypotheses at all. Providing a tree specifying the subjective conditional data probabilities is enough to perform Bayesian inference.&lt;/p&gt;
&lt;p&gt;This reveals something peculiar about Bayesian inference: &lt;strong&gt;A Bayesian agent chooses its predictions for all eventualities beforehand, and never deviates for all time. Furthermore these predictions can be totally arbitrary.&lt;/strong&gt; There does not appear to be any actual learning taking place, since the agent just follows a path down the tree as data comes in and provides predetermined prediction probabilities. In the Bayesian perspective, learning IS narrowing down a pre-defined possibility space with data. If decision making using Bayesian inference is rational, perhaps rationality amounts to consistency, i.e. never deviating from pre-chosen predictions.&lt;/p&gt;
&lt;p&gt;One might argue that freely choosing a subjective data distribution without defining hypotheses is not Bayesian. My reply is that I&amp;rsquo;ve shifted what we are counting as hypothesis. In this perspective, a hypothesis is a single data sequence continuation. In this sense, all hypotheses are deterministic data sequences (though they may be algorithmically random), and the subjective data distribution is counting up the contributions of all these hypotheses to each finite length prediction. More on this in 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/bayesian-information-theory/&#34;&gt;Bayesian information theory&lt;/a&gt;
  


.&lt;/p&gt;
&lt;h2 id=&#34;removing-probabilities&#34;&gt;Removing probabilities&lt;/h2&gt;
&lt;p&gt;We can go a step further and ask, do we need probabilities at all?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s imagine that the agent has the same predetermined tree as above, except each node is assigned a single prediction instead of a probability distribution over $\X$. Let $x\in\X^*$ and denote $\hat{\x}_x\in\X$ as the prediction given the sequence $x$ is observed. Choosing $\hat{\x}_x$ for each $x$ uniquely defines a tree, where $x = (x_1, x_2, \dots)$ encodes a path from the root to a node, and $\hat{\x}_x$ is the value assigned to that node.&lt;/p&gt;
&lt;p&gt;Consider an example. Suppose $\X = \set{0,1}$ and that we have the following prediction tree:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210329223311.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;This tree has infinite depth.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In the figure, each node is assigned a prediction. Without any data, the agent predicts $0$. Given $0$ is observed, the agent predicts $1$. Given $01$ is observed, the agent predicts $0$. And so on.&lt;/p&gt;
&lt;p&gt;Suppose we observe $010$, which falls in line with all of the agent&amp;rsquo;s predictions up to that point. The prediction given $010$ is $1$. Suppose we observe $0$ next (for a total observation of $0100$). Then the agent&amp;rsquo;s prediction of $1$ is wrong, but that is okay since the agent has sub-tree at $0100$ and can go on making predictions from there.&lt;/p&gt;
&lt;p&gt;Is this setup technically Bayesian? We could convert this deterministic prediction tree to a subjective data distribution that assigns a probability 0 or 1 to all outcomes. So in this example, $p(x_1=0) = 1$ and $p(x_1=1)=0$, corresponding to a prediction of $0$ given nothing. $p(x_2=0\mid 0)=0$ and $p(x_2=1\mid 0)=1$, and $p(x_3=0\mid 01)=1$ and so on.&lt;/p&gt;
&lt;p&gt;The problem is that $p$ defined in this way is degenerate. Suppose the agent wrongly predicts the continuation of $010$ to be $1$ when it is actually $0$. Then&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(0101) &amp;amp;= p(x_4=1\mid 010)p(x_3=0\mid 01)p(x_2=1\mid 0)p(x_1=0) \\ &amp;amp;= 1\cdot 1\cdot 1\cdot 1 = 1\,.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;But $p(0100) = p(x_4=0 \mid 010)\dots = 0\cdot 1\cdot 1\cdot 1 = 0$. So from then on, the subjective probability of the data continuing $0100$ is 0, but the conditional probabilities may not be. If $p$ is defined as a measure on $\X^\infty$ instead of indirectly via the conditional probabilities specified in the tree, then $p$ cannot simultaneously have well-defined conditional probabilities and zero probability unconditionally (e.g. $p(x_5=0 \mid 0100) = 1$ while $p(01000) = 0$ and $p(0100) = 0$).&lt;/p&gt;
&lt;p&gt;We might conclude that this kind of deterministic prediction tree does not formally conform to the definition of Bayesian inference I gave earlier. However, this tree is the limit of the probabilistic prediction tree above as each conditional probability $p(x_i \mid x_{&amp;lt;i})$ goes to 0 or 1. Just as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirac_delta_function&#34;target=&#34;_blank&#34;&gt;Dirac delta distribution&lt;/a&gt; is not technically a function, but can be defined as the limit of Gaussian functions as their variance goes to 0, we might be able to do the same with deterministic prediction trees.&lt;/p&gt;
&lt;p&gt;Note that an agent that makes the same prediction no matter what is observed (a rigid agent) is technically Bayesian, with a subjective data distribution that puts all probability mass on the prediction sequence $\o\in\X^\infty$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x_{1:n}) = \begin{cases}1 &amp;amp; x_{1:n} = \o_{1:n} \\ 0 &amp;amp; x_{1:n} \neq \o_{1:n}\end{cases}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Such an agent is not interesting, as it does not even behave as though it learns.&lt;/p&gt;
&lt;p&gt;We shall see that an agent that does behave as though it learns (conditionally determines its predictions based on past history) need not provide Bayesian probabilities on its predictions for prediction uncertainty to naturally emerge.&lt;/p&gt;
&lt;h1 id=&#34;reconstructing-bayesian-inference&#34;&gt;Reconstructing Bayesian Inference&lt;/h1&gt;
&lt;h2 id=&#34;non-determinism&#34;&gt;Non-Determinism&lt;/h2&gt;
&lt;p&gt;To add clarity to what I want to explain next, let&amp;rsquo;s represent the deterministic prediction tree above as a prediction function $f : \X^* \to \X$, so that $f(x) = \hat{\x}_x$ is the prediction given data sequence $x\in\X^*$. That is to say, $f(x)$ returns the value at node $x$ in the tree. We can think of $f$ as an assignment function of values to nodes.&lt;/p&gt;
&lt;p&gt;Suppose we want to predict what will happen further out into the future. Specifically, suppose we observe $\o_{&amp;lt;n}\in\X^*$ and we want to predict the outcome $\o_m$ at time $m &amp;gt; n$. We don&amp;rsquo;t have access to the outcome $\o_n$ or anything after it. That is to say, the prediction tree gives the prediction $f(\o_{&amp;lt;n})$ for step $n$, but to predict step $m$ we need $\o_{&amp;lt;m} = \o_{&amp;lt;n}`\o_{n:m-1}$.&lt;/p&gt;
&lt;p&gt;You might say that the solution is to fill in our missing data with predictions, i.e. choose $\hat{\o}_n=f(\o_{&amp;lt;n})$ as the outcome for step $n$, and $\hat{\o}_{n+1} = f(\o_{&amp;lt;n}`\hat{\o}_n)$ as the outcome for step $n+1$, etc., so that our prediction at step $m$ is $\hat{\o}_m=f(\o_{&amp;lt;n}`\hat{\o}_{n:m-1})$. However, I&amp;rsquo;d argue that this is not actually the agent&amp;rsquo;s prediction as defined by the prediction tree. If the agent instead observed $\o_{n:m-1} \neq \hat{\o}_{n:m-1}$, the agent might predict something different from $\hat{\o}_m$. We could define $\hat{\o}_m$ as the agent&amp;rsquo;s prediction of step $m$ given $\o_{&amp;lt;n}$. By doing so, we&amp;rsquo;d be assigning infinite sequences to each node $x$ in the prediction tree, rather than single outcomes, so that we may query the prediction of arbitrarily many timesteps given ONLY data $x$. We can represent this prediction tree by the function $g:\X^*\to\X^\infty$, which returns an infinite sequence in $\X^\infty$ instead of a single element of $\X$, such that $[g(x_{&amp;lt;n})]_{&amp;lt;n} = x_{&amp;lt;n}$ and $[g(x_{&amp;lt;n})]_n$ is the next-step prediction for $x_{&amp;lt;n}$. The sequence $[g(x_{&amp;lt;n})]_{\geq n}$  is the multi-step prediction for $x_{&amp;lt;n}$. Now it is clear that $[g(\o_{&amp;lt;m})]_m$ and $[g(\o_{&amp;lt;n})]_m$ are potentially different predictions, and we need to distinguish between them.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210330173612.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;The tree encoded by $g$, where each node is assigned an infinite sequence that begins with the location of the node in the tree. In this example, each sequence is the argmax prediction starting at that node, i.e. assuming every next-step prediction is correct.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In general, if we ask an agent for some prediction of any time-step, the agent can produce some sort of answer. However, if we wanted to know what answer the agent would produce if all requisite data was available (call this the agent&amp;rsquo;s best prediction), that may not be determinable with the data currently available. We are uncertain about what the agent&amp;rsquo;s best prediction will be (given all requisite data). A truthful agent would be just as uncertain as we are about its own future predictions.&lt;/p&gt;
&lt;p&gt;Let $\mc{F}_m(\o_{&amp;lt;n}) = \set{f(\o_{&amp;lt;n}`\tilde{\o}_{n:m-1}) \mid \tilde{\o}_{n:m-1}\in\X^{m-n+1}}$ be the set of all predictions the agent could make about step $m$ given data $\o_{&amp;lt;n}$. If this set contains more than one element, then the agent&amp;rsquo;s prediction at $m$ is not uniquely determined by $\o_{&amp;lt;n}$, i.e. non-deterministic.&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210329223333.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;This is the non-deterministic prediction encoded by $\mc{F}_2 : \X^* \to \X$, where the untransformed tree encoded by $f:\X^*\to\X$ is our prediction tree example from above. $\mc{F}_2$ is obtained by superimposing the two sub-trees of $f$ at depth 1, i.e. directly under the root node.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This understanding of the word &amp;ldquo;non-deterministic&amp;rdquo; is standard in computer science, exemplified by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Nondeterministic_Turing_machine&#34;target=&#34;_blank&#34;&gt;non-deterministic Turing machine&lt;/a&gt;, which defines a set of possible state transitions and tape operations given any state (of both the tape and automata), rather than single one determined uniquely by each state. In general, something is regarded as non-deterministic if it can take on more than one possible value, i.e. it is not determined.&lt;/p&gt;
&lt;p&gt;I distinguish non-determinism from randomness, the latter of which can be defined as maximal incompressibility (called &lt;a href=&#34;http://www.scholarpedia.org/article/Algorithmic_randomness&#34;target=&#34;_blank&#34;&gt;algorithmic randomness&lt;/a&gt;). Often these two concepts are conflated. Probability distributions can represent both non-determinism (by quantifying relative amounts of possibilities) or degrees of randomness (via Martin-Lof randomness). The key insight is that probability need not represent both at the same time. If Bayesian probability is generally understood as quantifying prediction non-determinism, then we can view the &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_interpretations&#34;target=&#34;_blank&#34;&gt;Bayesian-frequentist distinction&lt;/a&gt; as stemming from this conceptual decoupling of non-determinism from randomness.&lt;/p&gt;
&lt;p&gt;I make the same connection between Bayesian uncertainty and non-determinism in 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/classical-vs-bayesian-reasoning/#the-bayesian-perspective&#34;&gt;Classical vs Bayesian Reasoning#the-bayesian-perspective&lt;/a&gt;
  


.&lt;/p&gt;
&lt;h3 id=&#34;the-general-case&#34;&gt;The General Case&lt;/h3&gt;
&lt;p&gt;We can go a step further. Why are we assuming that a prediction of step $n$ can be made given $\o_{&amp;lt;n}$? Plenty of pertinent information may not be available in the data at all (or the agent does not know how to extract such information from the data, e.g. encrypted information). An honest agent would account for every input needed to make a certain prediction, even inputs that are not available in $\o_{&amp;lt;n}$.&lt;/p&gt;
&lt;p&gt;In general, we are left with a non-deterministic next-step prediction tree represented by $f : \Z \times \X^* \to \X$ where $\Z$ is an auxiliary input space (input in addition to the observed data stream), which is usually some latent space (set of possible data or world-states not observed). Each node $x$ in the prediction tree is assigned the prediction set $f(\Z, x) = \set{f(z,x) \mid z\in\Z}$. If the prediction set for $x$ is singleton (contains one element), then that prediction is deterministic. $f$ represents a prediction tree where each node is assigned a set of next-step predictions instead of just one.&lt;/p&gt;
&lt;h2 id=&#34;the-return-of-probability&#34;&gt;The Return Of Probability&lt;/h2&gt;
&lt;p&gt;In 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/classical-vs-bayesian-reasoning/#the-bayesian-axiom&#34;&gt;Classical vs Bayesian Reasoning#the-bayesian-axiom&lt;/a&gt;
  


, I introduced the Bayesian axiom - an informal axiom of epistemology - which states that the relative fraction of model (in our case $f$) inputs which produce each output can be regarded as knowledge. In this case, if the agent&amp;rsquo;s prediction $f(\Z, \o_{&amp;lt;n})$ is not uniquely determined, we might want to proceed with our decision making anyway. Supposing we have some normalized measure $\mu$ on $\Z$ (ideally the uniform measure if there is one), then the relative fraction of $\Z$ that produces each prediction $\chi\in\X$ is&lt;/p&gt;
&lt;p&gt;$$p(\chi \mid x) = \mu\set{f(z,x) \mid z\in\Z \and f(z,x) = \chi}\,.$$&lt;/p&gt;
&lt;p&gt;Though the agent&amp;rsquo;s prediction is not uniquely determined, we&amp;rsquo;ve quantified the relative number of possible auxiliary inputs to $f$ that give each prediction. These quantities can be used for decision making (e.g. most likely outcome or expected return).&lt;/p&gt;
&lt;p&gt;We have recovered the probabilistic prediction tree paradigm from earlier without presupposing that predictions should be probabilistic. Though not defined to be probabilistic, an agent&amp;rsquo;s prediction function that requires inputs which are not on hand naturally results in non-determinism. Quantifying that non-determinism using relative sizes of possibility sets naturally results in probabilities.&lt;/p&gt;
&lt;p&gt;With this perspective in mind, we could interpret any explicitly defined subjective data distribution to be implying the existence of a deterministic prediction function. Bayesian probabilities represent the number of inputs to this prediction function that produce the corresponding outputs. Given $p$, a canonical deterministic prediction function $f$ is the decoder for a compressed representation of the data, with a uniform distribution on the compression.&lt;/p&gt;
&lt;p&gt;To be more specific, let $p$ be a probability measure on $\X^\infty$. Let $f : \Z^\infty \to \X^\infty$ be an &lt;a href=&#34;https://en.wikipedia.org/wiki/Arithmetic_coding&#34;target=&#34;_blank&#34;&gt;arithmetic decoder&lt;/a&gt; using $p$, i.e. $f$ takes as input an infinite compressed sequence $z_{1:\infty}\in\Z^\infty$ and outputs the decompressed sequence $x_{1:\infty}\in\X^\infty$. For finite inputs and outputs, use the cylinder set $f(\Gamma_{z_{1:n}})$. Putting a uniform probability measure on $\Z^\infty$ results in measure $p$ on $\X^\infty$. Now we can draw a connection between probability in three different contexts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Shannon Information&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;If $x_{1:\infty}$ is sampled randomly from $p$, then $z_{1:\infty}$ s.t. $f(z_{1:\infty}) = x_{1:\infty}$ achieves the optimal compression of $x_{1:\infty}$ according to Shannon. That is to say, $\abs{z_{1:n}} = n \approx -\lg p(f(\Gamma_{z_{1:n}}))$, and these quantities converge as $n\to\infty$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Algorithmic Randomness&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;If input $z_{1:\infty}$ is algorithmically random, then output $f(z_{1:\infty})$ is $p$-random (or $p$-typical, see &lt;a href=&#34;https://en.wikipedia.org/wiki/Algorithmically_random_sequence&#34;target=&#34;_blank&#34;&gt;wiki&lt;/a&gt;). $z_{1:\infty}$ is a shortest algorithmic compression of $f(z_{1:\infty})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Non-determinism&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;Our prediction of $x_n$ given $x_{&amp;lt;n}$ depends on the compressed representation of $x_{1:n}$, an input $z_{1:k}$ to $f$ s.t. $f(\Gamma_{z_{1:k}}) = \Gamma_{x_{1:n}}$. Our non-deterministic prediction of $x_{n}$ given $x_{&amp;lt;n}$ is the set $f^{-1}(\Gamma_{x_{1:n}})$, which is all compressions compatible with $x_{1:n}$. This is quantified with the probability $p(x_n \mid x_{&amp;lt;n}) = \lambda(f^{-1}(\Gamma_{x_{1:n}})) / \lambda(f^{-1}(\Gamma_{x_{&amp;lt;n}}))$ where $\lambda$ is the uniform probability measure on $\Z^\infty$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;use-cases&#34;&gt;Use Cases&lt;/h1&gt;
&lt;p&gt;This section runs through a few canonical examples of Bayesian inference which can be used as intuition pumps while reading the previous sections.&lt;/p&gt;
&lt;h2 id=&#34;inferring-bias-on-a-coin&#34;&gt;Inferring bias on a coin&lt;/h2&gt;
&lt;p&gt;Let $\X = \set{0,1}$ be the outcome of a coin toss, and $\o \in \X^\infty$ be an infinite sequence of coin toss outcomes.&lt;/p&gt;
&lt;p&gt;Let $\mc{B}_\t$ be the Bernoulli distribution on $\X$ with parameter $\t\in[0,1]$ which is the probability of $1$, so $\mc{B}_\t(1) = \t$ and $\mc{B}_\t(0) = 1-\t$.&lt;/p&gt;
&lt;p&gt;The hypothesis $\mu_\t$ is the product of Bernoulli distributions:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu_\t(x_{1:n}) = \prod_{i=1}^n\mc{B}_\t(x_i) = \t^{\sum_i x_i}(1-\t)^{n-\sum_i x_i}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The hypothesis set is $\H = \set{\mu_\t}_{\t\in[0,1]}$. These hypotheses are i.i.d. w.r.t. sequence position, i.e. $\mu_\t(x_n \mid x_{&amp;lt;n}) = \mu_\t(x_n)$.&lt;/p&gt;
&lt;p&gt;Subjective data probability:&lt;br&gt;
$$&lt;br&gt;
p(x_{1:n}) = \int_0^1 p(\t)\mu_\t(x_{1:n}) d\t = \int_0^1 p(\t)\t^{\sum_i x_i}(1-\t)^{n-\sum_i x_i} d\t&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Data posterior:&lt;br&gt;
$$&lt;br&gt;
p(x_n \mid x_{&amp;lt;n}) = \int_0^1 p(\t\mid x_{&amp;lt;n})\mu_\t(x_n \mid x_{&amp;lt;n})d\t = \int_0^1 p(\t\mid x_{&amp;lt;n})\mc{B}_\t(x_n)d\t&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Hypothesis posterior:&lt;br&gt;
$$&lt;br&gt;
p(\t\mid x_{1:n}) = p(\t)\frac{\mu_\t(x_{1:n})}{p(x_{1:n})}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Note that the hypotheses are i.i.d. w.r.t. data position but the subjective data distribution $p$ is not. That is to say, $\mu_\t(x_n \mid x_{&amp;lt;n}) = \mu_\t(x_n)$ but $p(x_n \mid x_{&amp;lt;n}) \neq p(x_n)$.&lt;/p&gt;
&lt;h2 id=&#34;solomonoff-induction&#34;&gt;Solomonoff Induction&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s continue the coin tossing example, but instead of coin tossing, suppose any arbitrary process that produces a binary data stream. That is to say, we are allowing dependencies between bits in the data sequence.&lt;/p&gt;
&lt;p&gt;Bernoulli hypotheses are blind to patterns in the ordering of bits in the data sequence because $\mu_\t(x_{1:n}) = \mu_\t(\mathrm{permute}(x_{1:n}))$ where $\mathrm{permute}(x_{1:n})$ is any permutation (re-ordering) of $x_{1:n}$. For example, we observe a very long sequence of alternating $0$s and $1$s, i.e. $0101010101\dots$, then the Bernoulli hypothesis where $\t=1/2$ will get large posterior weight. This is the hypothesis that the data is totally random (maximum entropy). Clearly, the data is highly patterned and predictable.&lt;/p&gt;
&lt;p&gt;What hypotheses should we add to $\H$? Is it possible to have a hypothesis for every possible data pattern? Ray Solomonoff&amp;rsquo;s answer is yes, and this is achieved by the hypothesis set containing all semicomputable semimeasures on $\X^\infty$.&lt;/p&gt;
&lt;p&gt;A probability measure $\mu$ on $\X^\infty$ is &lt;a href=&#34;https://en.wikipedia.org/wiki/Computable_function&#34;target=&#34;_blank&#34;&gt;&lt;strong&gt;computable&lt;/strong&gt;&lt;/a&gt; if there exists a program (Turing machine) which outputs the probability $\mu(x_{1:n})$ of any input, $x_{1:n} \in \X^n$ (for any $n\in\mb{N}$), to the requested precision, $\e &amp;gt; 0$, in finite time and then halts. Alternatively, a probability measure is computable if there exists a program that outputs each possible outcome $x_{1:n} \in \X^n$ (for any $n\in\mb{N}$) with probability $\mu(x_{1:n})$, given a stream of uniformly random input bits (i.i.d. Bernoulli(1/2) data).&lt;/p&gt;
&lt;p&gt;A probability measure $\mu$ on $\X^\infty$ is &lt;a href=&#34;https://en.wikipedia.org/wiki/Semicomputable_function&#34;target=&#34;_blank&#34;&gt;&lt;strong&gt;semicomputable&lt;/strong&gt;&lt;/a&gt; if there exists a program (Turing machine) which outputs an infinite monotonic (increasing or decreasing) sequence of rational numbers $(\hat{q}_i)_{i\in\mb{N}}$ s.t. $\hat{q}_i$ converges to $\mu(x_{1:n})$ as $i\to\infty$, for any $x_{1:n} \in \X^n$ (for any $n\in\mb{N}$). All computable measures are semicomputable. A semicomputable measure is not computable if the error $\e_i = \abs{\hat{q}_i - \mu(x_{1:n})}$ at position $i$ in the output sequence cannot be computably determined. If this error could be determined, this program can be converted into the one above that approximates $\mu(x_{1:n})$ to the desired error in finite time. In other words, a semicomputable measure can be approximated to arbitrary accuracy, but you may not be able to determine how close any given approximation is, whereas a computable measure can be arbitrarily approximated with known error.&lt;/p&gt;
&lt;p&gt;A semimeasure $\mu$ on $\X^\infty$ is like the measure we defined earlier in &lt;a href=&#34;#defining-a-hypothesis&#34;&gt;#Defining A Hypothesis&lt;/a&gt; except for one difference: Rather than strict equality we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x_{&amp;lt;n}) \geq \int_\X \mu(x_{&amp;lt;n}`\chi)d\chi\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;For $\X = \set{0,1}$, this property becomes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Measure: $\mu(x_{&amp;lt;n}) = \mu(x_{&amp;lt;n}`0) + \mu(x_{&amp;lt;n}`1)$&lt;/li&gt;
&lt;li&gt;Semimeasure: $\mu(x_{&amp;lt;n}) \geq \mu(x_{&amp;lt;n}`0) + \mu(x_{&amp;lt;n}`1)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that $0 \leq \mu(x_{&amp;lt;n}) \leq 1$ still holds. We just allow probabilities to sum to less than 1.&lt;/p&gt;
&lt;p&gt;The set of all computable measures is conceptually nicer to think about than (and is a subset of) the set of all semicomputable semimeasures, but is not practically useful because it cannot be enumerated by a program, i.e. is &lt;a href=&#34;https://en.wikipedia.org/wiki/Recursively_enumerable_set&#34;target=&#34;_blank&#34;&gt;&lt;strong&gt;computably enumerable&lt;/strong&gt;&lt;/a&gt; (c.e.). To enumerate this set, you&amp;rsquo;d have to determine which programs halt after emitting their approximation of $\mu(x_{1:n})$.&lt;/p&gt;
&lt;p&gt;The set of all semicomputable semimeasures can be enumerated by a program, by enumerating all programs in lexicographic order, running them all simultaneously (called &lt;a href=&#34;https://en.wikipedia.org/wiki/Dovetailing_%28computer_science%29&#34;target=&#34;_blank&#34;&gt;dovetailing&lt;/a&gt;, see also &lt;a href=&#34;https://en.wikipedia.org/wiki/Recursively_enumerable_set#Examples&#34;target=&#34;_blank&#34;&gt;c.e. set&lt;/a&gt;), and continually filtering out the programs whose output does not compute a valid semimeasure. The virtue of using semimeasures rather than measures is that we don&amp;rsquo;t need to wait for the programs being enumerated to halt. If one is found to violate the semimeasure property (probabilities sum to greater than 1) it is filtered out. Otherwise it remains and we don&amp;rsquo;t have to check that the probabilities it assigns to various outcomes sum to exactly 1 (which would require waiting for it to halt).&lt;/p&gt;
&lt;p&gt;Let hypothesis set $\H$ be the set of all semicomputable semimeasures. Then the subjective data distribution is also a semicomputable semimeasure:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x_{1:n}) = \sum_{\mu\in\H} p(\mu)\mu(x_{1:n})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Solomonoff makes a prescription on what prior to use:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(\mu) = 2^{-K(\mu)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $K(\mu)$ is the &lt;a href=&#34;http://www.scholarpedia.org/article/Algorithmic_complexity#Prefix_complexity&#34;target=&#34;_blank&#34;&gt;prefix-free Kolmogorov complexity&lt;/a&gt; of hypothesis $\mu\in\H$. This prior weights hypotheses inversely by complexity - a sort of Occam&amp;rsquo;s razor.&lt;/p&gt;
&lt;h2 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;We have a family of functions $\set{h_\t}_{\t\in\T}$ with signature $h_\t : \X \to \Phi$ (for example, $h_\t$ could be a &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network&#34;target=&#34;_blank&#34;&gt;neural network&lt;/a&gt;), where $\Phi$ is the parameter space for a probability distribution $\P$ on the space $\Y$. That is to say, $\P(y \mid h_\t(x))$ is the probability of $y\in\Y$ given $x\in\X$ according to the model $h_\t(x)$. We call $\X$ the input space, and $\Y$ the target (output) space.&lt;/p&gt;
&lt;p&gt;We observe a data sequence $x_1y_1x_2y_2x_3y_3\dots x_n$ ending in $x_n$. Let $D = x_1y_1x_2y_2x_3y_3\dots x_{n-1}y_{n-1}$ be the training data. $x_n$ is called the test input. The goal is to predict the next element in the sequence, $y_n$.&lt;/p&gt;
&lt;p&gt;We can take a Bayesian approach by defining the following hypothesis space:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\H = \set{\mu_\t \mid \t\in\T}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\mu_\t$ is a probability measure on $\X\times\Y\times\X\times\Y\dots = (\X\times\Y)^\infty$ s.t. for all $\o\in(\X\times\Y)^\infty$ and for all $(x_i,y_i)\in\o$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu_\t(y_i \mid x_i) = \P(y_i \mid h_\t(x_i))\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\mu_\t$ is i.i.d. w.r.t. sequence position, i.e. $\mu_\t(y_i \mid x_i) = \mu_\t(y_i \mid x_1y_1\dots x_i)$. That is to say, all of our hypotheses assume the data is drawn i.i.d. (and so the data sequence $D$ becomes a data&lt;em&gt;set&lt;/em&gt;, in the sense that it is unordered). If we are doing supervised learning, then the marginal probabilities $\mu_\t(x_i)$ should all be uniform (the hypothesis is indifferent to which input $x\in\X$ comes next in the sequence). On the other hand, if we are doing unsupervised or semi-supervised learning, then $\mu_\t(x_i)$ should also depend on the model output $h_\t(x_i)$.&lt;/p&gt;
&lt;p&gt;Finally, put a prior $p$ on the parameter space $\T$. Then the subjective probability of the test output $y_n$ given test input $x_n$ is the data posterior of $y_n$ given the data sequence $D`x_n = x_1y_1x_2y_2x_3y_3\dots x_n$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(y_n \mid D`x_ny_n) &amp;amp;= \int_\T p(\t\mid D`x_n)\mu_\t(y_n \mid D`x_n)d\t \\&lt;br&gt;
&amp;amp;= \int_\T p(\t\mid D`x_n)\mu_\t(y_n \mid x_n)d\t&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where the parameter posterior is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(\t\mid D`x_n) &amp;amp;= p(\t)\frac{\mu_\t(D`x_n)}{p(D`x_n)}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;In the supervised case $\mu_\t(x_1) = \mu_\t(x_2) = \dots = \mu_\t(x_n) = \gamma$, and so $\mu_\t(D`x_n) = \gamma\mu_\t(D)$.  Likewise, $p(D`x_n) = \int_\T p(\t)\mu_\t(D`x_n) d\t = \gamma\int_\T p(\t)\mu_\t(D) d\t$, and so the fraction $\mu_\t(D`x_n)/p(D`x_n)$ simplifies to $\mu_\t(D)/p(D)$. Then the parameter posterior reduces to the &lt;a href=&#34;http://mlg.eng.cam.ac.uk/zoubin/bayesian.html&#34;target=&#34;_blank&#34;&gt;classic form&lt;/a&gt;: $p(\t\mid D`x_n) = p(\t\mid D) = p(\t)p(D\mid \t)/p(D)$. where $p(D\mid \t) = \mu_\t(D)$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Classical vs Bayesian Reasoning</title>
      <link>https://danabo.github.io/blog/posts/classical-vs-bayesian-reasoning/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/classical-vs-bayesian-reasoning/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\0}{\mathrm{false}}&lt;br&gt;
\newcommand{\1}{\mathrm{true}}&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\and}{\wedge}&lt;br&gt;
\newcommand{\or}{\vee}&lt;br&gt;
\newcommand{\a}{\alpha}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\x}{\xi}&lt;br&gt;
\newcommand{\fa}{\forall}&lt;br&gt;
\newcommand{\ex}{\exists}&lt;br&gt;
\newcommand{\X}{\mc{X}}&lt;br&gt;
\newcommand{\Y}{\mc{Y}}&lt;br&gt;
\newcommand{\Z}{\mc{Z}}&lt;br&gt;
\newcommand{\P}{\Psi}&lt;br&gt;
\newcommand{\y}{\psi}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\m}{\times}&lt;br&gt;
\newcommand{\E}{\mc{E}}&lt;br&gt;
\newcommand{\e}{\varepsilon}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\inv}[1]{{#1}^{-1}}&lt;br&gt;
\newcommand{\Iff}{\Leftrightarrow}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;My goal is to identify the core conceptual difference between someone who accepts &amp;ldquo;Bayesian reasoning&amp;rdquo; as a valid way to obtain knowledge about the world, vs someone who does not accept Bayesian reasoning, but does accept &amp;ldquo;classical reasoning&amp;rdquo;. By classical reasoning, I am referring to the various forms of boolean logic that have been developed, starting with Aristotelian logic, through propositional logic like that of Frege, and culminating in formal mathematics (e.g. higher order type theory). In such logics, the goal is to uniquely determine the truth values of things (such as theorems and propositions) from givens.&lt;/p&gt;
&lt;p&gt;My thesis is that the difference between Bayesian and classical reasoners comes down to how they deal with non-determined objects (e.g. if you cannot determine the truth value of something from your givens). The classical reasoner will shrug their shoulders and say &amp;ldquo;the answer cannot be determined, collect more givens or modify your definitions&amp;rdquo;. The Bayesian reasoner will regard the proportion of self-consistent instantiations of unknowns that make the target proposition true as epistemologically salient. That is to say, the Bayesian reasoner continues on without uniquely determined truth values, and the classical reasoner does not.&lt;/p&gt;
&lt;p&gt;This difference extends beyond epistemology into the realm of machine learning. Classical epistemology is interested in truth (i.e. uniquely determined quantities), whereas Bayesian epistemology is interested in degrees of certainty. In machine learning, the givens and unknowns in question are not boolean valued, but have arbitrary data types (e.g. vectors of reals). The classical learner is interested in what numbers can be uniquely determined from data, and the Bayesian learner is interested in proportions of possibilities that result in each number.&lt;/p&gt;
&lt;p&gt;This philosophical difference leads to a practical methodological difference. A classical reasoner/learner will define universes such that unknowns can be uniquely determined. Otherwise, the definitions are not useful. A Bayesian reasoner/learner will define universes such that calculating posterior probabilities are tractable. Otherwise, the definitions are not useful.&lt;/p&gt;
&lt;p&gt;A note on the definition of &lt;strong&gt;model&lt;/strong&gt;. In mathematics, a model is an instantiation of an unspecified object which satisfies given axioms. In machine learning, a model refers to a family of instantiations of free parameters, i.e. a model is the set of definitions which invoke free variables that are to be determined.&lt;/p&gt;
&lt;h1 id=&#34;propositional-logic&#34;&gt;Propositional logic&lt;/h1&gt;
&lt;h2 id=&#34;the-math-perspective&#34;&gt;The math perspective&lt;/h2&gt;
&lt;p&gt;I will be using the example in Russell and Norvig&amp;rsquo;s &lt;a href=&#34;http://aima.cs.berkeley.edu/&#34;target=&#34;_blank&#34;&gt;Artificial Intelligence: A Modern Approach&lt;/a&gt; (3rd edition), chapter 7.&lt;/p&gt;
&lt;p&gt;We are introduced to &amp;ldquo;wumpus world&amp;rdquo;, a &lt;a href=&#34;http://gridworld.info&#34;target=&#34;_blank&#34;&gt;grid world&lt;/a&gt; containing an enemy called the wumpus, death pits, and gold.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223110223.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
The relevant rules are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the agent starts in the bottom left corner&lt;/li&gt;
&lt;li&gt;the agent can only see what is contained in the cell it currently occupies&lt;/li&gt;
&lt;li&gt;the agent will detect a &amp;ldquo;breeze&amp;rdquo; if it&amp;rsquo;s cell is adjacent to a pit&lt;/li&gt;
&lt;li&gt;if the agent moves into a pit, it dies&lt;/li&gt;
&lt;li&gt;the goal is to get to the gold&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose the agent moves right:&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223110534.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
No breeze is detected in the starting cell [1,1], so the agent knows there is no pit up or to the right. In cell [2,1], there is a breeze, so the agent knows there is a pit above or to the right (or both).&lt;/p&gt;
&lt;p&gt;Each case can be defined by the following propositions:&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210223110745.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210223110745.png&#34; width=&#34;300&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
We define the following boolean variables:&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210223110852.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210223110852.png&#34; 
         alt=&#34;$P_{x,y}$ is instantiated as a different variable for each coordinate, i.e. $P_{1,1}, P_{1,2}, P_{2,1},\ldots$ are all different variables.&#34; width=&#34;500&#34;/&gt;&lt;/a&gt;&lt;/span&gt;&lt;span class=&#34;caption&#34;&gt;
            &lt;p&gt;$P_{x,y}$ is instantiated as a different variable for each coordinate, i.e. $P_{1,1}, P_{1,2}, P_{2,1},\ldots$ are all different variables.&lt;/p&gt;
        &lt;/span&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;model&lt;/strong&gt; (in the math sense) is an instantiation of these variables (in contrast to the machine learning sense where a model is the entire definition of the game and all the variables). There are 4 variables for each coordinate, and 16*4 = 64 variables in total. Thus a model can be viewed as a length 64 boolean vector. Suppose $m$ is such a vector. Then if $\a_1$ is true for $m$, we say that $m$ is a model of $\a_1$.&lt;/p&gt;
&lt;p&gt;Let $M(\a)$ be the set of all models (i.e. length 64 boolean vectors) satisfying some arbitrary sentence $\a$. Let $\beta$ be another sentence. We say that $\a$ &lt;strong&gt;entails&lt;/strong&gt; $\beta$, notated $\a \models \beta$, iff $M(\a) \subseteq M(\beta)$.&lt;/p&gt;
&lt;p&gt;Diagrammatically, we can depict the sets $M(\a_1)$ and $M(\a_2)$ (referring to the sentences above), as well as our knowledge base (KB) (i.e. what is given):&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223113244.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
In our wumpus world, the &amp;ldquo;sentences&amp;rdquo; above can be formally states as propositions, along with the rules for the game:&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223111036.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
We can prove simple things like &amp;ldquo;there is no pit in [1,2]&amp;rdquo; using the rules of logical inference. The following propositions are true in all models where $R_1,\ldots,R_5$ are true:&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210223112618.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
Each $R_i$ is entailed by the proceeding $R_j$ for $j &amp;lt; i$. A sequence of such propositions resulting in a desired proposition is a proof. From $R_{10}$, we can conclude $\neg P_{1,2}$. This sequence of propositions is a proof of $\a_1$ = &amp;ldquo;there is no pit in [1,2]&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We can also verify the statement $\a_1$ is true with brute force instead of logical inference: by enumerating all models ($2^{64}$ of them), selecting the models which satisfy $R_1,\ldots,R_5$ (i.e. $M(R_1\and\ldots\and R_5)$), and then checking if $\neg P_{1,2}$ is true in all of them.&lt;/p&gt;
&lt;p&gt;Notice that we cannot prove $P_{2,2}$ or its negation $\neg P_{2,2}$ given $R_1,\ldots,R_5$. That is because some models of $R_1\and\ldots\and R_5$ are consistent with $P_{2,2}$ while others are consistent with $\neg P_{2,2}$. That is to say, the truth value of variable $P_{2,2}$ is not uniquely determined by the givens $R_1,\ldots,R_5$. Speaking informally, we do not have enough information to know whether there is a pit at [2,2]. Therefore a rational agent would not make decisions based on $P_{2,2}$ being true or false. This is a core tenet of classical logic, whereas we shall see later, a Bayesian reasoner might be able to do more with the same information.&lt;/p&gt;
&lt;h2 id=&#34;the-machine-learning-perspective&#34;&gt;The machine learning perspective&lt;/h2&gt;
&lt;p&gt;In machine learning, a &lt;strong&gt;model&lt;/strong&gt; is a function $f : \O \to \X$, where $\O$ is called the state set, and $\X$ is called the observation set. This is different from the math notion of a model we saw above.&lt;/p&gt;
&lt;p&gt;Typically $\O$ and $\X$ are each the Cartesian products of other primitive types (which are sets, e.g. the reals, the integers, the booleans, etc.). Thus elements of $\O$ and $\X$ are typically tuples. Each element of the tuple is called a dimension. Generally, $\O$ and $\X$ are high-dimensional (elements of $\O$ and $\X$ are very long tuples, possibly infinite).&lt;/p&gt;
&lt;p&gt;An element $\o \in \O$ is called a state, and is considered to be a possible state of the world, where the world is the model. Often, $\O = \T\m\E$, where $\T$ is called the parameter set, and $\E$ is the noise set (both sets are themselves multi-dimensional). In the typical ML formulation, $\t\in\T$ is explicitly represented but $\e\in\E$ is not, because $\t$ is what is being &amp;ldquo;solved for&amp;rdquo; while $\e$ represents random inputs. In my formulation, I combine both into a single state $\o = (\t,\e)$. If $\o$ is some tuple, then a subtuple of $\o$ is called a substate, so $\t$ and $\e$ are substates of $\o = (\t, \e)$.&lt;/p&gt;
&lt;p&gt;An element $x \in \X$ is called an observation. Usually we are only given a partial observation. If $\X = T_1 \m T_2 \m T_3 \m \dots$ for primitive types $T_1, T_2, T_3, \ldots$, then a full observation is $x = (t_1, t_2, t_3, \ldots) \in \X$. A partial observation is a subset of elements in the tuple $x$. We denote partial observations with subscripts: $x_{1,5,10}$ is the tuple $(t_1, t_5, t_{10})$. We can also take slices: $x_{a:b} = (t_a, t_{a+1}, \ldots, t_{b-1}, t_b)$ is the slice from index $a$ to $b$. The shorthand $x_{&amp;gt;a}$ is the tuple of all indices larger than $a$, and $x_{&amp;lt;a}$ is the tuple of all indices less than $a$. It is sometimes convenient to think of a partial observation $x_I$ (for index tuple $I$) as a subset of $\X$, i.e. the set of all $x\in\X$ satisfying the partial observation. For example, $x_{1,5,10} = \set{(t_1, t_2, \ldots)\in\X \mid t_1 = x_1 \and t_5 = x_5 \and t_{10} = x_{10}}$. Let $x_{a:b}`x_{x:d} = x_{a:b,c:d}$ denote tuple concatenation.&lt;/p&gt;
&lt;p&gt;The goal of machine learning is to determine an unobserved partial observation  from an observed partial observation. If the unobserved part is going to be observed in the future, we call this prediction (if it happened in the past, we call this retrodiction). If the unobserved part is atemporal, or never observed, we call this inference. An unobservable partial observation is called latent.&lt;/p&gt;
&lt;h3 id=&#34;unsupervised-learning&#34;&gt;Unsupervised learning&lt;/h3&gt;
&lt;p&gt;The observation space is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\X = \Xi_1\m\Xi_2\m\dots\m\Xi_i\m\dots&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\x_i \in \Xi_i$ is called an &lt;em&gt;example&lt;/em&gt; (I am using &amp;ldquo;xi&amp;rdquo;, $\x$, instead of $x$ since $x$ already denotes a full observation).&lt;/p&gt;
&lt;p&gt;We are given a partial observation $D$, called the dataset:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
D = (\x_1, \x_2,\x_3, \ldots, \x_n)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Given model $f : \O \to \X$ and dataset $D$ (regarding $D$ as a subset of $\X$), then $\inv{f}(D)$ is the set of all states in $\O$ compatible with $D$.&lt;/p&gt;
&lt;p&gt;Typically in machine learning, the dataset $D$ does not uniquely determine a state $\o\in\O$. To further narrow down the possibilities, additional constraints are added. Typically, $\O = \T\m\E$, where the $\T$ component of the state tuple is narrowed down further by maximizing the data probability $p_\t(D)$ w.r.t. $\t\in\T$, and $\e \sim p_\t(D)$ is randomly chosen from the distribution. Additionally, the maximization of $p_\t(D)$ may be &amp;ldquo;regularized&amp;rdquo; by jointly minimizing some real-valued function $L(\t)$. Even then, $\t\in\T$ may not be uniquely determined (as is the case in deep learning), and so $\t$ will be arbitrarily chosen from the remaining possibilities.&lt;/p&gt;
&lt;p&gt;In the case of unsupervised learning, $f$ is called a generative model, and we use it to generate unobserved examples. Assume we&amp;rsquo;ve narrowed down the possibility space $\O$ to one state $\o^*$. We simply looking at&lt;/p&gt;
&lt;p&gt;$$f(\o^*) = (\x_1, \x_2, \ldots, \x_n, \x_{n+1}, \x_{n+2}, \dots)$$&lt;/p&gt;
&lt;p&gt;which provides $\x_{n+1}, \x_{n+2}, \dots$ outside of the partial observation $D$. Note that $\o$ typically contains a choice of noise $\e\in\E$, which injects randomness into the generated examples (generative models are normally thought of as probability distributions, and generating examples is a process of sampling from $p_{\t^*}(\x_i)$ for chosen parameter $\t^*$).&lt;/p&gt;
&lt;p&gt;Because $D$ does not uniquely determine an input $\o$ to $f$ (i.e. $\inv{f}(D)$ is not singleton), but a particular $\o^* \in \inv{f}(D)$ is chosen anyway, this results in some difficulties. Some $\o$ will produce generated examples $\x_{n+1}, \x_{n+2}, \dots$ which &amp;ldquo;look like the examples in $D$&amp;rdquo; where other choices of $\o$ (still compatible with $D$) will not. We say that $f(\o)$ generalizes if it outputs the unobserved partial observation that humans consider to be correct or appropriate (e.g. looks like the data in $D$). This is all very subjective, and it is very difficult to provide appropriate constraints (like the ones I mentioned above) so that for all possible $D$, the resulting $\o_D^*$ generalizes (i.e. many humans agree that $\x_{n+1}, \x_{n+2}, \dots$ &amp;ldquo;look like&amp;rdquo; $D$).&lt;/p&gt;
&lt;h3 id=&#34;supervised-learning&#34;&gt;Supervised learning&lt;/h3&gt;
&lt;p&gt;The observation space is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\X = \Xi_1\m\P_1\m\Xi_2\m\P_2\m\ldots\m\Xi_i\m\P_i\m\ldots&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\x_i \in \Xi_i$ is called an &lt;em&gt;input&lt;/em&gt;, and $\y_i\in\P_i$ is the associated &lt;em&gt;target&lt;/em&gt; (or &lt;em&gt;label&lt;/em&gt;) for $\x_i$. (usually the inputs and targets are notated with $x$ and $y$ - but I am using Greek)&lt;/p&gt;
&lt;p&gt;We are given a partial observation $D$, called the dataset:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
D = (\x_1, \y_1, \x_2, \y_2, \x_3, \y_3, \ldots, \x_n, \y_n)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Given model $f : \O \to \X$ and dataset $D$ (regarding $D$ as a subset of $\X$), then $\inv{f}(D)$ is the set of all states in $\O$ compatible with $D$.&lt;/p&gt;
&lt;p&gt;If we are subsequently given a &amp;ldquo;test input&amp;rdquo; $\x_{n+1} \in \Xi_{n+1}$, we want to predict $\y_{n+1} \in \P_{n+1}$. We can do so if $\inv{f}(D`\x_{n+1})$ uniquely determines $\y_{n+1}$, i.e. $D`\x_{n+1}`\y_{n+1} = f(\inv{f}(D`\x_{n+1}))$ (i.e. $\inv{f}(D`\x_{n+1}) = \inv{f}(D`\x_{n+1}`\y_{n+1})$)&lt;/p&gt;
&lt;h3 id=&#34;boolean-logic&#34;&gt;Boolean logic&lt;/h3&gt;
&lt;p&gt;The ML model is $f : \O\to\X$ where $\O = \B^{64}$ is the set of all length-64 boolean vectors, corresponding to the values of all the boolean variables $P_{x,y},W_{x,y},B_{x,y},S_{x,y}$ for all 16 grid cells. Any state $\o\in\O$ is a math model in the sense we used above. The observations are what is given, what we wish to know, and potentially everything we can know in principle. We were given the follow propositions (meaning they are observed as true):&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
R_1 &amp;amp;= \neg P_{1,1} \\&lt;br&gt;
R_2 &amp;amp;= (B_{1,1} \Iff (P_{1,2}\or P_{2,1})) \\&lt;br&gt;
R_3 &amp;amp;= (B_{2,1} \Iff (P_{1,1}\or P_{2,2}\or P_{3,1})) \\&lt;br&gt;
R_4 &amp;amp;= \neg B_{1,1} \\&lt;br&gt;
R_5 &amp;amp;= B_{2,1}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We want to know many things: the location of the gold, the wumpus, and all the pits. Under more immediate consideration is whether there are pits in any of the cells [2,1], [2,2], [3,1]. So let&amp;rsquo;s say the propositions of immediate interest are&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
Q_1 &amp;amp;= P_{2,1} \\&lt;br&gt;
Q_2 &amp;amp;= P_{2,2} \\&lt;br&gt;
Q_3 &amp;amp;= P_{3,1} \\&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then the observation set is $\X = \B^8$, and a full observation is the boolean tuple $x = (R_1, R_2, R_3, R_4, R_5, Q_1, Q_2, Q_3)$. (Note that one ML-model&amp;rsquo;s full observation is another ML-model&amp;rsquo;s partial observation. If later on we care about propositions about other cells on the board, we can just augment $\X$ with additional dimensions, thereby updating $f$ to $f&#39;$) We are given the partial observation $x_{1:5} = (\1, \ldots, \1)$, i.e. $R_1=\1,\ \dots,\ R_5=\1$. Can we uniquely determine the remaining parts of $x$?&lt;/p&gt;
&lt;p&gt;The set of math models (subset of $\O$) consistent with $x_{1:5}$ being true is:&lt;br&gt;
$$&lt;br&gt;
\inv{f}(x_{1:5}) = \inv{f}(\set{(\1, \dots, \1)}\m \B^3)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Above we proved that $P_{2,1} = \1$, and so $x_6 = Q_1 = \1$ must be the case for all states in $\inv{f}(x_{1:5})$. However, $x_{7:8} = (Q_2, Q_3) = (P_{2,2}, P_{3,1})$ is not uniquely determined in $\inv{f}(x_{1:5})$.&lt;/p&gt;
&lt;p&gt;Note that&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
R_2 &amp;amp;= (B_{1,1} \Iff (P_{1,2}\or P_{2,1})) \\&lt;br&gt;
R_3 &amp;amp;= (B_{2,1} \Iff (P_{1,1}\or P_{2,2}\or P_{3,1}))&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;are the rules of the game, i.e. that a breeze must be adjacent to a pit. In the ML framework, including the rules as partial observations means that the ML model $f$ considers possible worlds where the rules of the game are different - in fact $f$ allows for all possible games played on a 4x4 grid with pits, breezes, etc. It is equivalent to bake the rules of the game into $f$ as a domain restriction on $\O$, i.e. define $f&#39;$ whose domain is all states which obey our rules $R_2, R_3$.&lt;/p&gt;
&lt;p&gt;In the math framework, a theorem is a proposition that is true in all math models. In the ML framework, a theorem is a boolean dimension of $\X$ (output to $f$) which is true for all states (inputs to $f$). For a given boolean output dimension of $f$, that dimension becomes a theorem of $f&#39;$, the domain restriction of $f$ to all inputs where the output dimension is true (if there are no such inputs then this boolean dimension corresponds a paradox, a proposition that is not true in any math model).&lt;/p&gt;
&lt;p&gt;The general case is $f : \B^a \to \B^b$ where $a,b$ may be finite or infinite cardinalities. Given a partial observation $x_{1:n}$, we can ask whether any dimensions of $x_{&amp;gt;n}$ are also uniquely determined. A proof of $x_i = \1$ is a way of showing that $x_i$ is uniquely determined from $x_{1:n}$ without brute force enumeration of all states (inputs to $f$). If we care about part $\o_{1:m}$ of the input to $f$ as well, then in the ML framework, that equates to passing $\o_{1:m}$ through $f$ to $x_{j_1, \dots, j_m}$ as the identity function, and then trying to uniquely determine that partial observation.&lt;/p&gt;
&lt;h2 id=&#34;the-bayesian-perspective&#34;&gt;The Bayesian perspective&lt;/h2&gt;
&lt;p&gt;Continuing with our ML model $f : \O \to \X$ for wumpus world, what should the agent do next? Using non-Bayesian logic, we could not uniquely determine if cells [3,1] or [2,2] contain pits or not. It then seems most reasonable to travel to cell [1,2] to gather more information. Let&amp;rsquo;s suppose we do that, and find [1,2] also contains a breeze (I&amp;rsquo;m disregarding what is depicted in figure 7.2 and making up my own scenario). Then any of cells [1,3], [2,2], and [3,1] may contain pits. There is nowhere else to go without traveling over one of these dangerous cells. What do we do?&lt;/p&gt;
&lt;p&gt;A Bayesian would suggest counting up and comparing the number of states for which each possible observation is true. Suppose we know there are exactly 3 pits on the board. We&amp;rsquo;ve ruled out [1,1], [1,2], [2,1], and so there are 13 remaining cells that contain the 3 pits.&lt;/p&gt;
&lt;p&gt;State of knowledge:&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210223212901.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210223212901.png&#34; width=&#34;200&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
Possible pit configurations of [3,1], [2,2] and [3,1] (1 = pit, 0 = no pit):&lt;br&gt;
&lt;span class=&#34;image-container&#34;&gt;&lt;span class=&#34;link&#34; &gt;&lt;a href=&#34;../../Pasted%20image%2020210223213359.png&#34; 
        target=&#34;_blank&#34;&gt;&lt;img class=&#34;img&#34; src=&#34;../../Pasted%20image%2020210223213359.png&#34; width=&#34;300&#34;/&gt;&lt;/a&gt;&lt;/span&gt;
&lt;/span&gt;&lt;br&gt;
There are 10 remaining cells not depicted. Any pits not in [3,1], [2,2] and [3,1] will be in the remaining 10. Let&amp;rsquo;s count up the total number of states corresponding to each configuration:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;[1,3]&lt;/th&gt;
&lt;th&gt;[2,2]&lt;/th&gt;
&lt;th&gt;[3,1]&lt;/th&gt;
&lt;th&gt;Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;${10 \choose 2} = 45$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;${10 \choose 1} = 10$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;${10 \choose 1} = 10$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;${10 \choose 1} = 10$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;${10 \choose 0} = 1$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The total count is 76 states. As fractions (called probabilities), we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(0,1,0) &amp;amp;= 45/76 \\&lt;br&gt;
p(1,0,1) &amp;amp;= 10/76 \\&lt;br&gt;
p(1,1,0) &amp;amp;= 10/76 \\&lt;br&gt;
p(0,1,1) &amp;amp;= 10/76 \\&lt;br&gt;
p(1,1,1) &amp;amp;= 1/76&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The fraction of states where the middle cell [2,2] has a pit is $66/76$. The fraction of states where [3,1] has a pit is $21/76$, and likewise for [1,3]. We say that [2,2] is more likely to have a pit than [1,3] or [3,1]. If the agent is forced to choose one of the three cells to move to, [2,2] is the most dangerous option (highest probability of falling into a pit) and [3,1],[3,1] are equally less-dangerous.&lt;/p&gt;
&lt;p&gt;The difference between the classical and Bayesian paradigms is now clear. A classical agent does not distinguish between these three options, since none of these cells can be proved to be pit-free (this property is not uniquely determined from the givens). The Bayesian agent doesn&amp;rsquo;t need unique determination to have knowledge about the pit-ness of these cells, and concludes that [2,2] is more likely to have a pit than [3,1] or [1,3].&lt;/p&gt;
&lt;h2 id=&#34;probability-notation&#34;&gt;Probability notation&lt;/h2&gt;
&lt;p&gt;$\newcommand{\obs}{\mathrm{Data}}$I&amp;rsquo;m now going to regard logical propositions as functions of state $\o$. For example, $R_1(\o)$ returns true if $\o$ is a math-model of proposition $R_1$, and false otherwise.&lt;/p&gt;
&lt;p&gt;Let $\obs = R_1\and\dots\and R_5$. Then $\obs(\o)$ is true iff $\o$ is a math-model of our givens $R_1$ through $R_5$. If I write $P_{x,y}$, take that now to be a function of $\o$ as well.&lt;/p&gt;
&lt;p&gt;Let $p$ be the uniform probability measure on $\O$. Using random variable notation (see &lt;a href=&#34;http://zhat.io/articles/primer-probability-theory#random-variables&#34;target=&#34;_blank&#34;&gt;zhat&lt;/a&gt; for probability theory and &lt;a href=&#34;https://www.lesswrong.com/posts/W8YscokXMiDnLKJ96/bayesian-inference-on-1st-order-logic&#34;target=&#34;_blank&#34;&gt;LW&lt;/a&gt; for conditional probability notation) the fraction of states where the middle cell [2,2] has a pit is $p(P_{2,2}=\1\mid\obs=\1) = 66/76$. The fraction of states where [3,1] and [1,3] have a pit respectively is $p(P_{1,3} = \1\mid\obs=\1) = p(P_{3,1} = \1\mid\obs=\1) = 21/76$.&lt;/p&gt;
&lt;p&gt;In general for finite state sets $\O$, any ML-model $f : \O\to\X$ can be regarded as a random variable (or a tuple of random variables), i.e. a function from states to observables. I always assume a uniform probability measure, which corresponds to naive counting like in the example above. Let $f_{a:b}$ denote the slice of the tuple-valued random variable $f$ from index $a$ to $b$. Generally we want to determine the probability of some (unobserved) partial observation $x_{n:m}$ given the (observed) partial observation $x^*_{1:n}$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp; p(f_{n:m} = x_{n:m} \mid f_{1:n} = x^*_{1:n}) \\&lt;br&gt;
&amp;amp;\quad= p\set{\o\in\O \mid f(\o) \in x^*_{1:n}`x_{n:m}} / p\set{\o\in\O \mid f(\o) \in x^*_{1:n}} \\&lt;br&gt;
&amp;amp;\quad= p(\inv{f}(x^*_{1:n}`x_{n:m})) / p(\inv{f}(x^*_{1:n})) \\&lt;br&gt;
&amp;amp;\quad= \abs{\inv{f}(x^*_{1:n}`x_{n:m})} / \abs{\inv{f}(x^*_{1:n})}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;In general, the classically rational agent only regards uniquely determined partial observables (output dimensions on it&amp;rsquo;s ML-model $f$) as knowledge, and does not act on undetermined partial observables. In contrast, the Bayesian rational agent takes the relative proportion of possible states that produce each outcome as knowledge.&lt;/p&gt;
&lt;p&gt;How does the Bayesian agent pull this off? Do these &amp;ldquo;Bayesian&amp;rdquo; probabilities really constitute knowledge? We can turn the question around and ask if uniquely determined observables really constitute knowledge. It is rare for something to be uniquely determined in practice. I suspect that many of the difficulties encountered in applications of statistical inference and machine learning are because of this. A classical reasoner needs to make simplifications and assumptions in service of being able to then uniquely determine something of interest. It seems to me that most of informal rational thought comes down to some version of choosing an ML-model s.t. something of interest can be uniquely determined. Described in this way, classical reasoning sounds biased. On the flip side, in practice Bayesian ML-models requires special simplifications and assumptions to be computationally tractable, so a similar sort of ML-model-choosing bias occurs.&lt;/p&gt;
&lt;p&gt;Note that the size of $\O$ and the ML-model $f$ determine how many states produce each partial observation. Ostensibly these things are arbitrarily chosen by the agent. The classical reasoner objects that state-counts (probabilities) don&amp;rsquo;t constitute actual knowledge about the world, but are an artifact of the choice of ML-model $f$, and so it is inappropriate to treat these quantities as knowledge. The Bayesian reasoner counters that a classical ML-model (e.g. boolean logic) is also arbitrarily chosen. Unique determination is a property of $f$, not reality, and depends on the arbitrary simplifications and assumptions made by the agent. Thus, the Bayesian reasoner concludes, my approach is no less rational than yours.&lt;/p&gt;
&lt;p&gt;The classical reasoner would counter that the parts of the ML-model output which are observed can be arbitrarily inspected for &amp;ldquo;goodness of fit&amp;rdquo; to reality. Unique determination is much less fragile (i.e. stable w.r.t. modeling inaccuracies) than state-counts. Unique determination is robust against worst-case modeling errors (though in practice this is clearly not true).&lt;/p&gt;
&lt;p&gt;Punchline: any representation of the world whose predictions are not uniquely determined by data on hand can be viewed as Bayesian, if we are willing to provide a measure for calculating sizes of sets of model states, and then to use size-of-possibilities as decision making criteria.&lt;/p&gt;
&lt;h1 id=&#34;the-bayesian-axiom&#34;&gt;The Bayesian Axiom&lt;/h1&gt;
&lt;p&gt;This epistemological debate is still raging. The efficacy of classical reasoning has been argued about for the last two and a half millennia. Bayesian reasoning is a more modern invention that, depending on how you count it, has been going on for 100 to 300 years (early 20th century Bayesians to Thomas Bayes). The frontier of contemporary inquery is the complex: from brains to human societies to high-dimensional physical systems, the neat and orderly unique-determination of classical reasoning is hard to come by. For this reason, Bayesian reasoning is gaining traction and is posturing to topple classical reasoning as the common-sense epistemological default.&lt;/p&gt;
&lt;p&gt;Given the unsettled nature of these questions, it is my opinion that the &amp;ldquo;state-counts as knowledge&amp;rdquo; premise be taken as the &amp;ldquo;Bayesian axiom&amp;rdquo;. This neatly delineates classical and Bayesian epistemology down to one difference: Bayesian epistemology is classical epistemology plus an additional axiom. Some may accept this axiom and other&amp;rsquo;s may reject it, leading to different kinds of reasoning and knowledge. I don&amp;rsquo;t know if we will ever be able to determine that this axiom should or should not be used. In that case, like &lt;a href=&#34;https://en.wikipedia.org/wiki/Parallel_postulate&#34;target=&#34;_blank&#34;&gt;Euclid&amp;rsquo;s fifth axiom&lt;/a&gt;, &amp;ldquo;flat&amp;rdquo; and &amp;ldquo;curved&amp;rdquo; rationality shall forever remain parallel self-consistent options.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variational Solomonoff Induction</title>
      <link>https://danabo.github.io/blog/posts/variational-solomonoff-induction/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/variational-solomonoff-induction/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\R}{\mb{R}}&lt;br&gt;
\newcommand{\kl}[2]{D_{KL}\left(#1\ \| \ #2\right)}&lt;br&gt;
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\ }&lt;br&gt;
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}}\ }&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\ve}{\varepsilon}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\o}{\omega}&lt;br&gt;
\newcommand{\O}{\Omega}&lt;br&gt;
\newcommand{\sm}{\mathrm{softmax}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Free_energy_principle&#34;target=&#34;_blank&#34;&gt;free energy principle&lt;/a&gt; is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Variational_Bayesian_methods&#34;target=&#34;_blank&#34;&gt;variational Bayesian method&lt;/a&gt; for approximating posteriors. Can free energy minimization combined with program synthesis methods from machine learning tractably approximate &lt;a href=&#34;https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference&#34;target=&#34;_blank&#34;&gt;Solomonoff induction&lt;/a&gt; (i.e. universal inference)? In these notes, I explore what the combination of these ideas looks like.&lt;/p&gt;
&lt;h1 id=&#34;machine-learning&#34;&gt;Machine learning&lt;/h1&gt;
&lt;p&gt;I want to make an important clarification about &amp;ldquo;Bayesian machine learning&amp;rdquo;. First, I&amp;rsquo;ll briefly define some &amp;ldquo;modes&amp;rdquo; of machine learning.&lt;/p&gt;
&lt;p&gt;In parametric machine learning, we have a function $f_\t$ parametrized by $\t\in\T$. Let $q_\t(D)$ be a probability distribution on datasets $D$ defined in terms of $f_\t$. For supervised learning, $q_\t(D) = \prod_{(x,y)\in D} Pr(y; f_\t(x))$ is the product of probabilities of each target $y$ given distribution parameters $f_\t(x)$, e.g. $f_\t(x)$ returns the mean and variance of a Gaussian over $y$. For unsupervised learning, $f_\t(x)$ might return a real number which serves as the log-probability of each $x \in D$. In general $f_\t$ can be any kind of parametric ML model, but these days it is likely to be a neural network.&lt;/p&gt;
&lt;p&gt;Typical usage &amp;ldquo;modes&amp;rdquo; of $q_\t$ in machine learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MLE&lt;/strong&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34;target=&#34;_blank&#34;&gt;maximum likelihood&lt;/a&gt;): Training produces hypothesis with highest data likelihood.
&lt;ul&gt;
&lt;li&gt;$\t\in\T$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$\t^* = \argmax{\t}\log q_\t(D)$ for dataset $D$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MAP&lt;/strong&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation&#34;target=&#34;_blank&#34;&gt;maximum a posteriori&lt;/a&gt;): Training produces model with highest posterior probability.
&lt;ul&gt;
&lt;li&gt;$\t\in\T$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$\t^* = \argmax{\t}\left[\log q_\t(D) + \log p(\t)\right]$ for dataset $D$ and prior $p(\t)$.&lt;/li&gt;
&lt;li&gt;$\log p(\t)$ can be viewed as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Regularization_%28mathematics%29&#34;target=&#34;_blank&#34;&gt;regularizer&lt;/a&gt;. MAP is just MLE plus regularization - the most typical form of parametric machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bayesian Inference&lt;/strong&gt;: Prior over the parameter induces a posterior over parameters given data.
&lt;ul&gt;
&lt;li&gt;$\t\in\T$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$p(\t \mid D) = \frac{q_\t(D) p(\t)}{\int_\T q_\t(D) p(\t) d\t}$ is the exact posterior on hypotheses $\T$ given dataset $D$.&lt;/li&gt;
&lt;li&gt;Unlike in MLE and MAP, there is no notion of optimal parameter $\t^*$. Instead we have much more information: $p(\t \mid D)$ &amp;ldquo;scores&amp;rdquo; every parameter in $\T$, and all the scores taken together constitute our information.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variational Bayes&lt;/strong&gt; (free energy minimization): training produces a (approximate) posterior distribution over hypotheses.
&lt;ul&gt;
&lt;li&gt;$z \in \mc{Z}$ is a hypothesis.&lt;/li&gt;
&lt;li&gt;$\tilde{\t} = \argmin{\t}\kl{q_\t(z)}{p(z \mid D)}$ is the target parameter for dataset $D$ and model $p(D,z)$. This is assumed to be intractable to find.&lt;/li&gt;
&lt;li&gt;$\t^* = \argmin{\t}\kl{q_\t(z)}{p(z)} - \E_{z\sim f_\t(z)}\left[\lg p(D \mid z)\right]$ is the approximation. This is what we find through optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The variational Bayes &amp;ldquo;usage mode&amp;rdquo; is clearly different from the others. MLE and MAP are fitting $f_\t$ to the data, i.e. finding a single $\t^*\in\T$ that maximizes the probability of the data under $q_\t$. Bayesian inference is finding a distribution $p(\t \mid D)$ on $\T$ which represents the model&amp;rsquo;s beliefs about various parameters $\t\in\T$ being likely or unlikely as explanations of the data. This is not the same as fitting $f_\t$ to data, since we are not choosing any particular parameter in $\T$. Variational Bayes uses $q_{\t^*}$ as an approximation of $p(\t \mid D)$, where $\t^*\in\T$ is the optimal parameter of distribution $q_\t(z)$ and $z\in\mc{Z}$ is a hypothesis.&lt;/p&gt;
&lt;p&gt;In the first three modes, $\T$ are hypotheses and we are either selecting one or finding a distribution over them. In the variational Bayes mode, $\T$ are not hypotheses. Instead we introduce $\mc{Z}$ as the hypothesis space and $\T$ is the parameter space for the approximate posterior $q_\t(z)$ on $\mc{Z}$, i.e. $q_\t(z)$ approximates $p(z\mid D)$. We don&amp;rsquo;t have $\mc{Z}$ in the first three modes, and we are interested in $p(\t \mid D)$ rather than $p(z \mid D)$. Also in the first three modes, $q_\t(D)$ is a distribution on what is observed, datasets $D$, rather than over latent $\mc{Z}$.&lt;/p&gt;
&lt;h2 id=&#34;what-is-bayesian-machine-learning&#34;&gt;What is Bayesian machine learning?&lt;/h2&gt;
&lt;p&gt;Conventionally, a Bayesian model has a prior probability distribution over it&amp;rsquo;s parameters, and inference involves finding posterior distributions. This corresponds to the Bayesian inference mode above. Out of the four modes, MLE is definitively non-Bayesian. MAP might be called semi-Bayesian, simply because there is a prior on parameters $p(\t)$, but only the argmax of the posterior is being found, rather than a full posterior. The variational Bayes mode is where things get wonky. There are two models: $q_\t(z)$ and $p(z, D)$. The first is parametrized and is optimized greedily with something like gradient descent, as in the MLE or MAP cases. The second is Bayesian.&lt;/p&gt;
&lt;p&gt;Is variational Bayes a Bayesian ML method? In one sense yes, in another sense no. It&amp;rsquo;s efficacy depends on $q_\t(z)$ being a good approximation of the posterior $p(z \mid D)$, and whether $q_\t(z)$ is a good approximation depends on the efficacy of the chosen machine learning method (e.g. neural networks trained with gradient descent). I&amp;rsquo;d expect $q_\t(z)$ to be a non-Bayesian model (If it were Bayesian, how then do you tractably approximate it? That is the very thing we are trying to do with $p(z, D)$.) So then the efficacy of variational Bayes comes down to the properties of non-Bayesian machine learning. If at the end of the day, point-estimates of parameters are always doing the heavy lifting (i.e. generalizing well), why be Bayesian in the first place?&lt;/p&gt;
&lt;h1 id=&#34;solomonoff-induction&#34;&gt;Solomonoff induction&lt;/h1&gt;
&lt;p&gt;I learned about this topic from &lt;a href=&#34;https://www.springer.com/gp/book/9781489984456&#34;target=&#34;_blank&#34;&gt;An Introduction to Kolmogorov Complexity and Its Applications&lt;/a&gt; and &lt;a href=&#34;http://www.hutter1.net/ai/uaibook.htm&#34;target=&#34;_blank&#34;&gt;Universal Artificial Intelligence&lt;/a&gt;. I recommend both books as references.&lt;/p&gt;
&lt;p&gt;There are different formulations of Solomonoff induction, each utilizing a hypothesis space containing all programs - but different kinds of programs for each formulation. I outline three of them: &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt;, &lt;a href=&#34;#version-2&#34;&gt;#Version 2&lt;/a&gt;, &lt;a href=&#34;#version-3&#34;&gt;#Version 3&lt;/a&gt;. Only an understanding of &lt;a href=&#34;#version-2&#34;&gt;#Version 2&lt;/a&gt; is needed for the subsequent sections.&lt;/p&gt;
&lt;h2 id=&#34;notation&#34;&gt;Notation&lt;/h2&gt;
&lt;p&gt;Let $\B = \{0,1\}$ be the binary alphabet, $\B^n$ be the set of all length-$n$ binary strings, and $\B^\infty$ be the set of all infinite binary strings.&lt;/p&gt;
&lt;p&gt;Let $\B^* = \B^0 \times \B^1 \times \B^2 \times \B^3 \times\ldots$ be the set of all finite binary strings of any length.&lt;br&gt;
$\epsilon$ is the empty string, i.e. $\B^0 = \{\epsilon\}$.&lt;/p&gt;
&lt;p&gt;Let $x \sqsubseteq y$ denote that binary string $x$ is a prefix of (or equal to) binary string $y$.&lt;br&gt;
Let $x`y$ denote the string concatenation of $x$ and $y$.&lt;br&gt;
Let $x_{a:b}$ denote the slice of $x$ from position $a$ to $b$ (inclusive).&lt;br&gt;
Let $x_{&amp;lt;n} = x_{1:n-1}$ denote the prefix of $x$ up to $n-1$.&lt;br&gt;
Let $x_{&amp;gt;n} = x_{n+1:\infty}$ denote the &amp;ldquo;tail&amp;rdquo; of $x$ starting from $n+1$.&lt;/p&gt;
&lt;h2 id=&#34;version-1&#34;&gt;Version 1&lt;/h2&gt;
&lt;p&gt;Let $U$ be a universal machine, i.e. if $z\in\B^*$ is a program then $U(z) \in \B^*$ is some binary string, or $U(z)$ is undefined because $U$ does not halt on $z$. We do not give program $z$ input, but $z$ can include &amp;ldquo;data&amp;rdquo;, in the sense that it&amp;rsquo;s program specifies a binary string that gets loaded into memory when the program starts.&lt;/p&gt;
&lt;p&gt;Let $p(z)$  be a prior on finite binary strings.&lt;br&gt;
Then for observation $x \in \B^*$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p(x \mid z) = \begin{cases}1 &amp;amp; x \sqsubseteq U(z) \\ 0 &amp;amp; \mathrm{otherwise}\end{cases}$&lt;/li&gt;
&lt;li&gt;$p(z, x) = p(x \mid z)p(z) = \begin{cases}p(z) &amp;amp; x \sqsubseteq U(z) \\ 0 &amp;amp; \mathrm{otherwise}\end{cases}$&lt;/li&gt;
&lt;li&gt;$p(x) = \sum_{z\in\B^*} p(x,z) = \sum_{z \in \B^*;\ x \sqsubseteq U(z)} p(z)$&lt;/li&gt;
&lt;li&gt;$p(z \mid x) = p(z, x)/p(x) = \begin{cases}p(z)/p(x) &amp;amp; x \sqsubseteq U(z) \\ 0 &amp;amp; \mathrm{otherwise}\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$p(x)$ is the data probability.&lt;br&gt;
$p(z)$ is the prior.&lt;br&gt;
$p(z\mid x)$ is the posterior.&lt;/p&gt;
&lt;p&gt;If we observe $x \in \B^*$, we may ask for the probability of subsequently observing some $y\in\B^*$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(y \mid x) &amp;amp;= \frac{1}{p(x)}\sum_{z\in\B^*}p(x`y,z) \\&lt;br&gt;
&amp;amp;= \frac{1}{p(x)}\sum_{z \in \B^*;\ x`y \sqsubseteq U(z)} p(z) \\&lt;br&gt;
&amp;amp;= \sum_{z \in \B^*;\ x`y \sqsubseteq U(z)} p(z \mid x)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;What prior $p(z)$ should we choose? Solomonoff recommends&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(z) = 2^{-\ell(z)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\ell(z)$ is the length of program $z$. This has the effect of putting more prior probability on short programs, essentially encoding a preference for &amp;ldquo;simple&amp;rdquo; explanations of the data. This prior also has the benefit that it is fast to calculate $p(z)$ for any $z$. In general, choice of prior is a matter of taste, and should depend on practical considerations like tractability and regularizations such as &amp;ldquo;simplicity&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;version-2&#34;&gt;Version 2&lt;/h2&gt;
&lt;p&gt;Let $\mu$ be a probability measure on $\B^\infty$, meaning $\mu$ maps (measurable) subsets of $\B^\infty$ to probabilities. $\mu$ can be specified by defining it&amp;rsquo;s value on the &lt;strong&gt;cylinder sets&lt;/strong&gt; $\Gamma_x = \set{\o \in \B^\infty \mid x \sqsubset \o}$ for every $x\in\B^*$, i.e. the set of all infinite binary strings starting with $x$. I&amp;rsquo;ll let $\mu(x)$ be a shorthand denoting $\mu(\Gamma_x)$. Then $\mu(x)$ is the probability of finite string $x$. For any such measure $\mu$, it must be the case that&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x) = \mu(x`0) + \mu(x`1)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x) \geq \mu(x`y)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $x,y\in\B^*$.&lt;/p&gt;
&lt;p&gt;$\mu$ is a &lt;strong&gt;semimeasure&lt;/strong&gt; iff it satisfies&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mu(x) \geq \mu(x`0) + \mu(x`1)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;for all $x \in \B^*$. That is to say, if $\mu$ is a semimeasure then probabilities may sum to less than one (this is like supposing that some probability goes missing).&lt;/p&gt;
&lt;p&gt;The following are equivalent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mu$ is &lt;strong&gt;computable&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;There exists some program which computes the probability $\mu(x)$ for all inputs $x$.&lt;/li&gt;
&lt;li&gt;There exists some program which outputs $x$ with probability $\mu(x)$ (for all $x$) when given uniform random input bits.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\mu$ is &lt;strong&gt;semicomputable&lt;/strong&gt; (a.k.a. enumerable) if there exists some program which approximates the probability $\mu(x)$ (for all $x$) by outputting a sequence of rational numbers $\set{\hat{p}_n}$ approaching $\mu(x)$, but where it is impossible to determine how close the sequence is to $\mu(x)$ at any point in time. That is to say, you cannot know the error sequence $\varepsilon_n = \abs{\mu(x) - \hat{p}_n}$, but you know that $\varepsilon_n \to 0$ as $n\to\infty$. In contrast, if $\mu$ is computable then there exists a program that outputs both the sequence of rationals $\set{\hat{p}_n}$ AND the errors $\set{\varepsilon_n}$ (computability implies there exists a program that takes a desired error $\varepsilon&amp;gt;0$ as input and outputs in finite time (i.e. halts) the corresponding approximation $\hat{p}$ s.t. $\varepsilon&amp;gt;\abs{\mu(x)-\hat{p}}$).&lt;/p&gt;
&lt;p&gt;Let $\mc{M}$ be the set of all semicomputable semimeasures on infinite binary sequences. Let $p(\mu)$ be a prior on $\mc{M}$.&lt;/p&gt;
&lt;p&gt;Then for observation $x \in \B^*$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p(x \mid \mu) = \mu(x)$&lt;/li&gt;
&lt;li&gt;$p(x, \mu) = p(x \mid \mu)p(\mu) = p(\mu)\mu(x)$&lt;/li&gt;
&lt;li&gt;$p(x) = \sum_{\mu \in \mc{M}} p(x \mid \mu) = \sum_{\mu \in \mc{M}} p(\mu)\mu(x)$&lt;/li&gt;
&lt;li&gt;$p(\mu \mid x) = p(\mu)\frac{\mu(x)}{p(x)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$p(x)$ is the data probability.&lt;br&gt;
$p(\mu)$ is the prior.&lt;br&gt;
$p(\mu\mid x)$ is the posterior.&lt;/p&gt;
&lt;p&gt;If we observe $x \in \B^*$, we may ask for the probability of subsequently observing some $y\in\B^*$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
p(y \mid x) &amp;amp;= \sum_{\mu \in \mc{M}} p(y,\mu\mid x) \\&lt;br&gt;
&amp;amp;= \sum_{\mu \in \mc{M}} p(\mu\mid x)\mu(y \mid x) \\&lt;br&gt;
&amp;amp;= \sum_{\mu \in \mc{M}}p(\mu)\frac{\mu(x)}{p(x)}\mu(y\mid x)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\mc{M}$ is all semicomputable semimeasures, rather than all computable measures, because the former is a decidable set while the latter is not, i.e. in practice the former set of hypotheses can be feasibly enumerated by enumerating all programs, while the latter cannot be. If we required only measures, then we could not decide which programs produced proper measures (if the program doesn&amp;rsquo;t halt on $x$, that is like saying the probability that would have gone to strings starting with $x$ &amp;ldquo;disappears&amp;rdquo;). Allowing non-halting programs means we don&amp;rsquo;t have to filter out programs which don&amp;rsquo;t halt. Similar issue for computable vs semicomputable.&lt;/p&gt;
&lt;p&gt;Versions 1 and 2 are equivalent. That is to say, we can get the same data distribution $p(x)$ for the right choice of prior in each version.&lt;/p&gt;
&lt;p&gt;A typical choice of the prior in this version is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(\mu) = 2^{-K(\mu)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $K(\mu)$ is the &lt;a href=&#34;https://www.math.wisc.edu/~jmiller/Notes/contrasting.pdf&#34;target=&#34;_blank&#34;&gt;&lt;strong&gt;prefix-free Kolmogorov complexity&lt;/strong&gt;&lt;/a&gt; of $\mu$, i.e. the length of the shortest program that (semi)computes $\mu$ (given a space of prefix-free programs, i.e. program strings contain their own length information).&lt;/p&gt;
&lt;h2 id=&#34;version-3&#34;&gt;Version 3&lt;/h2&gt;
&lt;p&gt;As you might guess, this will also turn out to be (sorta) equivalent to the first two versions. This is like version 2, except instead of considering a hypothesis to be a program that samples data sequences given uniform random input bits, a hypothesis is such a program packaged together with a particular infinite input sequence. Thus, hypotheses are in a sense infinite programs.&lt;/p&gt;
&lt;p&gt;Let $U$ be a &lt;strong&gt;universal monotone machine&lt;/strong&gt;. That means $U$ can execute infinitely long programs in a streaming fashion by producing partial outputs as the program is read. Let $z \in \B^\infty$. Then for every finite prefix $z_{1:n}$, we get a partial output $U(z_{1:n}) = \o_{1:m} \in \B^m$. We require that $U(z_{1:n}) \sqsubseteq U(z_{1:n&#39;})$ for $n \leq n&#39;$. The output of $z$ is infinite if $m\to\infty$ as $n\to\infty$, is finite if $m &amp;lt; \infty$ for all $n$, or is undefined if $U$ does not halt on any $z_{1:n}$.&lt;/p&gt;
&lt;p&gt;Let $\tilde{z}$ be a program that samples from $\mu$ from version 2, i.e. $\tilde{z}$ takes uniform random bits as input and outputs some $x_{1:m}$ with probability $\mu(x_{1:m})$. Then we can produce an infinite &amp;ldquo;version 3&amp;rdquo; program by appending an infinitely long uniform random sequence $u$, i.e. $z = \tilde{z}`u$ where $U(\tilde{z}) = \epsilon$ (empty string, i.e. executing $\tilde{z}$ outputs nothing), and $U(\tilde{z}`u_{1:n&#39;})$ outputs some prefix of $x$ (if we let $x$ be infinite) by running $\tilde{z}$ on input $u_{1:n&#39;}$.&lt;/p&gt;
&lt;p&gt;If we feed uniform random bits into $U$, then $U$ itself samples from a distribution over infinite data sequences. The induced distribution is $p(x_{1:m})$ which is a Solomonoff distribution that we can use for universal inference. This is equivalent to putting a uniform prior on the infinite programs $\B^\infty$, i.e.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(z_{1:n}) = 2^{-n}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;For partial observation $x_{1:m} \in \B^*$ (the remaining part of $x$ is the unobserved future),&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x_{1:m}) = \sum_{\zeta\in\Phi(x_{1:m})} 2^{-\ell(\zeta)}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\ell(\zeta)$ is the length of finite string $\zeta$.&lt;/li&gt;
&lt;li&gt;$\Phi(x_{1:m})$ is a prefix-free set of all finite sequences $\zeta$ which output at least $x_{1:m}$ when fed into $U$ (i.e. for all $z_{1:n} \in \B^*$ if $x_{1:m} \sqsubseteq U(z_{1:n})$ then $z_{1:n} \in \bigcup_{\zeta\in\Phi(x_{1:m})} \Gamma_\zeta$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To calculate $p(x_{1:m})$, we ostensibly want to sum up the prior probabilities of all programs which output at least $x_{1:m}$, but remember that our programs are infinitely long, and the prior probability of any infinite program is 0 (because $2^{-n}\to 0$ as $n\to\infty$). The sum above performs a &lt;a href=&#34;https://en.wikipedia.org/wiki/Lebesgue_integration&#34;target=&#34;_blank&#34;&gt;Lebesgue integral&lt;/a&gt; over the infinite programs $\B^\infty$ by dividing them into &amp;ldquo;intervals&amp;rdquo; (i.e. sets of programs sharing the same prefix - geometrically an interval if you consider an infinite binary sequence to be a real number between 0 and 1) and summing up the lengths (prior probabilities) of the intervals. The function $\Phi$ is a convenient construction for producing this set of intervals for us. Finding $\Phi(x_{1:m})$ is complex, and not especially important to go into.&lt;/p&gt;
&lt;p&gt;The joint distribution is&lt;/p&gt;
&lt;p&gt;$$p(x_{1:m}, z_{1:n}) = \sum_{\zeta\in\Phi(x);\ z_{1:n}\sqsubseteq\zeta} 2^{-\ell(\zeta)}\,.$$&lt;/p&gt;
&lt;p&gt;From here, we can straightforwardly compute the probability of the data under partial hypothesis $z_{1:n}$:&lt;/p&gt;
&lt;p&gt;$$p(x_{1:m} \mid z_{1:n}) =p(x_{1:m}, z_{1:n})/p(z_{1:n}) = \sum_{\zeta\in\Phi(x);\ z_{1:n}\sqsubseteq\zeta} 2^{n-\ell(\zeta)}\,.$$&lt;/p&gt;
&lt;p&gt;And finally the data posterior of the future slice $x_{m:s}$ given $x_{&amp;lt;m}$ (for $m&amp;lt;s$):&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x_{m:s}\mid x_{&amp;lt;m}) = \frac{1}{p(x_{&amp;lt;m})}\sum_{\zeta\in\Phi(x_{1:s})} 2^{-\ell(\zeta)}\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;h1 id=&#34;variational-solomonoff-induction&#34;&gt;Variational Solomonoff induction&lt;/h1&gt;
&lt;p&gt;Suppose we observe finite sequence $x_{1:t} \in \B^*$ and we want to find the posterior $p(h \mid x_{1:t})$. Usually this is intractable to calculate, and in the case of Solomonoff induction, the posterior is not even computable. We can get around this limitation by approximating the posterior with a parametrized distribution $q_\t(h)$ over hypotheses $h\in\mc{H}$. For now I will be agnostic as to what kind of hypothesis space $\mc{H}$ is, and it can be any of the hypothesis spaces discussed above: &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt;, &lt;a href=&#34;#version-2&#34;&gt;#Version 2&lt;/a&gt;, &lt;a href=&#34;#version-3&#34;&gt;#Version 3&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this case, let&amp;rsquo;s find $\t^*\in\T$ that minimizes the KL-divergence $\kl{q_\t(h)}{p(h\mid x_{1:t})}$ so that $q_\t(h)$ is as close as possible to $p(h\mid x_{1:t})$. Note that $q_\t(h)$ does not depend on $x_{1:t}$ because we find $\t^*$ after $x_{1:t}$ is already observed ($x_{1:t}$ is like a constant w.r.t. this optimization), whereas the joint distribution $p(h, x)$ is defined up front before any data is observed.&lt;/p&gt;
&lt;p&gt;However, if $p(h \mid x_{1:t})$ is intractable to calculate, then so is $\kl{q_\t(h)}{p(h\mid x_{1:t})}$. With a few tricks, we can find an alternative optimization target that is tractable. Rewriting the KL-divergence:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp;\kl{q_\t(h)}{p(h\mid x_{1:t})} \\&lt;br&gt;
&amp;amp;\quad= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h\mid x_{1:t})}\right)\right] \\&lt;br&gt;
&amp;amp;\quad= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h)}\right)-\lg p(x_{1:t} \mid h) + \lg p(x_{1:t})\right] \\&lt;br&gt;
&amp;amp;\quad= \kl{q_\t(h)}{p(h)} - \E_{h\sim q_\t}\left[\lg p(x_{1:t} \mid h)\right] - \lg \frac{1}{p(x_{1:t})} \\&lt;br&gt;
&amp;amp;\quad= \mc{F}[q_\t] - \lg \frac{1}{p(x_{1:t})} \,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\mc{F}[q_\t]$ is defined as&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q_\t] &amp;amp;= \kl{q_\t(h)}{p(h)} - \E_{h\sim q_\t}\left[\lg p(x_{1:t} \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h)}\right)-\lg p(x_{1:t} \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h,x_{1:t})}\right)\right]\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$\mc{F}[q_\t]$ is called the &lt;strong&gt;variational free energy&lt;/strong&gt;. It depends explicitly on choice of parameter $\t$, but also keep in mind it depends implicitly on the observation $x_{1:t}$ and distribution $p(h, x_{1:t})$.&lt;/p&gt;
&lt;p&gt;Because $\lg \frac{1}{p(x_{1:t})}$ (called the &lt;strong&gt;surprise&lt;/strong&gt; of $x_{1:t}$) is positive and constant (because observation $x_{1:t}$ is constant), then minimizing $\mc{F}[q_\t]$ to $\lg \frac{1}{p(x_{1:t})}$ guarantees that $\kl{q_\t(h)}{p(h\mid x_{1:t})}$ is 0 (KL-divergence cannot be negative), which in turn guarantees that $q_\t(h)$ and $p(h\mid x_{1:t})$ are equal distributions on $\mc{H}$. If our optimization process does not fully minimize $\mc{F}[q_\t]$, then $q_\t(h)$ will approximate $p(h\mid x_{1:t})$ with some amount of error.&lt;/p&gt;
&lt;p&gt;The optimization procedure we want to perform is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
&amp;amp;\argmin{\t\in\T} \mc{F}[q_\t] \\&lt;br&gt;
=\ &amp;amp; \argmin{\t\in\T} \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h,x_{1:t})}\right)\right] \\&lt;br&gt;
=\ &amp;amp; \argmax{\t\in\T}\set{ \E_{h \sim q_\t}\left[R(h)\right] + \mb{H}_{h \sim q_\t}[q_\t(h)]}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This now has the form of a one-timestep reinforcement learning objective, where $R(h) = \lg p(h,x_{1:t})$ is the reward for &amp;ldquo;action&amp;rdquo; $h$, and $\mb{H}_{h \sim q_\t}[q_\t(h)]$ is the entropy of $q_\t(h)$. Here $q_\t(h)$ is called the &lt;strong&gt;policy&lt;/strong&gt;, i.e. the distribution actions are sampled from. Maximizing this objective jointly maximizes expected reward under the policy and entropy of the policy. An entropy term is typically added to RL objectives as a regularizer to encourage exploration (higher entropy policy means more random actions), but in this case the entropy term comes included.&lt;/p&gt;
&lt;p&gt;We can use standard &lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html&#34;target=&#34;_blank&#34;&gt;policy gradient methods&lt;/a&gt; (e.g. &lt;a href=&#34;https://deepmind.com/research/publications/impala-scalable-distributed-deep-rl-importance-weighted-actor-learner-architectures&#34;target=&#34;_blank&#34;&gt;IMPALA&lt;/a&gt;) to maximize the above RL objective (equivalent to minimizing free energy), so long as $q_\t(h)$ is fast to sample from, and the reward $R(h) = \lg p(h,x_{1:t})$ is fast to compute. We can control both.&lt;/p&gt;
&lt;h1 id=&#34;tractability&#34;&gt;Tractability&lt;/h1&gt;
&lt;p&gt;Tractability depends on our choice of $\mc{H}$ and prior $p(h)$. What operations do we want to be tractable? Typically we want:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To approximate hypothesis posteriors: $q_{\t^*}(h) \approx p(h \mid x_{1:t})$.&lt;/li&gt;
&lt;li&gt;To approximate predictive data distributions (data posterior): $p(x_{&amp;gt;t} \mid x_{1:t})$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;hypothesis-posterior&#34;&gt;Hypothesis posterior&lt;/h2&gt;
&lt;p&gt;The approximation $q_{\t^*}(h)$ allows us to do this. The tractability of finding a good parameter $\t^*$ for $q$ using policy gradient methods will require that the reward $R(h) = \lg p(h,x_{1:t})$ is fast to calculate.&lt;/p&gt;
&lt;p&gt;We can write the reward as the sum of two terms:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\lg p(h,x_{1:t}) = \lg p(h) + \lg p(x_{1:t} \mid h)\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Then we need fast calculation of prior probabilities $p(h)$, and data probabilities under hypotheses, $p(x_{1:t} \mid h)$.&lt;/p&gt;
&lt;h2 id=&#34;data-posterior&#34;&gt;Data posterior&lt;/h2&gt;
&lt;p&gt;We want to approximate $p(y \mid x)$, i.e. the probability of observing string $y$ after observing $x$. This is similar to the problem of calculating $p(x)$, the data probability.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x) = \E_{h\sim p(h)} [p(x\mid h)]\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If it were fast to compute $p(x\mid h)$ for a given $h$, and fast to sample from the prior $p(h)$, then we can approximate the data probability with Monte Carlo sampling:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(x) \approx \hat{p}(x) = \sum_{h \in H^{(k)}} p(x\mid h)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $H^{(k)} = \set{h_1, h_2, \ldots, h_k} \sim p(h)$ is an i.i.d. sample from $p(h)$ of size $k$.&lt;/p&gt;
&lt;p&gt;In the same way, we can approximate the data posterior using the identity&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y \mid x) = \E_{h\sim p(h \mid x)} [p(y\mid x, h)]\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The Monte Carlo approximation is:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(y\mid x) \approx \hat{p}(y\mid x) = \sum_{h \in H^{(k)}} p(y\mid x, h)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $H^{(k)} = \set{h_1, h_2, \ldots, h_k} \sim q_{\t^*}(h)$ is an i.i.d. sample drawn from the optimized approximate posterior $q_{\t^*}(h)$. So approximating the data posterior requires approximating the hypothesis posterior.&lt;/p&gt;
&lt;p&gt;$p(y\mid x, h)$ is the conditional data distribution under hypothesis $h$. If $h$ is a probability measure, then $p(x \mid h) = h(x)$ and $p(y\mid x, h) = h(y\mid x)$.&lt;/p&gt;
&lt;p&gt;For approximating the data distribution, we need fast sampling from hypothesis priors $p(h)$ and fast data-under-hypothesis probabilities $p(x \mid h)$.&lt;/p&gt;
&lt;p&gt;For approximating the data posterior, we need fast approximate posteriors $q_{\t^*}(h)$, and we need hypothesis data-conditionalization $p(y\mid x, h)$ to be fast.&lt;/p&gt;
&lt;h2 id=&#34;generalization&#34;&gt;Generalization&lt;/h2&gt;
&lt;p&gt;Speed is necessary but not sufficient for tractability. The approximations we find need to be good ones. Choosing an appropriate model $q$, which is a distribution over programs, is within the realm of program synthesis and machine learning. These days, program synthesis is done with neural language models on code tokens.&lt;/p&gt;
&lt;p&gt;Can neural networks approximate the true posterior $p(h \mid x)$? This is a generalization problem. The optimized generative model on programs, $q_{\t^*}(h)$, will have been trained on finitely many programs. Whether $q_{\t^*}(h&#39;) \approx p(h&#39; \mid x)$ for some program $h&#39;$ unseen during training will depend entirely on the generalization properties of the particular program synthesizer that is used in $q_\t$.&lt;/p&gt;
&lt;p&gt;The difficulty of applying machine learning to program synthesis is dealing with reward sparsity and generalizing highly non-smooth functions. To maximize reward $R(h) = \lg p(h,x_{1:t})$, the model $q$ needs to upweight programs $h$ that jointly have a high prior $p(h)$ and high likelihood $p(x_{1:t} \mid h)$. If the prior $p(h)$ is simple, perhaps $q$ can learn that function. On the other hand, if this prior encodes information about how long $h$ runs for (as I discuss in the 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/variational-solomonoff-induction/#prior&#34;&gt;Variational Solomonoff Induction#prior&lt;/a&gt;
  


 section), the prior is then not even computable. Same for $p(x_{1:t} \mid h)$. Without actually running $h$ on $x_{1:t}$, determining the output will not be possible in general. For $q$ to predict these things based on $h$&amp;rsquo;s code but without running $h$ is in general impossible. The function $p(h \mid x_{1:t})$ (as a function of $h$) highly chaotic, and we cannot expect $q$ to generalize in any strong sense. Innovations in program synthesis are still needed to do even somewhat well.&lt;/p&gt;
&lt;p&gt;As a reinforcement learning problem, maximizing this reward suffers from sparsity issues. Most programs will be trivial, in the sense that they just output constant values, or nothing. I expect that Solomonoff induction doesn&amp;rsquo;t start to become effective until you get to programs of moderate length that exhibit interesting behavior. In the context of this reinforcement learning problem, that means the policy $q$ needs to find moderately long programs with moderately high reward. When training first starts, it can take an excessive amount of episodes before any non-trivial reward is discovered. This can make reinforcement learning intractable. Innovations are needed here too.&lt;/p&gt;
&lt;h1 id=&#34;choices&#34;&gt;Choices&lt;/h1&gt;
&lt;p&gt;To summarize the requirements we found above:&lt;br&gt;
Is there a choice of $\mc{H}$ and $p(x,h)$ s.t.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prior $p(h)$ is fast to calculate and sample from.&lt;/li&gt;
&lt;li&gt;Approximate hypothesis posterior $q_{\t^*}(h)$ is fast to sample from.&lt;/li&gt;
&lt;li&gt;Hypothesis data-probability $p(x\mid h)$ is fast to calculate.&lt;/li&gt;
&lt;li&gt;Hypothesis data-conditionalization $p(y\mid x, h)$ is fast to calculate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;program-space&#34;&gt;Program space&lt;/h2&gt;
&lt;p&gt;Hypotheses can be deterministic or stochastic. Deterministic hypotheses would be represented by deterministic programs. Stochastic hypotheses can either be represented by stochastic programs (output is non-deterministic) or by deterministic programs that output probabilities. I think we should choose the latter.&lt;/p&gt;
&lt;p&gt;If our hypotheses are deterministic, then we get Solomonoff induction &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt;. Conditionalization is easy because $p(y \mid x, h) = p(y`x \mid h) = 1$ if $h$ outputs $x$ and $0$ otherwise. However, the vast majority of programs will not output $x$, so the reward $R(h) = \lg p(h,x)$ will be very sparse. That is to say, the reward will be $-\infty$ most of the time (in practice you would clip and scale the reward to something reasonable). This is bad for policy gradient methods and will result in high gradient variance (learning will be extremely slow).&lt;/p&gt;
&lt;p&gt;We should use stochastic hypotheses. If we use non-deterministic programs, conditionalization is hard. Thus we should use programs that output their probabilities.&lt;/p&gt;
&lt;p&gt;The choice of $\mc{H}$ is equivalent to choosing a programming language plus syntax rules so that only valid programs can be constructed. In this case, we want to restrict ourselves to programs that will obey the properties of probability measures $\mu$ on infinite sequences: $\mu(x) = \mu(x`0) + \mu(x`1)$ and  $\mu(x) \geq \mu(x`y)$.&lt;/p&gt;
&lt;p&gt;To ensure this, I propose that programs $h$ take the form of auto-regressive language models. That is to say these programs read in a sequence of input tokens and for each token output a vector of real numbers with the same length as the token space. Passing that vector through a softmax induces a probability distribution over the next input token. The programs maintain their own internal state. A language should be chosen such that all generated programs can be guaranteed to take this form.&lt;/p&gt;
&lt;p&gt;If the program has output probabilities $\hat{p}_1, \ldots, \hat{p}_t$ for input $x_{1:t}$ but does not halt to produce $\hat{p}_{t+1}$, then the probability $\mu_h(x_{1:n})$ for $n&amp;gt;t$ is undefined, and the induced measure $\mu_h$ becomes a semimeasure.&lt;/p&gt;
&lt;h2 id=&#34;prior&#34;&gt;Prior&lt;/h2&gt;
&lt;p&gt;Weighting by program length suffices as a prior:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(h) = 2^{-\ell(h)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;One difficulty in working with programs is long-running execution. This can make computing data probabilities take a long time. One remedy is to down-weight long-running programs in the prior. &lt;a href=&#34;http://www.scholarpedia.org/article/Universal_search&#34;target=&#34;_blank&#34;&gt;Levin search&lt;/a&gt; is an alternative to Solomonoff induction where the prior is weighted solely by runtime. We can mix both kinds of priors.&lt;/p&gt;
&lt;p&gt;This is straightforward in Solomonoff induction &lt;a href=&#34;#version-1&#34;&gt;#Version 1&lt;/a&gt; where each program takes no input and outputs a deterministic string. Let $p(h) = 2^{-\ell(h)} / f(\tau(h))$ where $\tau(h)$ is the total runtime of $h$, and $f$ is an increasing function that goes to infinity. For example, if $f(t) = 2^{t}$, then we have prior $2^{-\ell(h)-\tau(h)}$. If you wanted to compute $p(x)$ within some precision $\ve &amp;gt; 0$, you can enumerate all programs $h\in\mc{H}$ by length and run them in parallel (called dovetailing). For each program, stop execution when $2^{-\ell(h)-\tau(h)} &amp;lt; \ve$. Shorter programs will be given more runtime over longer programs. (Thank you Lance Roy for the helpful discussion about this.)&lt;/p&gt;
&lt;p&gt;However, if we are using the programs I previously suggested that output data probabilities, then these programs may be fast on some inputs and slow on others. I don&amp;rsquo;t have a solution, but a reasonable suggestion is to do some kind of heuristic analysis of the programs on some sample inputs and assign a slowness penalty in the prior.&lt;/p&gt;
&lt;h1 id=&#34;lifelong-learning&#34;&gt;Lifelong learning&lt;/h1&gt;
&lt;p&gt;Solomonoff induction is a framework for general-purpose life-long learning, which is a paradigm where an intelligent agent learns to predict it&amp;rsquo;s future (or gain reward) from only one continuous data stream. The agent must learn on-line, and there are no independence assumptions (the data is a timeseries).&lt;/p&gt;
&lt;p&gt;In the variational setup outlined above, we converted the problem of Bayesian inference into a reinforcement learning problem. At time $t$, data $x_{1:t}$ is observed, and a policy $q_{\t^*}$ on programs is trained using policy gradient methods. However, every $t$ requires its own $q_{\t_t^*}$, thus we would need to perform RL training at every timestep. One way to speed this up is to reuse policies from previous timesteps. That is to say, at time $t+1$ perform $\argmax{\t\in\T}\set{ \E_{h \sim q_\t}\left[R(h)\right] + \mb{H}_{h \sim q_\t}[q_\t(h)]}$ using Monte Carlo gradient descent starting from the previous parameter $\t_t^*$. This can be considered fine-tuning. However, this may fail to work if the posterior changes drastically between timesteps.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Active inference tutorial (actions)</title>
      <link>https://danabo.github.io/blog/posts/active-inference-tutorial-actions/</link>
      <pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/active-inference-tutorial-actions/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\kl}[2]{D_{KL}\left(#1\ \| \ #2\right)}&lt;br&gt;
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\ }&lt;br&gt;
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}}\ }&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\atup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
\newcommand{\r}{\rho}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Previous attempts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/&#34;&gt;Free Energy Principle 1st Pass&lt;/a&gt;
  


, I used a &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt; to try to understand the free energy formalism. I figured out the &amp;ldquo;timeless&amp;rdquo; and actionless case, but I became confused when actions and time were added.&lt;/li&gt;
&lt;li&gt;In 





  
    &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;Free energy principle and Solomonoff induction&lt;/a&gt;
  


, I tried to translate between the formalism presented in &lt;a href=&#34;https://danijar.com/apd/&#34;&gt;https://danijar.com/apd/&lt;/a&gt; (which is a deep learning collaboration between  Danijar Hafner and Karl Friston) and the tutorial &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt;. I also tried to make the connection to Solomonoff induction.&lt;/li&gt;
&lt;li&gt;In 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/variational-solomonoff-induction/&#34;&gt;Variational Solomonoff Induction&lt;/a&gt;
  


, I thought about whether free energy (as variational inference) could be applied to deep program synthesis to approximate Solomonoff induction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the same tutorial as before, &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;, I will go through the free energy formalism again and try to work out time and actions.&lt;/p&gt;
&lt;h1 id=&#34;review&#34;&gt;Review&lt;/h1&gt;
&lt;p&gt;To recap what I figured out in 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/&#34;&gt;Free Energy Principle 1st Pass&lt;/a&gt;
  


:&lt;/p&gt;
&lt;p&gt;Suppose $o\in\mc{O}$ is the observation space and $s\in\mc{S}$ is the hypothesis/state space (to use the notation of the &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt;). For now, let&amp;rsquo;s assume that $\mc{S}$ is the hidden state space of the environment in some timestep. $p(o,s)$ is the agent&amp;rsquo;s model of the environment, relating observations to hidden states, and the prior probability of the environment being in any particular hidden state. Let&amp;rsquo;s also ignore questions about the meaning of these probabilities (objective or subjective) and where they come from. If it&amp;rsquo;s easier to think about, assume these probabilities are subjective.&lt;/p&gt;
&lt;p&gt;If $o$ is observed (one timestep only), then we want to calculate the posterior probability $p(s\mid o)$ of each $s\in\mc{S}$. If this is intractable to do, we can instead find an approximation $q_o(s)$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
q_o := \argmin{q \in \mc{Q}} \kl{q(s)}{p(s\mid o)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\mc{Q}$ is some space of distributions that you choose $q_o$ from. Presumably $\mc{Q}$ is restricted somehow, otherwise the solution is $q=p$ in which case you are doing exact Bayesian inference.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\kl{q(s)}{p(s\mid o)} &amp;amp;= \E_{s\sim q}\left[\lg\frac{q(s)}{p(s\mid o)}\right] \\&lt;br&gt;
&amp;amp;= \E_{s\sim q}\left[\lg\frac{q(s)}{p(s)}\right] + \E_{s\sim q}\left[\lg \frac{1}{p(o \mid s)}\right] + \E_{s\sim q}[p(o)]\\&lt;br&gt;
&amp;amp;= \kl{q(s)}{p(s)} - \E_{s\sim q}\left[\lg p(o \mid s)\right] - \lg\frac{1}{p(o)} \\&lt;br&gt;
&amp;amp;= \mc{F}[q] - \lg\frac{1}{p(o)}\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= \kl{q(s)}{p(s)} - \E_{s\sim q}[\lg p(o \mid s)]\\&lt;br&gt;
&amp;amp;= \E_{s\sim q}\left[\lg\frac{q(s)}{p(s,o)}\right]&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is called &lt;strong&gt;variational free energy&lt;/strong&gt;. $\kl{q(s)}{p(s)}$ is called &lt;strong&gt;accuracy&lt;/strong&gt; and $\E_{s\sim q}[\lg p(o \mid s)]$ is called &lt;strong&gt;complexity&lt;/strong&gt;. $\lg\frac{1}{p(o)}$ is called &lt;strong&gt;surprise&lt;/strong&gt; (self-information of the observation $o$).&lt;/p&gt;
&lt;p&gt;Minimizing $\mc{F}[q]$ w.r.t. $q$ minimizes $\kl{q(s)}{p(s\mid o)}$. The surprise $\lg\frac{1}{p(o)}$ is constant w.r.t. this minimization. (Remember this is all assuming $o$ is given and fixed.)&lt;/p&gt;
&lt;h1 id=&#34;new-stuff&#34;&gt;New stuff&lt;/h1&gt;
&lt;p&gt;In the &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt;, we have free energy defined just as in my notes, except that everything now depends on policy $\pi$:&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210207121048.png&#34; alt=&#34;&#34;&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210207121055.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
The policy $\pi$ is a probability distribution over actions (, e.g. $\pi(a_t \mid o_{1:t}, a_{1:t-1})$. More on that later.&lt;/p&gt;
&lt;h2 id=&#34;interaction-loop&#34;&gt;Interaction loop&lt;/h2&gt;
&lt;p&gt;It is not clear to me whether $o,s$ are sequences over time, or just one time-step. In 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/variational-solomonoff-induction/&#34;&gt;Variational Solomonoff Induction&lt;/a&gt;
  


 I showed how to interpret $s$ as a hypothesis that explains an infinite sequence of observations, i.e. $s$ is not a sequence but $o$ is. When I write $o_{1:\infty}$, that is an observation sequence over time. When I write $s_{1:\infty}$ that is a state sequence over time. I&amp;rsquo;ll use $h$ later to denote a time-less hypothesis on observation sequences $o_{1:\infty}$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s unpack the agent-environment interaction loop. Given policy $\pi$,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(o_{1:\infty},s_{1:\infty}\mid\pi) = \sum_{a_{1:\infty}}\prod_t p(o_t,s_t\mid a_t,s_{t-1})\pi(a_t\mid o_{1:t-1},a_{1:t-1})&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $p(o_t,s_t\mid a_t,s_{t-1})$ is the probability of next observation and hidden state given input action $a_t$ and previous state, and $\pi(a_t\mid o_{1:t-1},a_{1:t-1})$ is the probability of the agent taking action $a_t$ given its entire history $o_{1:t-1},a_{1:t-1}$ (alternative we can give the agent internal state and condition on that).&lt;/p&gt;
&lt;p&gt;On the other hand, if $s$ (or $h$) is a time-less hypothesis: then we have&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(o_{1:\infty},h\mid\pi) = \sum_{a_{1:\infty}}\prod_t p(o_t\mid a_{1:t},o_{1:t-1},h)p(h)\pi(a_t\mid o_{1:t-1},a_{1:t-1})&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $p(h)$ is the prior on hypothesis $h$.&lt;/p&gt;
&lt;p&gt;Below, I will leave it ambiguous whether $s$ is a sequence of states or a time-less hypothesis. The math should be the same either way.&lt;/p&gt;
&lt;h2 id=&#34;active-inference&#34;&gt;Active inference&lt;/h2&gt;
&lt;p&gt;How are actions chosen? This is the big question I could not penetrate in my previous attempts. From the tutorial:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When inferring optimal actions, however, one cannot simply consider current observations. This is because actions are chosen to bring about preferred future observations. This means that, to infer optimal actions, a model must predict sequences of future states and observations for each possible policy, and then calculate the expected free energy (ð¸ð¹ð¸) of those different sequences of future states and observations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is talking about taking an expectation over future states and observations. Let&amp;rsquo;s assume $p(o_{1:\infty}, s_{1:\infty} \mid \pi)$ is the true environment dynamics. We are introduced to a new term $q(o_{1:\infty}\mid\pi)$ which are the agent&amp;rsquo;s observation preferences over time given a particular policy.&lt;/p&gt;
&lt;p&gt;The text is saying we want to choose policy $\pi$ to maximize $G(\pi)$. What&amp;rsquo;s troubling is that there is another term, $p(\pi)$, a prior over policies. If we are choosing policies freely, what does this prior represent?&lt;/p&gt;
&lt;p&gt;The text says that the preferred policy also minimizes free energy:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Since âpreferredâ here formally translates to âexpected by the modelâ, then the policy expected to produce preferred observations will be the one that maximizes the accuracy of the model (and hence minimizes ð¸ð¹ð¸).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;exact-inference&#34;&gt;Exact inference&lt;/h2&gt;
&lt;p&gt;To simplify things, let&amp;rsquo;s suppose the agent can do perfect Bayesian inference, so that $q_o(s\mid\pi) = p(s \mid o,\pi)$. Let&amp;rsquo;s see what happens if we plug in $p(s\mid o,\pi)$ for $q_o(s\mid \pi)$ in our free energy definition:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\mc{F} = \E_{s \sim p(s\mid o,\pi)}\left[\lg \frac{p(s\mid o,\pi)}{p(s,o\mid\pi)}\right] = \E_{s \sim p(s\mid o,\pi)}\left[\lg \frac{1}{p(o\mid \pi)}\right] = \lg \frac{1}{p(o\mid \pi)}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is just the surprise (i.e. self-information due to observing $o$). Minimizing free energy means choosing $\pi$ to maximize the data likelihood:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\pi^* := \argmax{\pi} p(o\mid\pi)&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Remember that $\mc{F}$ depends on a fixed $o$, which is what has already been observed. If $o$ is not observed, then we are talking about future $o$, and we need to take an expectation w.r.t. $o$, e.g.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\pi^* := \argmin{\pi} \E_{o\sim p(o\mid\pi)}\lg\frac{1}{p(o\mid\pi)} = \argmin{\pi}\mb{H}[p(o\mid\pi)]&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This is saying, choose a policy s.t. the future is as predictable as possible, i.e. minimizes entropy over observations, i.e. minimizes future expected surprise.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s introduce the agent&amp;rsquo;s preferences, encoded as a distribution on observations. The tutorial (and other texts) use $q(o)$, but I&amp;rsquo;m using $\r(o)$, because this is a very different thing from the approximate posterior $q_o$. Specifically $\r(o)$ is given and held fixed, while $q_o(s)$ depends on the particular observation $o$, as well as choice of optimization space $\mc{Q}$. In short, $q_o(s)$ is an output, while $\rho(o)$ is given.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s replace $p(o\mid\pi)$ with $\rho(o)$ (this should not depend on $\pi$). So instead of taking an expectation w.r.t. the model probabilities for $o$, we are taking an average weighted by preference for $o$:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\pi^* &amp;amp;:= \argmin{\pi} \E_{o\sim \r(o)}\lg\frac{1}{p(o\mid\pi)} \\&amp;amp;= \argmin{\pi} H(\r(o), p(o\mid\pi)) \\&amp;amp;= \argmin{\pi} \left\{\kl{\r(o)}{p(o\mid\pi)} + \mb{H}[\r(o)]\right\}&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross_entropy&#34;target=&#34;_blank&#34;&gt;cross-entropy&lt;/a&gt; of $q(o)$ and $p(o\mid\pi)$ (average number of bits if you encode a stream $o_{1:\infty}$ under $p$ while actually sampling from $p$). Since $\r(o)$ is fixed, then we are minimizing $\kl{\r(o)}{p(o\mid\pi)}$. That is to say, choose policy (thereby choosing actions) that make the environment (as the agent believes it to be) dynamics $p(o\mid\pi)$ conform to preferences $\r(o)$.&lt;/p&gt;
&lt;h3 id=&#34;reward-equivalence&#34;&gt;Reward equivalence&lt;/h3&gt;
&lt;p&gt;According to  &lt;a href=&#34;https://danijar.com/apd/,&#34;&gt;https://danijar.com/apd/,&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\r(o_{1:n}) \propto \exp(r(o_{1:n}))&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $r(o_{1:n})$ is the total reward received for observations $o_{1:n}$. Written another way, $\r(o_{1:n}) = \exp(r(o_{1:n}))/\mc{Z}$ for some normalization constant $\mc{Z}$, so then $r(o_{1:n}) = \ln\r(o_{1:n}) + \mc{C}$ for some constant offset $\mc{C}$.&lt;/p&gt;
&lt;p&gt;If we are just trying to maximize expected total reward $r(o_{1:n})$ w.r.t. the environment model, we get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\pi^* &amp;amp;:= \argmax{\pi} \E_{o \sim p(o \mid \pi)} [r(o_{1:n})] \\&lt;br&gt;
&amp;amp;= \argmax{\pi} \E_{o_{1:n},a_{1:n} \sim p(o_{1:n},a_{1:n} \mid \pi)} [\ln\r(o_{1:n})]\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;So far, I am not seeing anything conceptually new here. Storing agent preferences in a probability distribution $\r(o)$ is not really any different from storing preferences in a reward $r(o)$, and the two are easily converted into each other.&lt;/p&gt;
&lt;h2 id=&#34;putting-it-all-together&#34;&gt;Putting it all together&lt;/h2&gt;
&lt;p&gt;Now let&amp;rsquo;s suppose that $o$ has not yet been observed as before, but use approximate (future) posterior $q_{o,\pi}(s)$ and compute expected future free energy under preference $\rho(o)$.&lt;/p&gt;
&lt;p&gt;We get&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
G[\pi]&amp;amp;=\E_\rho[\mc{F}[o,\pi]] \\&lt;br&gt;
&amp;amp;= \E_{o\sim\rho}\E_{s \sim q_{o,\pi}}\left[\lg\frac{q_{o,\pi}(s)}{p(s,o\mid\pi)}\right] \\&lt;br&gt;
&amp;amp;= \E_{o\sim\rho}\kl{q_{o,\pi}(s)}{p(s\mid\pi)} - \E_{o\sim\rho}\E_{s\sim q_{o,\pi}}\left[\lg p(o \mid s,\pi)\right]&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $q_{o,\pi}$ is the optimal approximate posterior for the given observation $o$ and policy $\pi$ used to obtain $o$. From the perspective of $q_{o,\pi}$, $o$ is already observed using policy $\pi$ which determines the probability of that observation.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
q_{o,\pi} := \argmin{q} \mc{F}[o,\pi] = \argmin{q}\E_{s \sim q}\left[\lg\frac{q(s)}{p(s,o\mid\pi)}\right]&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;I believe the tutorial paper has a typo, where $p(o,s,\pi)$ should be $p(o,s,\mid\pi)$.&lt;/p&gt;
&lt;p&gt;We are choosing $\pi$ to minimize $G[\pi]$, which is just the expected free energy under $\rho(o)$ (preference for future observations).&lt;/p&gt;
&lt;p&gt;Do the optimizations on $\pi$ and $q$ interact? It seems like they don&amp;rsquo;t. $\pi$ is an outer optimization that depends on running the optimization on $q$ internally. There is not a single $q$, but many of them which the optimization on $\pi$ iterates through. So then what is the significance of connecting free energy minimization ($q$) to active inference ($\pi$)? If the policy optimization part were reformulated in terms of RL, we really just have a fancy kind of approximate Bayesian model combined with RL. The action learning and model updating are totally independent.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://arxiv.org/abs/1911.10601&#34;&gt;https://arxiv.org/abs/1911.10601&lt;/a&gt; for a discussion about the equivalence between variational approaches to RL and active inference.&lt;/p&gt;
&lt;h1 id=&#34;the-meaning-of-p&#34;&gt;The meaning of $p$&lt;/h1&gt;
&lt;p&gt;If $p(o_{1:\infty},s_{1:\infty}\mid\pi)$ is a dynamics model of the environment, how would the agent know it? Or alternatively, how are different hypotheses for environment dynamics handled in this framework?&lt;/p&gt;
&lt;p&gt;The two cases are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$p(o_{1:\infty},s_{1:\infty}\mid\pi)$ is the literal true dynamics of the environment.&lt;/li&gt;
&lt;li&gt;$p(o_{1:\infty},s_{1:\infty}\mid\pi)$ is the agent&amp;rsquo;s dynamics model of the environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first case is unreasonable, because we cannot assume any agent knows the truth. The second case does not allow the agent to update its ontology, i.e. change the state space $\mc{S}$ and it&amp;rsquo;s beliefs about how observations interact with, $p(o\mid s)$ and $p(s\mid o)$.&lt;/p&gt;
&lt;p&gt;We could suppose there is a latent $h$ for the environment hypothesis which is being marginlized, e.g. $p(o_{1:n},s_{1:n},h)$, but then $p(o_{1:n}\mid s_{1:n}) = \E_{h\sim p(h)}p(o_{1:n}\mid s_{1:n},h)$, which we can generally expect to be intractable but is required for free energy calculation. The free energy approximation was supposed to be tractable. Now do we have to approximate the approximation?&lt;/p&gt;
&lt;p&gt;I think the time-less hypothesis formulation is better, i.e. $p(o_{1:\infty}, h)$, because it allows the hypothesis to invent its own states (because states are no longer explicitly defined), and put emphasis on not just the present, but possible states in the past and future, i.e. the agent may be more interested in inferring past or future states. Furthermore, states may not be well defined things. I have a model of the world filled with all sorts of objects, each having independent states until they interact. I cannot comprehend thinking of everything I know as one gigantic state.&lt;/p&gt;
&lt;p&gt;Something I&amp;rsquo;ve heard hinted at elsewhere is that the agent, as a physical system, expresses some Bayesian prior $p$ and preferences $\r$ in an objective sense. What is the nature of this mapping between physical makeup and active-inference description? Is this entirely based on the agent&amp;rsquo;s behavior, or if we looked inside an agent, we could determine its model and preferences? I expect that if we look at behavior alone, then $p$ and $\r$ are underspecified.&lt;/p&gt;
&lt;p&gt;So then what about the agent&amp;rsquo;s physical makeup gives it a model $p$ and preferences $\r$? The optimization process to find $q_{o,\pi}$ must be physically carried out, and so presumably this could be observed. In optimizing for $q_{o,\pi}$, the agent would actually be engaged in two processes that implicitly specify $p$. Splitting free energy into accuracy and complexity:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Explaining observations: $\E_{h\sim q}\left[\lg p(o_{1:n} \mid h,a_{1:n})\right]$&lt;br&gt;
The agent thinks of hypotheses $h$ (sampling them from $q$) to explain observations $o_{1:n}$ given actions $a_{1:n}$.&lt;/li&gt;
&lt;li&gt;Regularization: $\kl{q(h)}{p(h)}$&lt;br&gt;
The agent updates its hypothesis generator $q$, implicitly conforming to $p$ which represents the agent&amp;rsquo;s grand total representation capacity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under this perspective, the agent&amp;rsquo;s ability to modify its own hypothesis generator $q(h)$ is somehow described by $p(h)$, which is fixed throughout the agent&amp;rsquo;s lifetime (unless the agent can self-modify). For a particular hypothesis $h$, the data probability $p(o_{1:n}\mid h)$ is the likelihood of the data under $h$. So $p$ is simultaneously encoding the agent&amp;rsquo;s theoretical capacity to generate hypotheses (which it never fully reaches because of limitations on $q(h)$) and the meaning of every hypothesis it can come up with. It is unclear to me whether $p(h, o_{1:n})$ can be uniquely determined given an agent&amp;rsquo;s physical makeup.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m also unconvinced about the way behavior is handled in this framework. Why think in terms of policies $\pi$ rather than actions $a_{1:n}$? Is the space of policies fixed through the agent&amp;rsquo;s lifetime? If $\pi$ is supposed to represent some kind of high level strategy, then how does the agent learn different kinds of strategies (updating its ontology). This is the same problem that Bayesian inference faces, that $q$ ostensibly fixes. But now we need to fix the same problem again for $\pi$.&lt;/p&gt;
&lt;p&gt;Question: $G[\pi]$ appears to be intractable to compute or optimize directly. Why do we not have a variational approximation to this as well?&lt;/p&gt;
&lt;p&gt;Why not just do RL? What is gained by &amp;ldquo;active inference&amp;rdquo;, which seems to me to be secretly RL on top of variational Bayes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Free Energy Principle 1st Pass</title>
      <link>https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/</link>
      <pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/free-energy-principle-1st-pass/</guid>
      <description>&lt;p&gt;$$&lt;br&gt;
\newcommand{\mb}{\mathbb}&lt;br&gt;
\newcommand{\mc}{\mathcal}&lt;br&gt;
\newcommand{\E}{\mb{E}}&lt;br&gt;
\newcommand{\B}{\mb{B}}&lt;br&gt;
\newcommand{\kl}[2]{D_{KL}\left(#1\ \| \ #2\right)}&lt;br&gt;
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\ }&lt;br&gt;
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}&lt;br&gt;
\newcommand{\atup}[1]{\left\langle#1\right\rangle}&lt;br&gt;
\newcommand{\set}[1]{\left\{#1\right\}}&lt;br&gt;
\newcommand{\t}{\theta}&lt;br&gt;
\newcommand{\T}{\Theta}&lt;br&gt;
\newcommand{\p}{\phi}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;Related notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;





  
    &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;The free-energy principle a unified brain theory&lt;/a&gt;
  


&lt;/li&gt;
&lt;li&gt;





  
    &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;Friston&amp;#39;s Free Energy (Active Inference)&lt;/a&gt;
  


&lt;/li&gt;
&lt;li&gt;Hackmd version of this note: &lt;a href=&#34;https://hackmd.io/@z5RLVXyrTg-JLCnL9c_xOQ/r1KSFUjkO&#34;&gt;https://hackmd.io/@z5RLVXyrTg-JLCnL9c_xOQ/r1KSFUjkO&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;my-current-understanding&#34;&gt;My current understanding&lt;/h1&gt;
&lt;h2 id=&#34;note-on-probability-notation&#34;&gt;Note on probability notation&lt;/h2&gt;
&lt;p&gt;These are my informal notes. Probability notation can be cumbersome and overly verbose. As is customary in machine learning and many of the sciences, I&amp;rsquo;m not going to bother using probability notation correctly. That is to say, I&amp;rsquo;m going to mangle and confuse probability measures, random variables, and probability mass/density functions. Hopefully readers can figure out the intended meaning of my notation from context, and I try to clarify when needed.&lt;/p&gt;
&lt;p&gt;In general, figuring out good notational conventions for probability in many fields (I&amp;rsquo;m looking at you machine learning, but the neuroscience free energy literature also has this problem) seems to be an unsolved problem, and one that I&amp;rsquo;d like to work on at some point! However that is not my concern here. I&amp;rsquo;m just taking the easiest route to expressing my knowledge. If you want to know what &amp;ldquo;proper&amp;rdquo; probability notation looks like, check out &lt;a href=&#34;http://zhat.io/articles/primer-probability-theory&#34;target=&#34;_blank&#34;&gt;my post on probability theory&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;bayesian-basics&#34;&gt;Bayesian basics&lt;/h2&gt;
&lt;p&gt;Suppose $\mc{H}$ is a set of possible hidden states, and $\mc{X}$ is a set of possible observations. Each $h\in\mc{H}$ &lt;strong&gt;explains&lt;/strong&gt; data $x\in\mc{X}$ through the &lt;strong&gt;conditional data distribution&lt;/strong&gt; $p_{X \mid H}(x \mid h)$, or also notated $p_{X \mid H=h}(x)$. A &lt;strong&gt;Bayesian mixture&lt;/strong&gt; is the mixture distribution&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p_{X,H}(x,h) = p_H(h)\cdot p_{X \mid H}(x \mid h)\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $p_H(h)$ is called the &lt;strong&gt;prior&lt;/strong&gt;. We can then compute the &lt;strong&gt;posterior&lt;/strong&gt; $p_{H \mid X}(h\mid x)$. The marginal distribution $p_X$ is called the &lt;strong&gt;subjective data distribution&lt;/strong&gt;, since it is partly determined by the choice of prior (if we assume the prior is subjective, i.e. quantifies personal belief). $p_X$ is an agent&amp;rsquo;s best prediction for what they will observe given what they believe (the prior).&lt;/p&gt;
&lt;h2 id=&#34;the-philosophy-of-bayesian-information&#34;&gt;The philosophy of Bayesian information&lt;/h2&gt;
&lt;p&gt;There are different ways to interpret quantity of information information. In the Bayesian setting, I like to think about the amount by which a possibility space was narrowed down. A probability $p_X(x)$ on $x\in\mc{X}$ represents the fraction of possibilities that remain after observing $x$. If we suppose there is a finite possibility space $\Omega$, and the function $X : \Omega \to \mc{X}$ is a random variable that tells us &amp;ldquo;which $x$&amp;rdquo; a given $\omega\in\Omega$ encodes, then the probability of $x$ is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p_X(x) = \frac{\abs{\set{\omega\in\Omega : X(\omega) = x}}}{\abs{\Omega}}\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which is the number of $\omega$s that encode $x$ over the total number of possibilities. This setup is supposing that $x$ is a &lt;strong&gt;partial observation&lt;/strong&gt;, meaning that even after observing $x$, we still don&amp;rsquo;t know which $\omega$ was drawn.&lt;/p&gt;
&lt;p&gt;In bits, we have $-\lg p_X(x)$, which is the number of halvings of the possibility space $\Omega$ due to observing $x$. Bits and probabilities can be viewed as different units for the same quantity. $-\lg p_X(x)$ is called the &lt;strong&gt;self-information&lt;/strong&gt; of $x$, and in the Bayesian setting, the &lt;strong&gt;surprise&lt;/strong&gt; due to observing $x$, implying that a higher number of bits makes $x$ more surprising, which makes sense because $x$ caused you to rule out much more of your possibility space.&lt;/p&gt;
&lt;p&gt;The connection between $p_X$, information gain $-\lg p_X(x)$, and a physical agent, is that for an agent to have a possibility space, it must have the physical representational capacity. If we presume that a system bounded in a finite region of space contains finite information, i.e. can only occupy finitely many distinguishable states, then our agent must have a finite possibility space $\Omega$ in its &lt;strong&gt;model&lt;/strong&gt; of the environment. Gaining information $-\lg p_X(x)$ requires that the agent physically update its internal possibility space, reducing it by the amount $p_X(x)$. That is to say, information gain quantifies a physical update to an agent. In this sense, subjective information quantifies a change to an agent due to its model of the environment and observations, whereas objective information quantifies a change in the environment.&lt;/p&gt;
&lt;p&gt;Note that $p_X$ may be the marginal distribution of $p_{X,H}$, in which case $p_X$ is a subjective data distribution. It is not strictly necessary to actually define hypotheses and priors. $p_X$ can be regarded as a prior, as the end result is the same: the agent has some possibility space, reflecting what the agent is capable of believing, and the proportions of each $x\in\mc{X}$ in that possibility space correspond to how confident the agent is in any given outcome relative to other outcomes.&lt;/p&gt;
&lt;p&gt;If $\Omega$ is infinite (countable or uncountable), then we cannot just divide by the size of $\Omega$ to compute probabilities. In this case, we need to provide a measure that tells us how much of the possibility space $\Omega$ any subset is worth, i.e. $P(A)$ for $A \subseteq \Omega$ measures the fraction of $\Omega$ that $A$ is worth even when $\Omega$ and $A$ are infinite. $P$ is called a &lt;strong&gt;probability measure&lt;/strong&gt;, but don&amp;rsquo;t worry about that. The point is that even in the case of infinite possibilities, we can still think of information gain in terms of narrowing down a possibility space.&lt;/p&gt;
&lt;h2 id=&#34;bayesian-surprise&#34;&gt;Bayesian surprise&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Mutual_information&#34;&gt;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Mutual_information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let $\mc{X}$ be data space, $\mc{H}$ be hypothesis space, and $X,H$ be data and hypothesis random variables.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
I[X,H] &amp;amp;= \E_X[\kl{p_{H\mid X}}{p_H}] \\&lt;br&gt;
&amp;amp; = \E_{x\sim X}[H(p_{H\mid X=x}, p_H) - H(p_{H\mid X=x})] \\&lt;br&gt;
&amp;amp; = \E_{x\sim X}[\E_{h \sim p_{H\mid X=x}}[-\lg p_H(h)] - \E_{h \sim p_{H\mid X=x}}[-\lg p_{H\mid X}(h \mid x)]] \\&lt;br&gt;
&amp;amp; = \E_{x,h \sim p_{X,H}}\left[\lg \left( \frac{p_{H\mid X}(h\mid x)}{p_H(h)}\right)\right] \\&lt;br&gt;
&amp;amp; = \sum_{x,h \in \mc{X} \times \mc{H}}p_{X,H}(x,h)\lg \left( \frac{p_{H\mid X}(h\mid x)}{p_H(h)}\right) \\&lt;br&gt;
&amp;amp; = \sum_{x,h \in \mc{X} \times \mc{H}}p_{X,H}(x,h)\lg \left( \frac{p_{X,H}(x,h)}{p_X(x)p_H(h)}\right)\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$I[X,H]$ is called &lt;strong&gt;Bayesian surprise&lt;/strong&gt;, which is the expected (over data) KL divergence from your prior to posterior (after observing data), which is itself the expected difference in uncertainty (measured in bits, the number of halvings of the full possibility space).&lt;/p&gt;
&lt;p&gt;Pointwise Bayesian information gain (information gained about hypothesis $h$ from data $x$) is $\lg (1/p_H(h)) - \lg (1/p_{H \mid X}(h \mid x)) = \lg (1/p_X(x)) - \lg (1/p_{X \mid H}(x\mid h))$, which is the change in amount of hypothesis weight (posterior mass) that shifted onto $h$.&lt;/p&gt;
&lt;p&gt;$\lg (1/p_{X \mid H}(x \mid h))$ is the &lt;strong&gt;surprise&lt;/strong&gt; (also &lt;strong&gt;self-information&lt;/strong&gt;) of observing $x$ under $h$. The higher this quantity, the more the possibility space of $h$ was narrowed down by $x$.&lt;/p&gt;
&lt;h2 id=&#34;variational-bayes&#34;&gt;Variational Bayes&lt;/h2&gt;
&lt;p&gt;Variational approximation to calculating the Bayesian posterior:&lt;br&gt;






  
    &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;The free-energy principle a unified brain theory#free-energy&lt;/a&gt;
  


&lt;/p&gt;
&lt;p&gt;sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&#34;&gt;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1901.07945&#34;target=&#34;_blank&#34;&gt;What does the free energy principle tell us about the brain?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose $x\in\mc{X}$ is observed and the posterior $p_{H \mid X=x}$ is intractable to compute. We can instead approximate it by minimizing&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
q_x^* = \argmin{q \in \mc{Q}} \kl{q}{p_{H \mid X=x}}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $\mc{Q}$ is some set of probability mass functions $q : \mc{H} \to [0, 1]$, chosen for convenience.&lt;/p&gt;
&lt;p&gt;Assuming we cannot perform this minimization directly, we can make use of the identity&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\kl{q}{p_{H \mid X=x}} = \mc{F}[q] - \lg (1/p_X(x))&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= \kl{q}{p_{H,X=x}} \\&lt;br&gt;
&amp;amp;= \E_{h \sim q} \lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right) \\&lt;br&gt;
&amp;amp;= \sum_{h\in\mc{H}} q(h) \lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right)&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;is the &lt;strong&gt;free energy&lt;/strong&gt;. $\lg (1/p_X(x))$ is the expected surprise of $x$ across all hidden states (weighted by the prior $p_H$).&lt;/p&gt;
&lt;p&gt;Free energy also equals&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= H(q, p_{H, X=x}) - H(q) \\&lt;br&gt;
&amp;amp;= \E_{h \sim q}\left[\lg\left(\frac{1}{p_{H, X}(h,x)}\right) - \lg\left(\frac{1}{q(h)}\right) \right] \\&lt;br&gt;
&amp;amp;= \sum_{h \in \mc{H}} q(h) \lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right)\,,&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where $H(q)$ is the entropy of $q$, and $H(q, p_{H, X=x})$ is the &lt;strong&gt;total energy&lt;/strong&gt;, which is equal to the cross-product of $p_{H, X=x}$ under $q$.&lt;/p&gt;
&lt;p&gt;Note that $p_{H, X=x}(h) = p_{H,X}(h,x)$ is not the same as the conditional distribution $p_{H \mid X=x}(h) = p_{H,X}(h,x) / p_X(x)$, and is not a valid probability distribution because its unnormalized.&lt;/p&gt;
&lt;p&gt;We also have free energy as &lt;strong&gt;complexity&lt;/strong&gt; minus &lt;strong&gt;accuracy&lt;/strong&gt;,&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{aligned}&lt;br&gt;
\mc{F}[q] &amp;amp;= \kl{q}{p_H} - \E_{h\sim q}\left[\lg p_{X \mid H}(x \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q}\left[\lg\left(\frac{q(h)}{p_H(h)}\right)-\lg p_{X \mid H}(x \mid h)\right] \\&lt;br&gt;
&amp;amp;= \E_{h \sim q}\left[\lg\left(\frac{q(h)}{p_{H,X}(h,x)}\right)\right]\,.&lt;br&gt;
\end{aligned}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;This form of free energy can be used in practice. Given any particular $q$ (e.g. as a neural network), the complexity $\kl{q}{p_H}$ and the accuracy $\E_{h\sim q}\left[\lg p_{X \mid H}(x \mid h)\right]$ can be approximated using Monte-Carlo sampling from $q$. This is assuming we have access to a prior $p_H$ over hidden states and predictive (or generative) distribution $p_{X\mid H}$. If $p_{H\mid X}$ is intractable, then a suitable $q^*$ that approximately and sufficiently minimizes $\mc{F}[q]$ becomes our approximation of that posterior.&lt;/p&gt;
&lt;p&gt;Note that there is a $q^*$ for every partial observation $x_{1:t}$, i.e. we need to perform another minimization to arrive at $q_{x_{1:t}}^*$ for every $t$.&lt;/p&gt;
&lt;h2 id=&#34;bayesian-inference-over-time&#34;&gt;Bayesian inference over time&lt;/h2&gt;
&lt;p&gt;I am basing this on Solomonoff induction (as formulated by Marcus Hutter in his &lt;a href=&#34;http://www.hutter1.net/ai/uaibook.htm&#34;target=&#34;_blank&#34;&gt;book&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We now suppose the agent observes an endless stream of data over time. The full possible set of observations are all infinite sequences $\mc{X}^\infty$ where $\mc{X}$ is the set of possible observations at each point in time, e.g. a frame of sensory data such as a video or audio frame. Let $X_{a:b}$ be the random variable denoting a slice of the data stream from time $a$ to time $b$  (inclusive). $X_{1:n}$ is the first $n$ timesteps of data, and $X_{n+1:\infty}$ is everything that is observed after time $n$. I will also use the shorthands $X_{&amp;lt;n} = X_{1:n-1}$ and $X_{&amp;gt;n} = X_{n+1:\infty}$.&lt;/p&gt;
&lt;p&gt;Now suppose the agent has a &lt;strong&gt;hypothesis space&lt;/strong&gt; $\mc{M}$, which is a set of probability distributions $\mu\in\mc{M}$. We call each $\mu$ a hypothesis. Let $\pi$ be a probability distribution over $\mc{M}$ (the prior). Then we have a mixture distribution&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(X_{1:n}) = \sum_{\mu\in\mc{M}} \pi(\mu)\cdot\mu(X_{1:n})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;If we define the joint distribution&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(X_{1:n},\mu) = \pi(\mu)\cdot\mu(X_{1:n})\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;then we have the usual Bayesian quantities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data likelihood: $p(X_{1:n} \mid \mu) = \mu(X_{1:n})$.&lt;/li&gt;
&lt;li&gt;Hypothesis prior: $p(\mu) = \pi(\mu)$.&lt;/li&gt;
&lt;li&gt;Hypothesis posterior: $p(\mu \mid X_{1:n}) = \pi(\mu)\cdot\mu(X_{1:n})/p(X_{1:n})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can think of a finite data sequence $x_{1:n} \in \mc{X}^n$ as a partial observation that the agent updates its mixture weights on:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
w_\mu(x_{1:n}) = \pi(\mu)\frac{\mu(x_{1:n})}{p(x_{1:n})}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;where the hypothesis posterior is&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(\mu \mid x_{1:n}) = w_\mu(x_{1:n})\,.&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We also have a new quantity, the &lt;strong&gt;data posterior&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
p(X_{&amp;gt;n} \mid X_{1:n}=x_{1:n}) = \sum_{\mu\in\mc{M}} w_\mu(x_{1:n})\mu(X_{&amp;gt;n} \mid X_{1:n}=x_{1:n})\,,&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which has the same form as the prior mixture, except that we reweighted by switching from $\pi(\mu)$ to $w_\mu(x_{1:n})$, and we conditionalized the hypotheses, i.e. switched from $\mu(X_{1:\infty})$ to $\mu(X_{&amp;gt;n} \mid X_{1:n}=x_{1:n})$.&lt;/p&gt;
&lt;p&gt;We can incorporate actions into this framework by specifying that all hypotheses in $\mu\in\mc{M}$ must be distributions over a combined observation and action stream. This stream would be a sequence $(x_1, a_1, x_2, a_2, \ldots)$ of alternating observation $x_t \in \mc{X}$ and action $a_t \in \mc{A}$ at every time step. Note that a hypothesis predicts the next observation $\mu(x_t \mid x_{&amp;lt;t}, a_{&amp;lt;t})$, but we don&amp;rsquo;t ask a hypothesis to predict a next action, i.e. $a_t$ given $x_{&amp;lt;t}, a_{&amp;lt;t}$, since that is the agent&amp;rsquo;s decision to make.&lt;/p&gt;






  
    &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;Free energy principle and Solomonoff induction&lt;/a&gt;
  



&lt;h2 id=&#34;hypotheses-vs-states-policies-vs-actions&#34;&gt;Hypotheses vs states; policies vs actions&lt;/h2&gt;
&lt;p&gt;There is a key distinction to make here: a hypothesis $\mu$ is itself a possible universe. $\mu$ is a distribution over all possible infinite data streams $\mc{X}^\infty$. $\mu$ can be arbitrarily complex, and consider all counterfactual latent states in the universe. $\mu$ may encode the dynamics of a time evolving system in its conditional probabilities $\mu(x_n \mid x_{&amp;lt;n})$. In this way $\mu$ is not a hidden state, but an entire possible universe.&lt;/p&gt;
&lt;p&gt;This is in contrast to the idea of a &lt;strong&gt;hidden state&lt;/strong&gt;, which is an unknown state of the universe &lt;strong&gt;at some point in time&lt;/strong&gt;. In the hidden state framework, the environment is defined by a known dynamics distribution $p(x_t, s_t \mid s_{t-1})$ or $p(x_t, s_t \mid s_{t-1}, a_{t-1})$ if we include actions. The only thing that is unknown is $s_{1:t}$. In this framework, knowing $s_{t-1}$ does not mean you know $s_t$ with certainty. In the mixture of hypotheses framework, if you know $\mu$, you know it for all time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: A hypothesis $\mu$ is static and true for all time, and a hidden state $s_t$ evolves and is true only at time $t$.&lt;/p&gt;
&lt;p&gt;We also need to make this distinction between policies and actions. A &lt;strong&gt;policy&lt;/strong&gt; $\pi$ (not to be confused with hypothesis prior above, but this is the conventional notation) is similar to $\mu$, in that it is a probability distribution over alternating observations and actions. $\pi(a_t \mid x_{&amp;lt;t}, a_{&amp;lt;t})$ specifies the agent&amp;rsquo;s action preferences given what it has already seen and done. If we combine an environment $\mu$ and a policy $\pi$, we can fully model the agent-environment interaction loop, i.e. the joint distribution over the space of combined sequences: $\mc{X}\times\mc{A}\times\mc{X}\times\mc{A}\times\ldots$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: A policy $\pi$ is static, i.e. a single agent choice that remains for all time, and an action $a_t$ is a choice made specifically at time $t$.&lt;/p&gt;
&lt;p&gt;However, it is possible to devise a setup where there is a set of possible policies $\Pi$, and the agent keeps &amp;ldquo;changing its mind&amp;rdquo; about which policy $\pi_t \in \Pi$ to use at time $t$. I find this formulation to be a bit redundant, because $\pi_t$ contains information about the agent&amp;rsquo;s preferences in all possible future situations, but if the agent changes its mind in the next step then that information is essentially overridden. Why not just have the agent choose an action $a_t$? It could make sense to impose a restriction that $\pi_t$ cannot evolve quickly over time, so that the policy represents a high-level choice about what to do in some time window. This is one avenue for formulating hierarchical decision making.&lt;/p&gt;
&lt;h1 id=&#34;free-energy-principle-and-time&#34;&gt;Free energy principle and time&lt;/h1&gt;
&lt;p&gt;This is where my understanding falls apart.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve reviewed two sources:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&#34;&gt;https://en.wikipedia.org/wiki/Free_energy_principle#Action_and_perception&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;1-wikipedia&#34;&gt;1. Wikipedia&lt;/h2&gt;
&lt;p&gt;From the first (Wikipedia):&lt;br&gt;


  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210124131932.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
I&amp;rsquo;m reiterating Wikipedia&amp;rsquo;s notation here. Overwrite in your brain my usages of $\mu$ and $s$ above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mu\in R$ is the state of the agent at timestep (not to be confused with hypotheses).&lt;/li&gt;
&lt;li&gt;$a \in A$ is an action taken at every timestep.&lt;/li&gt;
&lt;li&gt;$s \in S$ is an observation at each timestep (not to be confused with environment states).&lt;/li&gt;
&lt;li&gt;$\psi \in \Psi$ is the hidden environment state at every timestep.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is the Bayesian inference being done here? In my exposition on Bayesian inference over time, the posteriors of interest are explicitly given. I&amp;rsquo;d like to know what posterior we are interested in approximating with variational free energy here.&lt;/li&gt;
&lt;li&gt;Is this joint minimization being done simultaneously over all timesteps, or is it done in a greedy fashion on every step?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;2-step-by-step-tutorial&#34;&gt;2. Step-by-Step Tutorial&lt;/h2&gt;
&lt;p&gt;From the second: &lt;a href=&#34;https://psyarxiv.com/b4jm6/&#34;target=&#34;_blank&#34;&gt;A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The paper opens with an exposition that looks like it matches my own for time-less free energy minimization.&lt;/p&gt;
&lt;p&gt;Starting on page 16, we introduce policy $\pi$ (not to be confused with the prior). I&amp;rsquo;m confused about the relationship between the policy and the time evolution of the environment-agent loop. Does $\pi$ change over time, or is $\pi$ chosen up front and held fixed? Clearly it cannot be held fixed, because then the agent is not utilizing its free energy minimization to alter its behavior.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m also confused about the posterior approximation $q$. Now, $q(s \mid \pi)$ depends on the policy. Does this mean we run the free energy minimization for every $\pi$, each producing a different $q$? If that&amp;rsquo;s so, then why do we not write $q(s_t \mid o_{&amp;lt;t}, \pi)$ to indicate the dependency of $q$ on the observations $o_{&amp;lt;t}$ as well?&lt;/p&gt;
&lt;p&gt;Page 19 adds more confusion to the mix. We are introduced to a score function $G(\pi)$ for choosing policies $\pi$.&lt;/p&gt;
&lt;p&gt;

  &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210124142608.png&#34; alt=&#34;&#34;&gt;
&lt;br&gt;
First of all, I thought there is a joint distribution $p(o,s,\pi)$, implying that $p(\pi)$ is a prior which reflects the agent&amp;rsquo;s preferences over policies. So which is it? Does the agent use $G(\pi)$ or $p(\pi)$ to choose its policy? It&amp;rsquo;s also not clear if $\pi$ is chosen once and held fixed for all time, or if the policy changes over time.&lt;/p&gt;
&lt;p&gt;Second, and more perplexing, is that $G(\pi)$ is an expectation over $q(o,s\mid \pi)$, remember that $q$ is an approximate posterior over hidden states $s$. How can $q$ also be a distribution over observations $o$? Trying to make $q$ a joint distribution over $x$ and $h$ in my time-less free energy exposition above doesn&amp;rsquo;t make sense to me.&lt;/p&gt;
&lt;h1 id=&#34;my-open-questions&#34;&gt;My Open Questions&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Is the agent assumed to have a dynamics model of the environment where all that&amp;rsquo;s unknown to the agent is the environment state? that seems unrealistic. if the agent doesn&amp;rsquo;t know the &amp;ldquo;true&amp;rdquo; dynamics model, by what mechanism would the agent improve its dynamics model? The Bayesian posterior approximation is for estimating the effect of its actions on environment state, but this doesn&amp;rsquo;t address how the agent learns about the relationship between action and state.&lt;/li&gt;
&lt;li&gt;How are time and actions incorporated? I understand the time-less variational free energy formulation that I explained above. What I don&amp;rsquo;t understand is what this looks like when applied to an agent-environment interaction loop over time.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>How this blog works</title>
      <link>https://danabo.github.io/blog/posts/how-this-blog-works/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/how-this-blog-works/</guid>
      <description>&lt;p&gt;This blog is a window into my second brain. That is where I store all of my personal notes, ranging from journal entries to productive materials like study notes and math problems. I can mark any of these items for publication on my blog, and I have a script take care of the rest.&lt;/p&gt;
&lt;h1 id=&#34;second-brain&#34;&gt;Second brain&lt;/h1&gt;
&lt;p&gt;I use the &lt;a href=&#34;https://obsidian.md/&#34;target=&#34;_blank&#34;&gt;Obsidian&lt;/a&gt; editor to organize my second brain. It looks like this:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210206135929.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Writing some blog posts in Obsidian.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Every note is a markdown file, with support for extra features like &lt;a href=&#34;https://www.mathjax.org/&#34;target=&#34;_blank&#34;&gt;MathJax&lt;/a&gt; (latex math mode), and some of the features in &lt;a href=&#34;https://roamresearch.com/&#34;target=&#34;_blank&#34;&gt;Roam&lt;/a&gt;, namely &lt;a href=&#34;https://publish.obsidian.md/help/How&amp;#43;to/Format&amp;#43;your&amp;#43;notes&#34;target=&#34;_blank&#34;&gt;wiki-style internal links&lt;/a&gt;. So &lt;code&gt;[[Blogging experiment]]&lt;/code&gt; becomes 





  
    &lt;a href=&#34;https://danabo.github.io/blog/posts/blogging-experiment/&#34;&gt;Blogging experiment&lt;/a&gt;
  


. It has some convenient features like being able to paste images from my clipboard and auto-generate an image embed command:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210206140636.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Previous screenshot I pasted into this very markdown file.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There is also a nifty graph view of all my notes:&lt;/p&gt;
&lt;p&gt;

  &lt;figure&gt;
    
    &lt;img src=&#34;https://danabo.github.io/blog/Pasted%20image%2020210206140906.png&#34; alt=&#34;&#34;&gt;
    &lt;figcaption&gt;Looks like a constellation! Each node is a markdown file. Each edge is due to one file linking to another with `[[...]]`. I&amp;#39;m not yet sure how helpful this is, but it&amp;#39;s nice to have a way to look at everything at once, in lieu of a traditional hierarchical structure.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;publishing-flow&#34;&gt;Publishing flow&lt;/h1&gt;
&lt;p&gt;My blog is written in &lt;a href=&#34;https://gohugo.io/&#34;target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt; (for now), which is a static site generator that takes markdown files as input. Ideally, I could just run hugo directly on my Obsidian directory, but (1) I don&amp;rsquo;t want to publish everything and (2) Obsidian defines its own extended markdown syntax, as I explained above. My workaround is to have a script copy and transform my Obsidian notes marked for publication. Here&amp;rsquo;s how I do it&amp;hellip;&lt;/p&gt;
&lt;p&gt;Markdown files can optionally have a frontmatter, which is a yaml header at the top of the page. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Hello World&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;author&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;John Doe&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In any note in my second brain (Obsidian), I can set &lt;code&gt;blog: true&lt;/code&gt; and that note will be published on my blog.&lt;/p&gt;
&lt;p&gt;Every so often, I run &lt;a href=&#34;https://github.com/danabo/blog/blob/master/publish.sh&#34;target=&#34;_blank&#34;&gt;publish.sh&lt;/a&gt; from the blog repo, which in turn runs &lt;a href=&#34;https://github.com/danabo/blog/blob/master/blog.py&#34;target=&#34;_blank&#34;&gt;blog.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/danabo/blog/blob/master/blog.py&#34;target=&#34;_blank&#34;&gt;blog.py&lt;/a&gt; is where the magic happens. It&amp;rsquo;s a Python script that uses &lt;a href=&#34;https://github.com/google/python-fire&#34;target=&#34;_blank&#34;&gt;fire&lt;/a&gt; to give it a &lt;a href=&#34;https://en.wikipedia.org/wiki/Command-line_interface&#34;target=&#34;_blank&#34;&gt;CLI&lt;/a&gt;. It will go through all the markdown files in my second brain directory and look for the ones with frontmatter containing &lt;code&gt;blog: true&lt;/code&gt;. For those files, it will do a few transformations, like converting internal links, &lt;code&gt;[[...]]&lt;/code&gt; and Obsidian&amp;rsquo;s image command &lt;code&gt;![[...]]&lt;/code&gt; to regular markdown. It also scrubs markdown comments, &lt;code&gt;&amp;lt;!-- ... --&amp;gt;&lt;/code&gt;, and anything inside a &lt;code&gt;&amp;lt;!-- hide --&amp;gt;...&amp;lt;!-- endhide --&amp;gt;&lt;/code&gt; pair so that I can have private sections inside published notes.&lt;/p&gt;
&lt;p&gt;Code to transform Obsidian markdown to &lt;a href=&#34;https://gohugo.io/&#34;target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt; markdown:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;transform_body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# Remove local-only blocks&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;(&amp;lt;!--\s*hide\s*--&amp;gt;.*&amp;lt;!--\s*endhide\s*--&amp;gt;)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DOTALL&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MULTILINE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;# Remove everything after unclosed `&amp;lt;!-- hide --&amp;gt;`&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;(&amp;lt;!--\s*hide\s*--&amp;gt;.*)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DOTALL&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MULTILINE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;# Remove comments&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# https://stackoverflow.com/a/28208465&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;(&amp;lt;!--.*?--&amp;gt;)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DOTALL&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MULTILINE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;# Transform internal links (wiki-style links).&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# https://gohugo.io/content-management/cross-references/#use-ref-and-relref&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;([^!])\[\[(.*?)\]\]&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;\1\{\{\&amp;lt; locallink &amp;#34;\2&amp;#34; \&amp;gt;\}\}&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DOTALL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;[&lt;em&gt;Edit: I&amp;rsquo;ve since updated &lt;a href=&#34;https://github.com/danabo/blog/blob/master/blog.py&#34;target=&#34;_blank&#34;&gt;blog.py&lt;/a&gt; to use an iterator-based parser so that I can ignore comments and wiki-links inside code blocks, which is a problem I ran into writing this very post!&lt;/em&gt;]&lt;/p&gt;
&lt;p&gt;For internal links, I call the &lt;a href=&#34;https://github.com/danabo/blog/blob/master/layouts/shortcodes/locallink.html&#34;target=&#34;_blank&#34;&gt;locallink&lt;/a&gt; Hugo &lt;a href=&#34;https://gohugo.io/content-management/shortcodes/&#34;target=&#34;_blank&#34;&gt;shortcode&lt;/a&gt; I made, i.e. &lt;code&gt;{{ locallink &amp;quot;...&amp;quot; }}&lt;/code&gt;, which checks if the given post name exists. If so, it returns an anchor to the absolute URL for that note. If not, it returns a &lt;em&gt;red&lt;/em&gt; anchor indicating the post does not exist, 





  
    &lt;a href=&#34;&#34; class=&#34;broken&#34;&gt;like this&lt;/a&gt;
  


. That way, if I&amp;rsquo;ve referenced a note that is not marked for publication, the current note will be published. The red link is kind of like a &lt;a href=&#34;https://en.wikipedia.org/wiki/Wikipedia:Red_link&#34;target=&#34;_blank&#34;&gt;missing wiki page&lt;/a&gt;. Perhaps if readers become curious about notes I didn&amp;rsquo;t publish, I might become motivated to publish them.&lt;/p&gt;
&lt;p&gt;locallink &lt;a href=&#34;https://gohugo.io/&#34;target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt; shortcode:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-html&#34; data-lang=&#34;html&#34;&gt;{{ $name := (.Get 0) }}
{{ $postFile := (print &amp;#34;content/posts/&amp;#34; $name &amp;#34;.md&amp;#34;) }}
{{ if (fileExists $postFile) }}
&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;href&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;\=&amp;#34;{{&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;ref&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;}}&amp;#34;\&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;{{ $name }}&lt;span class=&#34;err&#34;&gt;&amp;lt;&lt;/span&gt;/a\&amp;gt;
{{ else }}
&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;href&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;\=&amp;#34;&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;\=&amp;#34;&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;broken&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&amp;#34;\&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;{{ $name }}&lt;span class=&#34;err&#34;&gt;&amp;lt;&lt;/span&gt;/a\&amp;gt;
{{ end }}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/danabo/blog/blob/master/publish.sh&#34;target=&#34;_blank&#34;&gt;publish.sh&lt;/a&gt; will first run blog.py, and then run &lt;code&gt;git commit -v&lt;/code&gt; which shows me the diff. If I add a commit description in the prompt, publish.sh will go ahead and push the changes, and then update the gh-pages branch. If I quit the editor without adding a commit message, publish.sh will abort.&lt;/p&gt;
&lt;h1 id=&#34;reader-experience&#34;&gt;Reader experience&lt;/h1&gt;
&lt;p&gt;Currently the reader sees a typical blog layout: a &amp;ldquo;blog roll&amp;rdquo; of recent posts with previews and tags. I don&amp;rsquo;t intend my notes to have any particular time ordering. Notes are objects in flux. I might edit anything. Since I&amp;rsquo;m using time of last edit as the post date, anything I touch will float back to the top. I might decide to change that.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blogging experiment</title>
      <link>https://danabo.github.io/blog/posts/blogging-experiment/</link>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://danabo.github.io/blog/posts/blogging-experiment/</guid>
      <description>&lt;p&gt;What is this blog?&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a collection of notes pulled directly from my digital journal. My digital journal is a bunch of &lt;a href=&#34;https://www.markdownguide.org/&#34;target=&#34;_blank&#34;&gt;markdown&lt;/a&gt; files that I edit in &lt;a href=&#34;https://obsidian.md/&#34;target=&#34;_blank&#34;&gt;obsidian&lt;/a&gt;. You could call it my &amp;ldquo;second brain&amp;rdquo;. I write everything here. It&amp;rsquo;s a journal in the traditional sense, in that I write about what&amp;rsquo;s going on in my life and what&amp;rsquo;s on my mind in order to collect my thoughts. I also write down logical stuff like TODO-lists, lists of links I want to keep, things I might want to look into. I take copious notes here when I am reading about technical topics. I write down and develop my spurious ideas.&lt;/p&gt;
&lt;p&gt;I want to give the world a window into some of what&amp;rsquo;s in this journal; not because I derive pleasure from exposing my private life to the world (I&amp;rsquo;m actually fairly shy), but because I don&amp;rsquo;t want to be isolated. Learning and thinking alone is less than ideal. I risk reinforcing my own misunderstandings. Feedback from people can reveal things I would not otherwise see.&lt;/p&gt;
&lt;p&gt;Beyond just the benefits for academic studying, feedback from the world helps to give purpose and context to my life. &lt;a href=&#34;https://astralcodexten.substack.com/p/still-alive&#34;target=&#34;_blank&#34;&gt;Scott Alexander recently wrote&lt;/a&gt; that a lot of his  intellectual growth came as a result of blogging, because his blog caused people to reach out to him and tell him things. So the basic premise is that if I broadcast to the world what&amp;rsquo;s on my mind, someone somewhere will take interest and connect with me. Otherwise, we may never know each other exists.&lt;/p&gt;
&lt;p&gt;There is, however, a fundamental tension between quality and broadcasting. Can my notes ever be too raw, too personal, too incomplete, or too short to publish? I want to have a steady stream of output, but I don&amp;rsquo;t want to hide the occasional good stuff in a barrage of no-effort text that one cares about. Not to mention the privacy issues of having a public journal. I don&amp;rsquo;t have a good solution to this tension, so that is why I&amp;rsquo;m experimenting. I said I&amp;rsquo;m providing a window into my personal journal because I will have a process for auto-publishing notes I mark for publication. Hopefully that will remove the activation barrier for posting while also allowing me to keep my finger on quality control (and privacy). More on how this works below.&lt;/p&gt;
&lt;h1 id=&#34;intellectual-journey&#34;&gt;Intellectual journey&lt;/h1&gt;
&lt;p&gt;While I intend to post about miscellaneous topics, there will be a main topic of interest.&lt;/p&gt;
&lt;p&gt;I have been embarking on an academic project to make sense of ideas floating around in machine learning, artificial intelligence, and other fields like neuroscience and epistemology. I want to form my own perspective, independently, on information, uncertainty, randomness, and epistemology.&lt;/p&gt;
&lt;p&gt;Progress is slow. It takes months of on-and-off reading to understand a theoretical topic like &lt;a href=&#34;http://www.scholarpedia.org/article/Algorithmic_randomness&#34;target=&#34;_blank&#34;&gt;algorithmic randomness&lt;/a&gt;. I want people to see what I&amp;rsquo;m working on. I have another blog, &lt;a href=&#34;https://zhat.io/,&#34;&gt;https://zhat.io/,&lt;/a&gt; where I posted explainers on topics that I&amp;rsquo;ve been learning about. The problem is that it takes so long to write pedagogical material, that I haven&amp;rsquo;t even gotten to the topics I&amp;rsquo;ve been studying (still explaining the prerequisites). I tried posting &amp;ldquo;notes&amp;rdquo; (separate from &amp;ldquo;posts&amp;rdquo;) which are informal and often incomplete snapshots of my actual study notes. However I found that I didn&amp;rsquo;t have a clear sense of when my study notes were ready to be published as notes. There was a certain amount of work that I had to do to translate my raw notes over to blog notes, which created a sense of officiality.&lt;/p&gt;
&lt;p&gt;I air on the side of caution when I feel I need to be correct about everything I say. That of course makes total sense. However that stands in contrast with a piece of advice I&amp;rsquo;ve been given for blogging: don&amp;rsquo;t revise, just publish. Blogging is fast and lose. Blogs are just public personal notes. In &lt;a href=&#34;https://zhat.io/&#34;&gt;https://zhat.io/&lt;/a&gt; I made the mistake of taking an authoritative tone, which created a burden of perfectionism. I can&amp;rsquo;t claim something and be wrong about it. But the result was very long technical posts that no one was reading anyway.&lt;/p&gt;
&lt;p&gt;This new blog is an experiment in something more lightweight and streamlined. It&amp;rsquo;s a window into my intellectual journey. My posts are journalism. I&amp;rsquo;m writing about what I saw and experienced while reading and thinking. This is my way of handling the quality-broadcasting tension for academic writing. I&amp;rsquo;m not claiming to be an expert on any topic, or to be explaining any topic. If readers cannot follow along, then that is a good problem to have (that means I have readers). Ideally I&amp;rsquo;ll post often, receive feedback through various channels (such as not following something), and that will provide motivation to explain things. I like writing pedagogy, but I need to know it will actually be read for it to be worth the time investment.&lt;/p&gt;
&lt;p&gt;I intend this blog to be very incremental. Everything is in flux (even it&amp;rsquo;s name, visual style and domain). I want to avoid having to spend a month writing drafts of a long post on a big topic. Instead, I will write a little bit every so often. Think of them as teasers. If people want to hear more, I&amp;rsquo;ll write more. This allows me to get things out of my head and feel good about it. Perhaps I can create more flow in my studying if I feel accomplished more often.&lt;/p&gt;
&lt;p&gt;In that vein, I leave the details of my blogging system and digital journal to future posts.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>