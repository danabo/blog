<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.80.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>variational-ml&nbsp;&ndash;&nbsp;Dan&#39;s Notepad</title><link rel="stylesheet" href="https://danabo.github.io/blog/css/core.min.ca865499b26624a6a7b7e7c6a09b8ef4db427d2fe9ad2ca79f6ba8b23433dbbb302163fdcbf2d6c0dbb66e7472f15ff1.css" integrity="sha384-yoZUmbJmJKant&#43;fGoJuO9NtCfS/prSynn2uosjQz27swIWP9y/LWwNu2bnRy8V/x"><link rel="alternate" type="application/rss+xml" href="https://danabo.github.io/blog/tags/variational-ml/index.xml" title="Dan's Notepad" /><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="variational-ml" /><body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="https://danabo.github.io/blog/"><span class="site name">Dan's Notepad</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="https://danabo.github.io/blog/tags/">Tags</a><a class="nav item" href=""></a><a class="nav item" href="https://zhat%2eio/"target="_blank">Zhat</a></nav></div></span></div><div class="site slogan"><span class="title">A window into my second brain</span></div></section><section id="content"><section class="article header"><h1>variational-ml</h1></section><ul class="note list"><li class="item"><a class="note" href="https://danabo.github.io/blog/posts/on-variational-inference/">
            <p class="note title">On Variational Inference</p><p class="note date">July 6, 2022</p><p class="note content">This is a primer on variational inference in machine learning, based on sections of Jordan et al. (An Introduction to Variational Methods for Graphical Models; 1999). I go over the mathematical forms of variational inference, and I include a discussion on what it means for something to be &ldquo;variational.&rdquo; I hope this conveys a bit of the generating ideas that give rise to the various forms of variational inference.
<span class="mldr">&mldr;</span></p></a><p class="note labels"><a class="tag" href="https://danabo.github.io/blog/tags/machine-learning/">machine-learning</a><a class="tag" href="https://danabo.github.io/blog/tags/variational-ml/">variational-ml</a></p></li><li class="item"><a class="note" href="https://danabo.github.io/blog/posts/variational-solomonoff-induction/">
            <p class="note title">Variational Solomonoff Induction</p><p class="note date">February 18, 2021</p><p class="note content">$$
\newcommand{\mb}{\mathbb}
\newcommand{\mc}{\mathcal}
\newcommand{\E}{\mb{E}}
\newcommand{\B}{\mb{B}}
\newcommand{\R}{\mb{R}}
\newcommand{\kl}[2]{D_{KL}\left(#1\ \| \ #2\right)}
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\ }
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}}\ }
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\ve}{\varepsilon}
\newcommand{\t}{\theta}
\newcommand{\T}{\Theta}
\newcommand{\o}{\omega}
\newcommand{\O}{\Omega}
\newcommand{\sm}{\mathrm{softmax}}
$$
The free energy principle is a variational Bayesian method for approximating posteriors. Can free energy minimization combined with program synthesis methods from machine learning tractably approximate Solomonoff induction (i.e. universal inference)? In these notes, I explore what the combination of these ideas looks like.
Machine learning I want to make an important clarification about &ldquo;Bayesian machine learning&rdquo;.<span class="mldr">&mldr;</span></p></a><p class="note labels"><a class="tag" href="https://danabo.github.io/blog/tags/free-energy/">free-energy</a><a class="tag" href="https://danabo.github.io/blog/tags/machine-learning/">machine-learning</a><a class="tag" href="https://danabo.github.io/blog/tags/variational-ml/">variational-ml</a></p></li></ul></section><section id="footer"><div class="footer-wrap">
    <p class="copyright">Â©2021 Daniel Abolafia.</p>
    <p class="powerby"><span>Powered&nbsp;by&nbsp;</span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span>&nbsp;&amp;&nbsp;</span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p></div></section><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ 
                tex2jax: { 
                    inlineMath: [['$','$'], ['\\(','\\)']] 
                },

                "HTML-CSS": {
                    preferredFont: "TeX",
                    availableFonts: ["TeX"]
                }
            });
        </script></body>

</html>