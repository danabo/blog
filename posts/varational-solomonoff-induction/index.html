<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.80.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>Varational Solomonoff Induction&nbsp;&ndash;&nbsp;Dan&#39;s Notepad</title><link rel="stylesheet" href="https://danabo.github.io/blog/css/core.min.402a135398ac20effd874582441adfb484001fb367ab4dabaaa1c8fdbff0b65e4396f6a8bf95739eeecbd62f725be605.css" integrity="sha384-QCoTU5isIO/9h0WCRBrftIQAH7Nnq02rqqHI/b/wtl5Dlvaov5Vznu7L1i9yW&#43;YF"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Varational Solomonoff Induction" /><body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="https://danabo.github.io/blog/"><span class="site name">Dan's Notepad</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="https://danabo.github.io/blog/tags/">Tags</a><a class="nav item" href=""></a><a class="nav item" href="https://zhat%2eio/"target="_blank">Zhat</a></nav></div></span></div><div class="site slogan"><span class="title">A window into my second brain</span></div></section><section id="content"><div class="article-container"><section class="article header">
    <h1 class="article title">Varational Solomonoff Induction</h1><p class="article date">February 18, 2021</p></section><article class="article markdown-body"><p>$$<br>
\newcommand{\mb}{\mathbb}<br>
\newcommand{\mc}{\mathcal}<br>
\newcommand{\E}{\mb{E}}<br>
\newcommand{\B}{\mb{B}}<br>
\newcommand{\R}{\mb{R}}<br>
\newcommand{\kl}[2]{D_{KL}\left(#1\ \| \ #2\right)}<br>
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\ }<br>
\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}}\ }<br>
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}<br>
\newcommand{\set}[1]{\left\{#1\right\}}<br>
\newcommand{\ve}{\varepsilon}<br>
\newcommand{\t}{\theta}<br>
\newcommand{\T}{\Theta}<br>
\newcommand{\o}{\omega}<br>
\newcommand{\O}{\Omega}<br>
\newcommand{\sm}{\mathrm{softmax}}<br>
$$</p>
<p>The <a href="https://en.wikipedia.org/wiki/Free_energy_principle"target="_blank">free energy principle</a> is a <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods"target="_blank">variational Bayesian method</a> for approximating posteriors. I want to know, what would it look like to apply free energy minimization to <a href="https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference"target="_blank">Solomonoff induction</a>, i.e. universal inference?</p>
<h1 id="machine-learning">Machine learning</h1>
<p>In parametric machine learning, we have a function $f_\t$ parametrized by $\t\in\T$. Let $q_\t(D)$ be a probability distribution on datasets $D$ defined in terms of $f_\t$. For supervised learning, $q_\t(D) = \prod_{(x,y)\in D} Pr(y; f_\t(x))$ is the product of probabilities of each target $y$ given distribution parameters $f_\t(x)$, e.g. $f_\t(x)$ returns the mean and variance of a Gaussian over $y$. For unsupervised learning, $f_\t(x)$ might return a real number which serves as the log-probability of each $x \in D$. In general $f_\t$ can be any kind of parametric ML model, but these days it is likely to be a neural network.</p>
<p>Typical usages of $q_\t$ in machine learning:</p>
<ul>
<li><strong>MLE</strong> (<a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation"target="_blank">maximum likelihood</a>): Training produces hypothesis with highest data likelihood.
<ul>
<li>$\t\in\T$ is a hypothesis.</li>
<li>$\t^* = \argmax{\t}\log q_\t(D)$ for dataset $D$.</li>
</ul>
</li>
<li><strong>MAP</strong> (<a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"target="_blank">maximum a posteriori</a>): Training produces model with highest posterior probability.
<ul>
<li>$\t\in\T$ is a hypothesis.</li>
<li>$\t^* = \argmax{\t}\left[\log q_\t(D) + \log p(\t)\right]$ for dataset $D$ and prior $p(\t)$.</li>
<li>$\log p(\t)$ can be viewed as a <a href="https://en.wikipedia.org/wiki/Regularization_%28mathematics%29"target="_blank">regularizer</a>. MAP is just MLE plus regularization - the most typical form of parametric machine learning.</li>
</ul>
</li>
<li><strong>Bayesian Inference</strong>: Prior over the parameter induces a posterior over parameters given data.
<ul>
<li>$\t\in\T$ is a hypothesis.</li>
<li>$p(\t \mid D) = \frac{q_\t(D) p(\t)}{\int_\T q_\t(D) p(\t) d\t}$ is the exact posterior on hypotheses $\T$ given dataset $D$.</li>
<li>Unlike in MLE and MAP, there is no notion of optimal parameter $\t^*$. Instead we have much more information: $p(\t \mid D)$ &ldquo;scores&rdquo; every parameter in $\T$, and all the scores taken together constitute our information.</li>
</ul>
</li>
<li><strong>Variational Bayes</strong> (free energy minimization): training produces a (approximate) posterior distribution over hypotheses.
<ul>
<li>$z \in \mc{Z}$ is a hypothesis.</li>
<li>$\tilde{\t} = \argmin{\t}\kl{q_\t(z)}{p(z \mid D)}$ is the target parameter for dataset $D$ and model $p(D,z)$. This is assumed to be intractable to find.</li>
<li>$\t^* = \argmin{\t}\kl{q_\t(z)}{p(z)} - \E_{z\sim f_\t(z)}\left[\lg p(D \mid z)\right]$ is the approximation. This is what we find through optimization.</li>
</ul>
</li>
</ul>
<p>The variational Bayes &ldquo;usage mode&rdquo; is clearly different from the others. MLE and MAP are fitting $f_\t$ to the data, i.e. finding a single $t^*\in\T$ that maximizes the probability of the data under $q_\t$. Bayesian inference is finding a distribution $p(\t \mid D)$ on $\T$ which represents the model&rsquo;s beliefs about various parameters $\t\in\T$ being likely or unlikely as explanations of the data. This is not the same as fitting $f_\t$ to data, since we are not choosing any particular parameter in $\T$. Variational Bayes uses $q_{\t^*}$ as an approximation of $p(\t \mid D)$, where $\t^*\in\T$ is the optimal parameter of distribution $q_\t(z)$ and $z\in\mc{Z}$ is a hypothesis.</p>
<p>In the first three modes, $\T$ are hypotheses and we are either selecting one or finding a distribution over them. In the variational Bayes mode, $\T$ are not hypotheses. Instead we introduce $\mc{Z}$ as the hypothesis space and $\T$ is the parameter space for the approximate posterior $q_\t(z)$ on $\mc{Z}$, i.e. $q_\t(z)$ approximates $p(z\mid D)$. We don&rsquo;t have $\mc{Z}$ in the first three modes, and we are interested in $p(\t \mid D)$ rather than $p(z \mid D)$. Also in the first three modes, $q_\t(D)$ is a distribution on what is observed, datasets $D$, rather than over latent $\mc{Z}$.</p>
<h1 id="solomonoff-induction">Solomonoff induction</h1>
<p>I learned about this topic from <a href="https://www.springer.com/gp/book/9781489984456"target="_blank">An Introduction to Kolmogorov Complexity and Its Applications</a> and <a href="http://www.hutter1.net/ai/uaibook.htm"target="_blank">Universal Artificial Intelligence</a>. I recommend both books as references.</p>
<p>Let $\B = \{0,1\}$ be the binary alphabet, $\B^n$ be the set of all length-$n$ binary strings, and $\B^\infty$ be the set of all infinite binary strings.</p>
<p>Let $\B^* = \B^0 \times \B^1 \times \B^2 \times \B^3 \times\ldots$ be the set of all finite binary strings of any length.<br>
$\epsilon$ is the empty string, i.e. $\B^0 = \{\epsilon\}$.</p>
<p>Let $x \sqsubseteq y$ denote that binary string $x$ is a prefix of (or equal to) binary string $y$.<br>
Let $x`y$ denote the string concatenation of $x$ and $y$.<br>
Let $x_{a:b}$ denote the slice of $x$ from position $a$ to $b$ (inclusive).<br>
Let $x_{&lt;n} = x_{1:n-1}$ denote the prefix of $x$ up to $n-1$.<br>
Let $x_{&gt;n} = x_{n+1:\infty}$ denote the &ldquo;tail&rdquo; of $x$ starting from $n+1$.</p>
<h2 id="version-1">Version 1</h2>
<p>Let $U$ be a universal machine, i.e. if $z\in\B^*$ is a program then $U(z) \in \B^*$ is some binary string, or $U(z)$ is undefined because $U$ does not halt on $z$. We do not give program $z$ input, but $z$ can include &ldquo;data&rdquo;, in the sense that it&rsquo;s program specifies a binary string that gets loaded into memory when the program starts.</p>
<p>Let $p(z)$  be a prior on finite binary strings.<br>
Then for observation $x \in \B^*$,</p>
<ul>
<li>$p(x \mid z) = \begin{cases}1 &amp; x \sqsubseteq U(z) \\ 0 &amp; \mathrm{otherwise}\end{cases}$</li>
<li>$p(z, x) = p(x \mid z)p(z) = \begin{cases}p(z) &amp; x \sqsubseteq U(z) \\ 0 &amp; \mathrm{otherwise}\end{cases}$</li>
<li>$p(x) = \sum_{z\in\B^*} p(x,z) = \sum_{z \in \B^*;\ x \sqsubseteq U(z)} p(z)$</li>
<li>$p(z \mid x) = p(z, x)/p(x) = \begin{cases}p(z)/p(x) &amp; x \sqsubseteq U(z) \\ 0 &amp; \mathrm{otherwise}\end{cases}$</li>
</ul>
<p>$p(x)$ is the data probability.<br>
$p(z)$ is the prior.<br>
$p(z\mid x)$ is the posterior.</p>
<p>If we observe $x \in \B^*$, we may ask for the probability of subsequently observing some $y\in\B^*$:</p>
<p>$$<br>
\begin{aligned}<br>
p(y \mid x) &amp;= \frac{1}{p(x)}\sum_{z\in\B^*}p(x`y,z) \\<br>
&amp;= \frac{1}{p(x)}\sum_{z \in \B^*;\ x`y \sqsubseteq U(z)} p(z) \\<br>
&amp;= \sum_{z \in \B^*;\ x`y \sqsubseteq U(z)} p(z \mid x)\,.<br>
\end{aligned}<br>
$$</p>
<p>What prior $p(z)$ should we choose? Solomonoff recommends</p>
<p>$$<br>
p(z) = 2^{-\ell(z)}<br>
$$</p>
<p>where $\ell(z)$ is the length of program $z$. This has the effect of putting more prior probability on short programs, essentially encoding a preference for &ldquo;simple&rdquo; explanations of the data. This prior also has the benefit that it is fast to calculate $p(z)$ for any $z$. In general, choice of prior is a matter of taste, and should depend on practical considerations like tractability and regularizations such as &ldquo;simplicity&rdquo;.</p>
<h2 id="version-2">Version 2</h2>
<p>Let $\mu$ be a probability measure on $\B^\infty$, meaning $\mu$ maps (measurable) subsets of $\B^\infty$ to probabilities. $\mu$ can be specified by defining it&rsquo;s value on the <strong>cylinder sets</strong> $\Gamma_x = \set{\o \in \B^\infty \mid x \sqsubset \o}$ for every $x\in\B^*$, i.e. the set of all infinite binary strings starting with $x$. I&rsquo;ll let $\mu(x)$ be a shorthand denoting $\mu(\Gamma_x)$. Then $\mu(x)$ is the probability of finite string $x$. For any such measure $\mu$, it must be the case that</p>
<p>$$<br>
\mu(x) = \mu(x`0) + \mu(x`1)\,,<br>
$$</p>
<p>and</p>
<p>$$<br>
\mu(x) \geq \mu(x`y)\,,<br>
$$</p>
<p>for all $x,y\in\B^*$.</p>
<p>$\mu$ is a <strong>semimeasure</strong> iff it satisfies</p>
<p>$$<br>
\mu(x) \geq \mu(x`0) + \mu(x`1)<br>
$$</p>
<p>for all $x \in \B^*$. That is to say, if $\mu$ is a semimeasure then probabilities may sum to less than one (this is like supposing that some probability goes missing).</p>
<p>The following are equivalent:</p>
<ul>
<li>$\mu$ is <strong>computable</strong></li>
<li>There exists some program which computes the probability $\mu(x)$ for all inputs $x$.</li>
<li>There exists some program which outputs $x$ with probability $\mu(x)$ (for all $x$) when given uniform random input bits.</li>
</ul>
<p>$\mu$ is <strong>semicomputable</strong> (a.k.a. enumerable) if there exists some program which approximates the probability $\mu(x)$ (for all $x$) by outputting a sequence of rational numbers $\set{\hat{p}_n}$ approaching $\mu(x)$, but where it is impossible to determine how close the sequence is to $\mu(x)$ at any point in time. That is to say, you cannot know the error sequence $\varepsilon_n = \abs{\mu(x) - \hat{p}_n}$, but you know that $\varepsilon_n \to 0$ as $n\to\infty$. In contrast, if $\mu$ is computable then there exists a program that outputs both the sequence of rationals $\set{\hat{p}_n}$ AND the errors $\set{\varepsilon_n}$ (computability implies there exists a program that takes a desired error $\varepsilon&gt;0$ as input and outputs in finite time (i.e. halts) the corresponding approximation $\hat{p}$ s.t. $\varepsilon&gt;\abs{\mu(x)-\hat{p}}$).</p>
<p>Let $\mc{M}$ be the set of all semicomputable semimeasures on infinite binary sequences. Let $p(\mu)$ be a prior on $\mc{M}$.</p>
<p>Then for observation $x \in \B^*$,</p>
<ul>
<li>$p(x \mid \mu) = \mu(x)$</li>
<li>$p(x, \mu) = p(x \mid \mu)p(\mu) = p(\mu)\mu(x)$</li>
<li>$p(x) = \sum_{\mu \in \mc{M}} p(x \mid \mu) = \sum_{\mu \in \mc{M}} p(\mu)\mu(x)$</li>
<li>$p(\mu \mid x) = p(\mu)\frac{\mu(x)}{p(x)}$</li>
</ul>
<p>$p(x)$ is the data probability.<br>
$p(\mu)$ is the prior.<br>
$p(\mu\mid x)$ is the posterior.</p>
<p>If we observe $x \in \B^*$, we may ask for the probability of subsequently observing some $y\in\B^*$:</p>
<p>$$<br>
\begin{aligned}<br>
p(y \mid x) &amp;= \sum_{\mu \in \mc{M}} p(y,\mu\mid x) \\<br>
&amp;= \sum_{\mu \in \mc{M}} p(\mu\mid x)\mu(y \mid x) \\<br>
&amp;= \sum_{\mu \in \mc{M}}p(\mu)\frac{\mu(x)}{p(x)}\mu(y\mid x)\,.<br>
\end{aligned}<br>
$$</p>
<p>$\mc{M}$ is all semicomputable semimeasures, rather than all computable measures, because the former is a decidable set while the latter is not, i.e. in practice the former set of hypotheses can be feasibly enumerated by enumerating all programs, while the latter cannot be. If we required only measures, then we could not decide which programs produced proper measures (if the program doesn&rsquo;t halt on $x$, that is like saying the probability that would have gone to strings starting with $x$ &ldquo;disappears&rdquo;). Allowing non-halting programs means we don&rsquo;t have to filter out programs which don&rsquo;t halt. Similar issue for computable vs semicomputable.</p>
<p>Versions 1 and 2 are equivalent. That is to say, we can get the same data distribution $p(x)$ for the right choice of prior in each version.</p>
<p>A typical choice of the prior in this version is</p>
<p>$$<br>
p(\mu) = 2^{-K(\mu)}<br>
$$</p>
<p>where $K(\mu)$ is the <a href="https://www.math.wisc.edu/~jmiller/Notes/contrasting.pdf"target="_blank"><strong>prefix-free Kolmogorov complexity</strong></a> of $\mu$, i.e. the length of the shortest program that (semi)computes $\mu$ (given a space of prefix-free programs, i.e. program strings contain their own length information).</p>
<h2 id="version-3">Version 3</h2>
<p>As you might guess, this will also turn out to be (sorta) equivalent to the first two versions. This is like version 2, except instead of considering a hypothesis to be a program that samples data sequences given uniform random input bits, a hypothesis is such a program packaged together with a particular infinite input sequence. Thus, hypotheses are in a sense infinite programs.</p>
<p>Let $U$ be a <strong>universal monotone machine</strong>. That means $U$ can execute infinitely long programs in a streaming fashion by producing partial outputs as the program is read. Let $z \in \B^\infty$. Then for every finite prefix $z_{1:n}$, we get a partial output $U(z_{1:n}) = \o_{1:m} \in \B^m$. We require that $U(z_{1:n}) \sqsubseteq U(z_{1:n'})$ for $n \leq n'$. The output of $z$ is infinite if $m\to\infty$ as $n\to\infty$, is finite if $m &lt; \infty$ for all $n$, or is undefined if $U$ does not halt on any $z_{1:n}$.</p>
<p>Let $\tilde{z}$ be a program that samples from $\mu$ from version 2, i.e. $\tilde{z}$ takes uniform random bits as input and outputs some $x_{1:m}$ with probability $\mu(x_{1:m})$. Then we can produce an infinite &ldquo;version 3&rdquo; program by appending an infinitely long uniform random sequence $u$, i.e. $z = \tilde{z}`u$ where $U(\tilde{z}) = \epsilon$ (empty string, i.e. executing $\tilde{z}$ outputs nothing), and $U(\tilde{z}`u_{1:n'})$ outputs some prefix of $x$ (if we let $x$ be infinite) by running $\tilde{z}$ on input $u_{1:n'}$.</p>
<p>If we feed uniform random bits into $U$, then $U$ itself samples from a distribution over infinite data sequences. The induced distribution is $p(x_{1:m})$ which is a Solomonoff distribution that we can use for universal inference. This is equivalent to putting a uniform prior on the infinite programs $\B^\infty$, i.e.</p>
<p>$$<br>
p(z_{1:n}) = 2^{-n}\,.<br>
$$</p>
<p>For partial observation $x_{1:m} \in \B^*$ (the remaining part of $x$ is the unobserved future),</p>
<p>$$<br>
p(x_{1:m}) = \sum_{\zeta\in\Phi(x_{1:m})} 2^{-\ell(\zeta)}\,,<br>
$$</p>
<p>where</p>
<ul>
<li>$\ell(\zeta)$ is the length of finite string $\zeta$.</li>
<li>$\Phi(x_{1:m})$ is a prefix-free set of all finite sequences $\zeta$ which output at least $x_{1:m}$ when fed into $U$ (i.e. for all $z_{1:n} \in \B^*$ if $x_{1:m} \sqsubseteq U(z_{1:n})$ then $z_{1:n} \in \bigcup_{\zeta\in\Phi(x_{1:m})} \Gamma_\zeta$).</li>
</ul>
<p>To calculate $p(x_{1:m})$, we ostensibly want to sum up the prior probabilities of all programs which output at least $x_{1:m}$, but remember that our programs are infinitely long, and the prior probability of any infinite program is 0 (because $2^{-n}\to 0$ as $n\to\infty$). The sum above performs a <a href="https://en.wikipedia.org/wiki/Lebesgue_integration"target="_blank">Lebesgue integral</a> over the infinite programs $\B^\infty$ by dividing them into &ldquo;intervals&rdquo; (i.e. sets of programs sharing the same prefix - geometrically an interval if you consider an infinite binary sequence to be a real number between 0 and 1) and summing up the lengths (prior probabilities) of the intervals. The function $\Phi$ is a convenient construction for producing this set of intervals for us. Finding $\Phi(x_{1:m})$ is complex, and not especially important to go into.</p>
<p>The joint distribution is</p>
<p>$$p(x_{1:m}, z_{1:n}) = \sum_{\zeta\in\Phi(x);\ z_{1:n}\sqsubseteq\zeta} 2^{-\ell(\zeta)}\,.$$</p>
<p>From here, we can straightforwardly compute the probability of the data under partial hypothesis $z_{1:n}$:</p>
<p>$$p(x_{1:m} \mid z_{1:n}) =p(x_{1:m}, z_{1:n})/p(z_{1:n}) = \sum_{\zeta\in\Phi(x);\ z_{1:n}\sqsubseteq\zeta} 2^{n-\ell(\zeta)}\,.$$</p>
<p>And finally the data posterior of the future slice $x_{m:s}$ given $x_{&lt;m}$ (for $m&lt;s$):</p>
<p>$$<br>
p(x_{m:s}\mid x_{&lt;m}) = \frac{1}{p(x_{&lt;m})}\sum_{\zeta\in\Phi(x_{1:s})} 2^{-\ell(\zeta)}\,.<br>
$$</p>
<h1 id="variational-solomonoff-induction">Variational Solomonoff induction</h1>
<p>Suppose we observe finite sequence $x_{1:t} \in \B^*$ and we want to find the posterior $p(h \mid x_{1:t})$. Usually this is intractable to calculate, and in the case of Solomonoff induction, the posterior is not even computable. We can get around this limitation by approximating the posterior with a parametrized distribution $q_\t(h)$ over hypotheses $h\in\mc{H}$. For now I will be agnostic as to what kind of hypothesis space $\mc{H}$ is, and it can be any of the hypothesis spaces discussed above: <a href="#version-1">#Version 1</a>, <a href="#version-2">#Version 2</a>, <a href="#version-3">#Version 3</a>.</p>
<p>In this case, let&rsquo;s find $\t^*\in\T$ that minimizes the KL-divergence $\kl{q_\t(h)}{p(h\mid x_{1:t})}$ so that $q_\t(h)$ is as close as possible to $p(h\mid x_{1:t})$. Note that $q_\t(h)$ does not depend on $x_{1:t}$ because we find $\t^*$ after $x_{1:t}$ is already observed ($x_{1:t}$ is like a constant w.r.t. this optimization), whereas the joint distribution $p(h, x)$ is defined up front before any data is observed.</p>
<p>However, if $p(h \mid x_{1:t})$ is intractable to calculate, then so is $\kl{q_\t(h)}{p(h\mid x_{1:t})}$. With a few tricks, we can find an alternative optimization target that is tractable. Rewriting the KL-divergence:</p>
<p>$$<br>
\begin{aligned}<br>
&amp;\kl{q_\t(h)}{p(h\mid x_{1:t})} \\<br>
&amp;\quad= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h\mid x_{1:t})}\right)\right] \\<br>
&amp;\quad= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h)}\right)-\lg p(x_{1:t} \mid h) + \lg p(x_{1:t})\right] \\<br>
&amp;\quad= \kl{q_\t(h)}{p(h)} - \E_{h\sim q_\t}\left[\lg p(x_{1:t} \mid h)\right] - \lg \frac{1}{p(x_{1:t})} \\<br>
&amp;\quad= \mc{F}[q_\t] - \lg \frac{1}{p(x_{1:t})} \,.<br>
\end{aligned}<br>
$$</p>
<p>where $\mc{F}[q_\t]$ is defined as</p>
<p>$$<br>
\begin{aligned}<br>
\mc{F}[q_\t] &amp;= \kl{q_\t(h)}{p(h)} - \E_{h\sim q_\t}\left[\lg p(x_{1:t} \mid h)\right] \\<br>
&amp;= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h)}\right)-\lg p(x_{1:t} \mid h)\right] \\<br>
&amp;= \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h,x_{1:t})}\right)\right]\,.<br>
\end{aligned}<br>
$$</p>
<p>$\mc{F}[q_\t]$ is called the <strong>variational free energy</strong>. It depends explicitly on choice of parameter $\t$, but also keep in mind it depends implicitly on the observation $x_{1:t}$ and distribution $p(h, x_{1:t})$.</p>
<p>Because $\lg \frac{1}{p(x_{1:t})}$ (called the <strong>surprise</strong> of $x_{1:t}$) is positive and constant (because observation $x_{1:t}$ is constant), then minimizing $\mc{F}[q_\t]$ to $\lg \frac{1}{p(x_{1:t})}$ guarantees that $\kl{q_\t(h)}{p(h\mid x_{1:t})}$ is 0 (KL-divergence cannot be negative), which in turn guarantees that $q_\t(h)$ and $p(h\mid x_{1:t})$ are equal distributions on $\mc{H}$. If our optimization process does not fully minimize $\mc{F}[q_\t]$, then $q_\t(h)$ will approximate $p(h\mid x_{1:t})$ with some amount of error.</p>
<p>The optimization procedure we want to perform is</p>
<p>$$<br>
\begin{aligned}<br>
&amp;\argmin{\t\in\T} \mc{F}[q_\t] \\<br>
=\ &amp; \argmin{\t\in\T} \E_{h \sim q_\t}\left[\lg\left(\frac{q_\t(h)}{p(h,x_{1:t})}\right)\right] \\<br>
=\ &amp; \argmax{\t\in\T}\set{ \E_{h \sim q_\t}\left[R(h)\right] + \mb{H}_{h \sim q_\t}[q_\t(h)]}<br>
\end{aligned}<br>
$$</p>
<p>This now has the form of a one-timestep reinforcement learning objective, where $R(h) = \lg p(h,x_{1:t})$ is the reward for &ldquo;action&rdquo; $h$, and $\mb{H}_{h \sim q_\t}[q_\t(h)]$ is the entropy of $q_\t(h)$. Here $q_\t(h)$ is called the <strong>policy</strong>, i.e. the distribution actions are sampled from. Maximizing this objective jointly maximizes expected reward under the policy and entropy of the policy. An entropy term is typically added to RL objectives as a regularizer to encourage exploration (higher entropy policy means more random actions), but in this case the entropy term comes included.</p>
<p>We can use standard <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html"target="_blank">policy gradient methods</a> (e.g. <a href="https://deepmind.com/research/publications/impala-scalable-distributed-deep-rl-importance-weighted-actor-learner-architectures"target="_blank">IMPALA</a>) to maximize the above RL objective (equivalent to minimizing free energy), so long as the reward $R(h) = \lg p(h,x_{1:t})$ is fast to compute.</p>
<h1 id="tractability">Tractability</h1>
<p>Tractability depends on our choice of $\mc{H}$ and prior $p(h)$. What operations do we want to be tractable? Typically we want:</p>
<ol>
<li>To calculate approximate hypothesis posteriors.</li>
<li>To calculate predictive data distributions (data posterior).</li>
</ol>
<h2 id="hypothesis-posterior">Hypothesis posterior</h2>
<p>The approximation $q_\t(h)$ allows us to do this. The tractability of finding a good $q_\t(h)$ using policy gradient methods will require the reward $R(h) = \lg p(h,x_{1:t})$ is fast to calculate.</p>
<p>We can write the reward as the sum of two terms:</p>
<p>$$<br>
\lg p(h,x_{1:t}) = \lg p(h) + \lg p(x_{1:t} \mid h)\,.<br>
$$</p>
<p>Then we need fast calculation of prior probabilities $p(h)$, and data probabilities under hypotheses, $p(x_{1:t} \mid h)$.</p>
<h2 id="data-posterior">Data posterior</h2>
<p>We want to approximate $p(y \mid x)$, i.e. the probability of observing string $y$ after observing $x$. This is similar to the problem of calculating $p(x)$, the data probability.</p>
<p>$$<br>
p(x) = \E_{h\sim p(h)} [p(x\mid h)]\,.<br>
$$</p>
<p>If it were fast to compute $p(x\mid h)$ for a given $h$, and fast to sample from the prior $p(h)$, then we can approximate the data probability with Monte Carlo sampling:</p>
<p>$$<br>
p(x) \approx \hat{p}(x) = \sum_{h \in H^{(k)}} p(x\mid h)<br>
$$</p>
<p>where $H^{(k)} = \set{h_1, h_2, \ldots, h_k} \sim p(h)$ is an i.i.d. sample from $p(h)$ of size $k$.</p>
<p>In the same way, we can approximate the data posterior using the identity</p>
<p>$$<br>
p(y \mid x) = \E_{h\sim p(h \mid x)} [p(y\mid x, h)]\,.<br>
$$</p>
<p>The Monte Carlo approximation is:</p>
<p>$$<br>
p(y\mid x) \approx \hat{p}(y\mid x) = \sum_{h \in H^{(k)}} p(y\mid x, h)<br>
$$</p>
<p>where $H^{(k)} = \set{h_1, h_2, \ldots, h_k} \sim q_{\t^*}(h)$ is an i.i.d. sample drawn from the optimized approximate posterior $q_{\t^*}(h)$. So approximating the data posterior requires approximating the hypothesis posterior.</p>
<p>$p(y\mid x, h)$ is the conditional data distribution under hypothesis $h$. If $h$ is a probability measure, then $p(x \mid h) = h(x)$ and $p(y\mid x, h) = h(y\mid x)$.</p>
<p>For approximating the data distribution, we need fast sampling from hypothesis priors $p(h)$ and fast data-under-hypothesis probabilities $p(x \mid h)$.</p>
<p>For approximating the data posterior, we need fast approximate posteriors $q_{\t^*}(h)$, and we need hypothesis data-conditionalization $p(y\mid x, h)$ to be fast.</p>
<h1 id="choices">Choices</h1>
<p>Is there a choice of $\mc{H}$ and $p(x,h)$ s.t.</p>
<ul>
<li>Prior $p(h)$ is fast to calculate and sample from.</li>
<li>Approximate hypothesis posterior $q_{\t^*}(h)$ is fast to sample from.</li>
<li>Hypothesis data-probability $p(x\mid h)$ is fast to calculate.</li>
<li>Hypothesis data-conditionalization $p(y\mid x, h)$ is fast to calculate.</li>
</ul>
<h2 id="program-space">Program space</h2>
<p>Hypotheses can be deterministic or stochastic. Deterministic hypotheses would be represented by deterministic programs. Stochastic hypotheses can either be represented by stochastic programs (output is non-deterministic) or by deterministic programs that output probabilities. I think we should choose the latter.</p>
<p>If our hypotheses are deterministic, then we get Solomonoff induction <a href="#version-1">#Version 1</a>. Conditionalization is easy because $p(y \mid x, h) = p(y`x \mid h) = 1$ if $h$ outputs $x$ and $0$ otherwise. However, the vast majority of programs will not output $x$, so the reward $R(h) = \lg p(h,x)$ will be very sparse. That is to say, the reward will be $-\infty$ most of the time (in practice you would clip and scale the reward to something reasonable). This is bad for policy gradient methods and will result in high gradient variance (learning will be extremely slow).</p>
<p>We should use stochastic hypotheses. If we use non-deterministic programs, conditionalization is hard. Thus we should use programs that output their probabilities.</p>
<p>The choice of $\mc{H}$ is equivalent to choosing a programming language plus syntax rules so that only valid programs can be constructed. In this case, we want to restrict ourselves to programs that will obey the properties of probability measures $\mu$ on infinite sequences: $\mu(x) = \mu(x`0) + \mu(x`1)$ and  $\mu(x) \geq \mu(x`y)$.</p>
<p>To ensure this, I propose that programs $h$ take the form of auto-regressive language models. That is to say these programs read in a sequence of input tokens and for each token output a vector of real numbers with the same length as the token space. Passing that vector through a softmax induces a probability distribution over the next input token. The programs maintain their own internal state. A language should be chosen such that all generated programs can be guaranteed to take this form.</p>
<p>If the program has output probabilities $\hat{p}_1, \ldots, \hat{p}_t$ for input $x_{1:t}$ but does not halt to produce $\hat{p}_{t+1}$, then the probability $\mu_h(x_{1:n})$ for $n&gt;t$ is undefined, and the induced measure $\mu_h$ becomes a semimeasure.</p>
<h2 id="prior">Prior</h2>
<p>Weighting by program length suffices as a prior:</p>
<p>$$<br>
p(h) = 2^{-\ell(h)}<br>
$$</p>
<p>One difficulty in working with programs is long-running execution. This can make computing data probabilities take a long time. One remedy is to down-weight long-running programs in the prior. <a href="http://www.scholarpedia.org/article/Universal_search"target="_blank">Levin search</a> is an alternative to Solomonoff induction where the prior is weighted solely by runtime. We can mix both kinds of priors.</p>
<p>This is straightforward in Solomonoff induction <a href="#version-1">#Version 1</a> where each program takes no input and outputs a deterministic string. Let $p(h) = 2^{-\ell(h)} / f(\tau(h))$ where $\tau(h)$ is the total runtime of $h$, and $f$ is an increasing function that goes to infinity. For example, if $f(t) = 2^{t}$, then we have prior $2^{-\ell(h)-\tau(h)}$. If you wanted to compute $p(x)$ within some precision $\ve &gt; 0$, you can enumerate all programs $h\in\mc{H}$ by length and run them in parallel (called dovetailing). For each program, stop execution when $2^{-\ell(h)-\tau(h)} &lt; \ve$. Shorter programs will be given more runtime over longer programs. (Thank you Lance Roy for the helpful discussion about this.)</p>
<p>However, if we are using the programs I previously suggested that output data probabilities, then these programs may be fast on some inputs and slow on others. I don&rsquo;t have a solution, but a reasonable suggestion is to do some kind of heuristic analysis of the programs on some sample inputs and assign a slowness penalty in the prior.</p>
<h2 id="approximate-posterior">Approximate posterior</h2>
<p>Choosing appropriate $q_\t(h)$, a distribution over programs, is within the realm of program synthesis and machine learning. These days, program synthesis is done with neural language models on code tokens. Thus $q_\t$ is also most likely auto-regressive.</p>
<p>Can neural networks approximate the true posterior $p(h \mid x)$? This is a generalization problem. The optimized generative model on programs, $q_{\t^*}(h)$, will have been trained on finitely many programs. Whether $q_{\t^*}(h') \approx p(h' \mid x)$ for some program $h'$ unseen during training will depend entirely on the generalization properties of the particular program synthesizer that is used in $q_\t$.</p>
<h1 id="lifelong-learning">Lifelong learning</h1>
<p>Solomonoff induction is a framework for general-purpose life-long learning, which is a paradigm where an intelligent agent learns to predict it&rsquo;s future (or gain reward) from only one continuous data stream. The agent must learn on-line, and there are no independence assumptions (the data is a timeseries).</p>
<p>In the variational setup outlined above, we converted the problem of Bayesian inference into a reinforcement learning problem. At time $t$, data $x_{1:t}$ is observed, and a policy $q_{\t^*}$ on programs is trained using policy gradient methods. However, every $t$ requires its own $q_{\t_t^*}$, thus we would need to perform RL training at every timestep. One way to speed this up is to reuse policies from previous timesteps. That is to say, at time $t+1$ perform $\argmax{\t\in\T}\set{ \E_{h \sim q_\t}\left[R(h)\right] + \mb{H}_{h \sim q_\t}[q_\t(h)]}$ using Monte Carlo gradient descent starting from the previous parameter $\t_t^*$. This can be considered fine-tuning. However, this may fail to work if the posterior changes drastically between timesteps.</p>
</article><section class="article labels"><a class="tag" href=https://danabo.github.io/blog/tags/free-energy/>free energy</a></section>
</div>
<div class="article bottom"><section class="article navigation"><p><a class="link" href="https://danabo.github.io/blog/posts/active-inference-tutorial-actions/"><span class="iconfont icon-article"></span>Active inference tutorial (actions)</a></p></section></div></section><section id="footer"><div class="footer-wrap">
    <p class="copyright">©2021 Daniel Abolafia.</p>
    <p class="powerby"><span>Powered&nbsp;by&nbsp;</span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span>&nbsp;&amp;&nbsp;</span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p></div></section><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
        <script
            type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script></body>

</html>