<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.80.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>Bayesian information theory&nbsp;&ndash;&nbsp;Dan&#39;s Notepad</title><link rel="stylesheet" href="https://danabo.github.io/blog/css/core.min.663c1b913e8f8da65500ac57052b0c8dffe91fbf55bc616423803d94b150f74f29bf3c72fca9922bcbb8cd97c17e8532.css" integrity="sha384-ZjwbkT6PjaZVAKxXBSsMjf/pH79VvGFkI4A9lLFQ908pvzxy/KmSK8u4zZfBfoUy"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Bayesian information theory" /><body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="https://danabo.github.io/blog/"><span class="site name">Dan's Notepad</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="https://danabo.github.io/blog/tags/">Tags</a><a class="nav item" href=""></a><a class="nav item" href="https://zhat%2eio/"target="_blank">Zhat</a></nav></div></span></div><div class="site slogan"><span class="title">A window into my second brain</span></div></section><section id="content"><div class="article-outer">
    <div class="article-container"><section class="article header">
    <h1 class="article title">Bayesian information theory</h1><p class="article date">April 9, 2021<span class="lastmod"> â€¢ edited April 12, 2021</span></p></section><article class="article markdown-body"><p>$$<br>
\newcommand{\0}{\mathrm{false}}<br>
\newcommand{\1}{\mathrm{true}}<br>
\newcommand{\mb}{\mathbb}<br>
\newcommand{\mc}{\mathcal}<br>
\newcommand{\mf}{\mathfrak}<br>
\newcommand{\and}{\wedge}<br>
\newcommand{\or}{\vee}<br>
\newcommand{\a}{\alpha}<br>
\newcommand{\t}{\theta}<br>
\newcommand{\T}{\Theta}<br>
\newcommand{\D}{\Delta}<br>
\newcommand{\o}{\omega}<br>
\newcommand{\O}{\Omega}<br>
\newcommand{\x}{\xi}<br>
\newcommand{\z}{\zeta}<br>
\newcommand{\fa}{\forall}<br>
\newcommand{\ex}{\exists}<br>
\newcommand{\X}{\mc{X}}<br>
\newcommand{\Y}{\mc{Y}}<br>
\newcommand{\Z}{\mc{Z}}<br>
\newcommand{\P}{\Psi}<br>
\newcommand{\y}{\psi}<br>
\newcommand{\p}{\phi}<br>
\newcommand{\l}{\lambda}<br>
\newcommand{\B}{\mb{B}}<br>
\newcommand{\m}{\times}<br>
\newcommand{\E}{\mb{E}}<br>
\newcommand{\e}{\varepsilon}<br>
\newcommand{\set}[1]{\left\{#1\right\}}<br>
\newcommand{\par}[1]{\left(#1\right)}<br>
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}<br>
\newcommand{\inv}[1]{{#1}^{-1}}<br>
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}<br>
\newcommand{\dom}[2]{#1_{\mid #2}}<br>
\newcommand{\df}{\overset{\mathrm{def}}{=}}<br>
\newcommand{\M}{\mc{M}}<br>
\newcommand{\up}[1]{^{(#1)}}<br>
$$</p>
<p>$\newcommand{\H}{\Omega}$</p>
<p>Shannon&rsquo;s information theory defines quantity of information (e.g. <a href="https://en.wikipedia.org/wiki/Information_content#Definition"target="_blank">self-information</a> $-\lg p(x)$) in terms of probabilities. In the context of data compression, these probabilities are given a frequentist interpretation (Shannon makes this interpretation explicit in his <a href="http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf"target="_blank">1948 paper</a>). In 





  
    <a href="https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/">Deconstructing Bayesian Inference</a>
  


, I introduced the idea of a subjective data distribution. If quantities of information are calculated using a subjective data distribution, what is their meaning? Below I will answer this question by building from the ground up a different notion of Bayesian inference.</p>
<p>My thesis is that subjective (Bayesian) probabilities quantify non-determinism, rather than randomness (where non-determinism means that something takes on more than one value, i.e. is a set rather than a single value). Below I motivate the idea that quantity of information based on non-determinism can be interpreted as measuring the reduction in size (&ldquo;narrowing down&rdquo;) of a possibility space.</p>
<h1 id="information-and-finite-possibilities">Information and finite possibilities</h1>
<p>Following 





  
    <a href="https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/">Deconstructing Bayesian Inference</a>
  


, let&rsquo;s suppose an agent is predicting the continuation of a data string, and the agent&rsquo;s prediction is not uniquely determined, i.e. non-deterministic. I will represent the agent&rsquo;s prediction as a set of possible predictions, called the agent&rsquo;s <strong>hypothesis set</strong>.</p>
<p>Formally, the agent receives an endless stream of data $\o$ drawn from the set $\X^\infty$, where $\X$ is some finite character set. In the examples below, let&rsquo;s assume $\X = \set{0,1}$. Given some finite sequence $x\in\X^*$, a prediction is a continuation (not necessarily the correct continuation), i.e. an infinite sequence starting with prefix $x$.</p>
<p>Let $\H \subseteq \X^\infty$ be the agent&rsquo;s hypothesis set. When finite data $x$ is observed, we narrow down $\H$ to the subset of all sequences starting with $x$. This is called <strong>conditionalizing</strong> (or <strong>restriction</strong>). Denote $\dom{\H}{x} = \set{\o\in\H \mid x\sqsubset\o}$ as the subset of $\H$ consisting of sequences starting with the prefix $x$. The set $\dom{\H}{x}$ is $\H$ conditioned on $x$.</p>
<p>For example, a rigid agent that only ever predicts $0$s no matter what has the following hypothesis set:</p>
<p>$$<br>
\H = \set{0000000000\dots}\,.<br>
$$</p>
<p>Alternatively, consider:</p>
<p>$$<br>
\H = \set{0000000000\dots, 1111111111\dots}\,.<br>
$$</p>
<p>Before observing anything, the agent&rsquo;s prediction for the first timestep is not determined - it could be 0 or it could be 1. When the first bit is observed, be it a 0 or a 1, the agent&rsquo;s predictions after that become fully determined: $\dom{\H}{0} = \set{0000000000\dots}$ and $\dom{\H}{1} = \set{1111111111\dots}$.</p>
<p>Let&rsquo;s consider a more complex hypothesis set:</p>
<p>$$<br>
\begin{aligned}<br>
\H = \{&amp;0000000000\dots,<br>
\\&amp;0100000000\dots,<br>
\\&amp;1000000000\dots,<br>
\\&amp;1001111111\dots,<br>
\\&amp;1010101010\dots,<br>
\\&amp;1101100110\dots,<br>
\\&amp;1110111111\dots,<br>
\\&amp;1111111111\dots\}<br>
\end{aligned}<br>
$$</p>
<p>Here are the conditionalized sets on the shortest prefixes:</p>
<p>$$<br>
\begin{aligned}<br>
\dom{\H}{0} = \{&amp;0000000000\dots,<br>
\\&amp;0100000000\dots\}<br>
\end{aligned}<br>
$$</p>
<p>$$<br>
\begin{aligned}<br>
\dom{\H}{1}  = \{&amp;1000000000\dots,<br>
\\&amp;1001111111\dots,<br>
\\&amp;1010101010\dots,<br>
\\&amp;1101100110\dots,<br>
\\&amp;1110111111\dots,<br>
\\&amp;1111111111\dots\}<br>
\end{aligned}<br>
$$</p>
<p>$$<br>
\begin{aligned}<br>
\dom{\H}{00} &amp;= \set{0000000000\dots}\\<br>
\dom{\H}{01} &amp;= \set{0100000000\dots}<br>
\end{aligned}<br>
$$</p>
<p>$$<br>
\begin{aligned}<br>
\dom{\H}{10}  = \{&amp;1000000000\dots,<br>
\\&amp;1001111111\dots,<br>
\\&amp;1010101010\dots\}<br>
\end{aligned}<br>
$$</p>
<p>$$<br>
\begin{aligned}<br>
\dom{\H}{11}  = \{&amp;1101100110\dots,<br>
\\&amp;1110111111\dots,<br>
\\&amp;1111111111\dots\}<br>
\end{aligned}<br>
$$</p>
<p>$$<br>
\begin{aligned}<br>
\dom{\H}{100}  = \{&amp;1000000000\dots,<br>
\\&amp;1001111111\dots\}<br>
\end{aligned}<br>
$$</p>
<p>$$<br>
\dom{\H}{101}  = \{1010101010\dots\}<br>
$$</p>
<p>$$<br>
\dom{\H}{110}  = \{1101100110\dots\}<br>
$$</p>
<p>$$<br>
\begin{aligned}<br>
\dom{\H}{111}  = \{&amp;1110111111\dots,<br>
\\&amp;1111111111\dots\}<br>
\end{aligned}<br>
$$</p>
<p>There are more sequences in $\H$ starting with $1$ than with $0$, and so $\dom{H}{0}$ is a relatively smaller portion of $\H$ than $\dom{H}{1}$ is - specifically $\dom{H}{0}$ is 1/4 of $\H$ and $\dom{H}{1}$ is 3/4 of $\H$.</p>
<p>If the agent&rsquo;s goal is to make a prediction, it has incentive to narrow down as much of $\H$ as possible to reduce prediction uncertainty, prior to making the prediction. Thus, for this particular $\H$, the agent prefers to observe $0$ than $1$ as the first bit. In other words, smaller subsets $\H$ are better because they also mean $\H$ was narrowed down by a greater amount.</p>
<p>It can be easier to think in terms of maximizing the amount of &ldquo;narrowing-down&rdquo;. We can formally quantify it by counting the number of &ldquo;halvings&rdquo; of $\H$ a given subset is worth.</p>
<p>For subset $A \subseteq \H$ (still assuming finite $\H$), define the <strong>information gain</strong> of $A$ (or <strong>surprise</strong>) as:</p>
<p>$$<br>
h_{\H}(A) \df -\lg\par{\frac{\abs{A}}{\abs{\H}}}<br>
$$</p>
<p>where $\lg$ is log base 2. In general $h(A)$ is a real number, so if $n \leq h(A) &lt; n+1$, then $\abs{A}$ is smaller than $1/2^{-n}$ the size of $\H$, and no smaller than  $1/2^{-n-1}$ of $\H$. In the context of narrowing down $\H$ with data $x$, the quantity $h(\dom{\H}{x})$ tells us how many halvings of $\H$ the data $x$ gave us. When the goal is to be as certain as possible about predictions of the future (where prediction uncertainty is represented by the set $\dom{\H}{x}$), the larger $h(\dom{\H}{x})$ is the better.</p>
<p>Let&rsquo;s calculate possible information gains for the example above:</p>
<p>$$<br>
\begin{aligned}<br>
h_\H(\dom{\H}{0}) &amp;= -\lg\par{\frac{2}{8}} = 2 \\<br>
h_\H(\dom{\H}{1}) &amp;= -\lg\par{\frac{6}{8}} \approx 0.415 \\<br>
h_\H(\dom{\H}{00}) &amp;= -\lg\par{\frac{1}{8}} = 3 \\<br>
h_\H(\dom{\H}{10}) &amp;= -\lg\par{\frac{3}{8}} \approx 1.415 \\<br>
&amp;\vdots<br>
\end{aligned}<br>
$$</p>
<p>Suppose we previously observed $1$, for an information gain of about $0.415$, and reduced the remaining hypothesis set to $\dom{\H}{1}$. Then if we observe $0$, the hypothesis set is further reduced to $\dom{\H}{10}$. Going from $\H$ to $\dom{\H}{10}$ is worth a total information gain of about $1.415$, but what about the relative information gain going from $\dom{\H}{1}$ to $\dom{\H}{10}$? This quantity is called <strong>conditional information gain</strong>, defined as</p>
<p>$$<br>
\begin{aligned}<br>
h_\H(A \mid B) &amp;\df h_\H(A\cap B) - h_\H(B) \\<br>
&amp;= -\lg\par{\frac{\abs{A \cap B}}{\abs{B}}} \\<br>
&amp;= h_B(A\cap B)\,.<br>
\end{aligned}<br>
$$</p>
<p>So $h_\H(\dom{\H}{10} \mid \dom{\H}{1}) = h_\H(\dom{\H}{10}) - h_\H(\dom{\H}{1}) \approx 1.415 - 0.415 = 1$. Conditional information gain is just the information gain starting from a different hypothesis set, so $h_\H(\dom{\H}{10} \mid \dom{\H}{1}) = h_{\dom{\H}{1}}(\dom{\H}{10}) = -\lg(3/6) = 1$, which is the number of halvings it takes to get from $\dom{\H}{1}$ to $\dom{\H}{10}$.</p>
<p>The agent would like $h(\dom{\H}{x})$ to be maximized given a pre-defined $\H$, but the agent may not have any control over this quantity, unless the agent can take actions that affect what data $x$ it observes. However, if the agent gets to choose $\H$, then would the agent choose a very small set to begin with so that it does not need to be narrowed down very much?</p>
<p>There is a problem. Take again as an example the rigid agent: $\H = \set{0000000000\dots}$. If only $0$s are ever observed, then this is a great hypothesis set, because the agent will be maximally certain about its prediction of future $0$s, and the agent will be right. But suppose the agent is wrong, e.g. the agent observes data $x = 001$. Then $\dom{\H}{001} = \set{}$ is the empty set. The agent can no longer make any prediction! If we quantify this narrowing-down, we get</p>
<p>$$<br>
h_\H(\dom{\H}{001}) = -\lg\par{\frac{0}{1}} = \infty\,.<br>
$$</p>
<p>We&rsquo;ve maximized the amount of narrowing-down - it&rsquo;s infinite. But at the same time this defeats the actual goal of being maximally certain about predictions. Having an empty hypothesis set is a degenerate state. Clearly, too much information gain is bad. Is there an ideal trade-off?</p>
<h2 id="compound-hypotheses">Compound hypotheses</h2>
<p>Consider the following hypothesis set:</p>
<p>$$<br>
\begin{aligned}<br>
\H = \{&amp;0000000000\dots,<br>
\\&amp;0100000000\dots,<br>
\\&amp;0010000000\dots,<br>
\\&amp;0110000000\dots,<br>
\\&amp;1001111111\dots,<br>
\\&amp;1101111111\dots,<br>
\\&amp;1011111111\dots,<br>
\\&amp;1111111111\dots\}<br>
\end{aligned}<br>
$$</p>
<p>The first symbol in these sequences fully determines the long-run behavior of the sequences, i.e. sequences starting with 0 end with 0s, and sequences starting with 1 end with 1s. However, the 2nd and 3rd symbols are not determined by the 1st. Perhaps it would make sense to not care about predicting them. In that case, we are not so interested in narrowing down $\H$ to one sequence, as we are in narrowing down $\H$ into a particular long-run pattern.</p>
<p>$\newcommand{\h}{\mc{H}}$$\newcommand{\tr}{\rightarrowtail}$We can formally represent what we care about and don&rsquo;t care about predicting, by partitioning $\H$. In this example, suppose we make the following partition:</p>
<p>$$<br>
\begin{aligned}<br>
\mf{H} = \set{\h_1, \h_2} = \{<br>
\{&amp;0000000000\dots,<br>
\\&amp;0100000000\dots,<br>
\\&amp;0010000000\dots,<br>
\\&amp;0110000000\dots\},<br>
\\\{&amp;1001111111\dots,<br>
\\&amp;1101111111\dots,<br>
\\&amp;1011111111\dots,<br>
\\&amp;1111111111\dots\}\}<br>
\end{aligned}<br>
$$</p>
<p>$\h_1$ contains only sequences ending in 0s, and $\h_2$ contains only sequences ending in 1s.</p>
<p>In general, for a partition $\mf{H}$ of $\H$, call each $\h\in\mf{H}$ a <strong>compound hypothesis</strong>, indicating that its a set of <strong>primitive hypotheses</strong> (data continuations). As we shall see, compound hypotheses correspond closely to the hypotheses-as-data-distributions formulation which we saw in 





  
    <a href="https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/">Deconstructing Bayesian Inference</a>
  


.</p>
<p>For some partition $\mf{H}$ of $\H$, let $\dom{\mf{H}}{x} = \set{\dom{\h}{x} \mid \h \in \mf{H}}$ be the partition of $\dom{\H}{x}$ consisting of the parts in $\mf{H}$ which have each been conditionalized on $x$ (narrowed down to the sequences starting with $x$).</p>
<p>In our example, $\dom{\mf{H}}{0} = \set{\h_1, \set{}}$ and $\dom{\mf{H}}{1} = \set{\set{}, \h_2}$. Let&rsquo;s consider empty compound hypotheses to be eliminated. Then in either scenario, observing just the 1st symbol narrows down $\mf{H}$ to exactly one compound hypothesis, analogous to our original goal of narrowing down $\H$ to one hypothesis. The remaining compound hypothesis (either $\h_1$ or $\h_2$) is uncertain about what the 2nd and 3rd symbols will be, but certain about all symbols after that.</p>
<h2 id="information-gain">Information Gain</h2>
<p>Earlier, agent&rsquo;s goal was to gain maximum prediction certainty by narrowing down $\H$ as much as possible. Now with partition $\mf{H}$, the agent&rsquo;s goal is to narrow down $\mf{H}$ as much as possible, ideally reducing all of the parts but one to empty sets. However, if no compound hypothesis $\dom{\h}{x}\in\dom{\mf{H}}{x}$ is empty, is there still a sense in which the agent narrowed its compound hypotheses down? This question can be answered by considering information gain quantities.</p>
<p>$\H \tr \dom{\H}{x}$ is the total information gained, and is quantified by $h_\H(\dom{\H}{x})$. This is the number of halvings the hypothesis set $\H$ is reduced by due to observing $x$. This quantity does not depend on the choice of partition $\mf{H}$.</p>
<p>$\h \tr \dom{\h}{x}$ is the information gained within compound hypothesis $\h \in \mf{H}$, and is quantified by $h_\H(\dom{\h}{x} \mid \h) = h_\h(\dom{\h}{x})$. This is the information gained where $\h$ is treated as its own hypothesis set. This quantity only depends on the given $\h$, and not the other parts in $\mf{H}$. For each $\h\in\mf{H}$ there is an information gain $\h \tr \dom{\h}{x}$. Since we are considering $\h$ to be a set of sequences that the agent doesn&rsquo;t care about distinguishing, reductions in $\h$ are essentially wasted information gain. If we regard variation of sequences within $\h$ to be noise, then this quantity measures information gained about that noise.</p>
<p>Let $\o\in\H$ be the full data sequence, of which only a finite prefix $x \sqsubset \o$ has been observed. Call $\o$ the <strong>true hypothesis</strong>. Likewise, for partition $\mf{H}$ there is exactly one compound hypothesis $\h\in\mf{H}$ containing $\o$. Call this the <strong>true compound hypothesis</strong>.</p>
<p>Does observing $x$ tell the agent anything about which compound hypothesis $\h\in\mf{H}$ is true (contains $\o$)? Consider first the amount of information gain needed to reach certainty: $\H\tr\h$, i.e. reduce all but one compound hypotheses to the empty set (again, we don&rsquo;t care about the size of the remaining compound hypothesis). This is quantified by $h_\H(\h)$. When $x$ is observed, $\H$ becomes $\dom{\H}{x}$ and $\h$ becomes $\dom{\h}{x}$. At that point, the information gain needed to achieve certainty that the same compound hypothesis true is $\dom{\H}{x}\tr\dom{\h}{x}$, quantified by $h_{\dom{\H}{x}}(\dom{\h}{x})$.</p>
<p>We haven&rsquo;t achieved compound hypothesis certainty, but the quantity of information gain needed to do so has changed:</p>
<p>$$<br>
\D_x\up{\h} = h_\H(\h) - h_{\dom{\H}{x}}(\dom{\h}{x})\,.<br>
$$</p>
<p>Let&rsquo;s confirm that the sign is correct. If $h_{\dom{\H}{x}}(\dom{\h}{x}) &gt; h_\H(\h)$ then we need more bits to achieve $\dom{\H}{x}\tr\dom{\h}{x}$ than to achieve $\H\tr\h$, i.e. the task of narrowing down to this compound hypothesis has gotten harder. In that case, $\D_x\up{\h}$ is negative. If, on the other hand, $h_\H(\h) &gt; h_{\dom{\H}{x}}(\dom{\h}{x})$ then this compound hypothesis has become a larger portion of the remaining $\dom{\H}{x}$ than it was before observing $x$, so the task of narrowing down to this compound hypothesis has gotten easier. In that case, $\D_x\up{\h}$ is positive.</p>
<p>

  <img src="https://danabo.github.io/blog/Pasted%20image%2020210409163721.png" alt="">
<br>
Note that $\D_x\up{\h}$ is upper bounded. If the agent has succeeded in ruling out all other compound hypotheses so that $\dom{\h}{x} = \dom{\H}{x}$, then $h_{\dom{\H}{x}}(\dom{\h}{x}) = 0$ and $\D_x\up{\h} = h_\H(\h)$, which is the maximum amount of information that can be gained about whether $\h$ is true.</p>
<p>If $\h$ is known to be false, i.e. $\dom{\h}{x} = \set{}$, then $h_{\dom{\H}{x}}(\dom{\h}{x}) = \infty$ and so $\D_x\up{\h} = -\infty$. Thus $\D_x\up{\h}$ is not lower bounded, i.e. there is a maximum amount of information to be gained about whether a compound hypothesis is true, but an infinite amount of information to lose. For example, if $\h$ is true but $x$ is very misleading, then $\D_x\up{\h}$ will be very negative. In the long run, if the agent observes enough data, $\o_{1:n}$ for large $n$, then $\D_{\o_{1:n}}\up{\h}$ will go up and eventually converge to $h_\H(\h)$. The misleading initial data caused the agent to lose information, in the sense that even more information needs to be gained to achieve the same certainty that $\h$ is true.</p>
<p>Also note that $\D_x\up{\h}$ is a total change given the entire data sequence $x$. We can also quantify the change due to a single timestep $x_n$:</p>
<p>$$<br>
\D_{x_n}\up{\dom{\h}{x_{&lt;n}}} = h_{\dom{\H}{x_{&lt;n}}}(\dom{\h}{x_{&lt;n}}) - h_{\dom{\H}{x_{1:n}}}(\dom{\h}{x_{1:n}})\,.<br>
$$</p>
<p>Then the total change is the sum of changes for each timestep:</p>
<p>$$<br>
\D_x\up{\h} = \sum_{i=1}^\abs{x} \D_{x_i}\up{\dom{\h}{x_{&lt;i}}}\,.<br>
$$</p>
<p>It may even be of some interest to plot incremental information gains over time for each compound hypothesis $\h$. What we will observe is that in the long run is that $\D_x\up{\h}$ asymptotically converges to $h_\H(\h)$ for the true compound hypothesis $\h$, and the incremental change $\D_{x_n}\up{\dom{\h}{x_{&lt;n}}}$ gets smaller and smaller over time for that same $\h$. Meanwhile, for all other compound hypotheses $\h'$ (the false ones), $\D_x\up{\h'}$ diverges to $-\infty$ and the incremental change $\D_{x_n}\up{\dom{\h'}{x_{&lt;n}}}$ goes negative and grows larger and larger in magnitude. For &ldquo;misleading&rdquo; data, the short-run behavior of these plots may be oscillatory before long-run behavior takes over.</p>
<h3 id="useful-identities">Useful Identities</h3>
<p>Doing some algebraic manipulation, we get:</p>
<p>$$<br>
\begin{aligned}<br>
\D_x\up{\h} &amp;= h_\H(\h) - h_{\dom{\H}{x}}(\dom{\h}{x}) \\<br>
&amp;= -\lg\par{\frac{\abs{\h}}{\abs{\H}}} + \lg\par{\frac{\abs{\dom{\h}{x}}}{\abs{\dom{\H}{x}}}} \\<br>
&amp;= \lg\par{\frac{\abs{\dom{\h}{x}}/\abs{\dom{\H}{x}}}{\abs{\h}/\abs{\H}}} \\<br>
&amp;= \lg\par{\frac{f_x\up{\h}}{f\up{\h}}}\,,<br>
\end{aligned}<br>
$$</p>
<p>where $f\up{\h} = \abs{\h}/\abs{\H}$ is the fraction of predictions that $\h$ takes up, and $f_x\up{\h} = \abs{\dom{\h}{x}}/\abs{\dom{\H}{x}}$ is that fraction after $x$ is observed. This gives us an interpretation for the quantity $\D_x\up{\h}$: the number of doublings it takes to go from $f\up{\h}$ to $f_x\up{\h}$. E.g. if $\h$ is a quarter the size of $\H$ and $\dom{\h}{x}$ is half the size of $\dom{\H}{x}$, then $\D_x\up{\h} = \lg\frac{1/2}{1/4} = \lg 2 = 1$. That means the agent has one less bit to gain about whether $\dom{\h}{x}$ is true, and has in that sense gained one bit of information.</p>
<p>Doing even more algebra, we get another useful identity:</p>
<p>$$<br>
\begin{aligned}<br>
\D_x\up{\h} &amp;= \lg\par{\frac{\abs{\dom{\h}{x}}/\abs{\dom{\H}{x}}}{\abs{\h}/\abs{\H}}} \\<br>
&amp;= -\lg\par{\frac{\abs{\dom{\H}{x}}}{\abs{\H}}} + \lg\par{\frac{\abs{\dom{\h}{x}}}{\abs{\h}}} \\<br>
&amp;= h_\H(\dom{\H}{x}) - h_\h(\dom{\h}{x})<br>
\end{aligned}<br>
$$</p>
<p>Thus, $h_\H(\dom{\H}{x}) = h_\h(\dom{\h}{x}) + \D_x\up{\h}$, for all $\h\in\mf{H}$. That is to say, the total information gain $h_\H(\dom{\H}{x})$ can be decomposed as the sum of information gained within a given compound hypothesis $\h$ (information gained about noise, i.e. what we don&rsquo;t care about predicting), plus the information gained about whether $\h$ is true. Total info gain $h_\H(\dom{\H}{x})$ decomposes similarly for every $\h\in\mf{H}$.</p>
<h2 id="other-information-quantities">Other Information Quantities</h2>
<p>See my <a href="http://zhat.io/articles/primer-shannon-information"target="_blank">primer to Shannon&rsquo;s information theory</a> for more intuition about the interpretation of these quantities, and specifically the sections on<br>
<a href="http://zhat.io/articles/primer-shannon-information#mutual-information"target="_blank">mutual information</a> and <a href="http://zhat.io/articles/primer-shannon-information#entropy"target="_blank">entropy</a>.</p>
<p>For sets $A,B\subseteq\H$, the quantity $i_\H(A, B)$ is called the <a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information"target="_blank"><strong>pointwise mutual information</strong></a> (PMI) between $A$ and $B$, defined by</p>
<p>$$<br>
\begin{aligned}<br>
i_\H(A,B) &amp;\df h_\H(A) - h_\O(A \mid B) \\<br>
&amp;= -\lg\par{\frac{\abs{A}}{\abs{\H}}} + \lg\par{\frac{\abs{A\cap B}}{\abs{B}}} \\<br>
&amp;= \lg\par{\frac{\abs{A\cap B}}{\abs{A}\abs{B}}\abs{\H}} \\<br>
&amp;= \lg\par{\frac{\abs{A\cap B}}{\abs{A}\abs{B}}} - \lg\par{\frac{1}{\abs{\H}}}\,.<br>
\end{aligned}<br>
$$</p>
<p>Note that PMI is symmetric, so $i_\H(A, B) = i_\H(B, A)$.</p>
<p>Notice that $\D_x\up{\h} = i_\H(\h, \dom{\H}{x})$. Using our intuition about narrowing down compound hypotheses from before, we can interpret the meaning of $i_\H(A, B)$ in general.</p>
<p>Let</p>
<p>$$<br>
\frac{\H \tr \H'}{U \tr U'}<br>
$$</p>
<p>denote the statement &ldquo;$\H$ is narrowed down to $\H'$ while $U$ is narrowed down to $U'$&rdquo;, with the assumption that $U \subseteq \H$ and $U'\subseteq\H'$. So the idea of gaining information about whether compound hypothesis $\h$ is true can be written succinctly as $\frac{\H \tr \dom{\H}{x}}{\h \tr \dom{\h}{x}}$.</p>
<p>In general, $i_\H(A, B)$ quantifies $\frac{\H \tr B}{A \tr (A \cap B)}$, and is the number of doublings achieved from fraction $f = \abs{A}/\abs{\H}$ to fraction $f'=\abs{A\cap B}/\abs{B}$, i.e. $\lg(f'/f)$.</p>
<p>

  <img src="https://danabo.github.io/blog/Pasted%20image%2020210409163741.png" alt="">
<br>


  <img src="https://danabo.github.io/blog/Pasted%20image%2020210409163757.png" alt="">
</p>
<h3 id="entropy">Entropy</h3>
<p>It is useful to have a single quantity representing the state of $\dom{\mf{H}}{x}$.</p>
<p>Let $\mf{A}$ be some partition of $\H$. Define <strong>entropy</strong></p>
<p>$$<br>
\mb{H}(\mf{A}) \df \sum_{A\in\mf{A}} \frac{\abs{A}}{\abs{\H}} h_\H(A)\,.<br>
$$</p>
<p>This quantity doesn&rsquo;t require $\O$ to be explicitly specified because it determined by the argument, i.e. $\O = \bigcup \mf{A}$.</p>
<p>Let $\mf{H}$ be a compound hypothesis set. Then $\mb{H}(\mf{H})$ quantifies roughly how much of a difference there is in information that can be gained about whether each compound hypothesis $\h\in\mf{H}$ is true. Specifically, it is the expected information gain across $\mf{H}$, though expectations don&rsquo;t have the same meaning here because these &ldquo;probabilities&rdquo; don&rsquo;t denote randomness. $\mb{H}(\mf{H})$ is maximized if $h_\H(\h) = h_\H(\h')$ for all $\h,\h'\in\mf{H}$, and $\mb{H}(\mf{H})$ is 0 if one $\h\in\mf{H}$ is non-empty while all other compound hypotheses are empty. If we observe $x$, then high $\mb{H}(\dom{\mf{H}}{x})$ indicates high uncertainty about which compound hypothesis is true, and small $\mb{H}(\dom{\mf{H}}{x})$ indicates high certainty about which compound hypothesis is true.</p>
<h3 id="mutual-information">Mutual Information</h3>
<p>Let $\mf{A}$ and $\mf{B}$ be partitions of $\H$. Define <strong>mutual information</strong> (MI)</p>
<p>$$<br>
\mb{I}(\mf{A}, \mf{B}) \df \sum_{A\in\mf{A}}\sum_{B\in\mf{B}} \frac{\abs{A\cap B}}{\abs{\H}} i_\H(A, B)\,.<br>
$$</p>
<p>This is the expected pointwise mutual information between the two partitions, which quantifies roughly how much redundancy there is between them.</p>
<p>MI plays the following role in Bayesian information gain: Let $\mf{X}_n = \set{\dom{\H}{x} \mid x\in\X^n}$ be the partition of $\H$ consisting of all possible conditionalizations $\dom{\H}{x}$ for each possible length-$n$ data sequence $x\in\X^n$. Then $\mb{I}(\mf{H}, \mf{X}_n)$ quantifies how much information could be gained (in expectation) about which compound hypothesis is true given a (not yet observed) length-$n$ data sequence. If MI is minimized, $\mb{I}(\mf{H}, \mf{X}_n)=0$, then we expect that $\D_x\up{\h} = i_\H(\h, \dom{\H}{x})=0$ for $\h\in\mf{H}$ and $\dom{\H}{x}\in\mf{X}_n$. This would indicate that the partitions $\mf{H}$ and $\mf{X}_n$ are orthogonal, in a sense. On the other hand, MI is maximized when $\mb{I}(\mf{H}, \mf{X}_n)=\min\set{\mb{H}(\mf{H}), \mb{H}(\mf{X}_n)}$, and indicates that each $x\in\X^n$ will narrow down some $\h\in\mf{H}$ to empty sets (with either one remaining compound hypothesis which is narrowed down, or multiple remaining compound hypotheses which are not narrowed down at all), i.e. the partitions are parallel, in a sense.</p>
<h1 id="infinite-possibilities">Infinite possibilities</h1>
<p>In practice agents would want to have a large enough hypothesis set to be able to make predictions in all circumstances. That is to say, given any finite observation $x\in\X^*$, it is desirable for at least one sequence $\o\in\H$ to begin with $x$, denoted $x \sqsubset \o$. Then, $\dom{\H}{x}$ is non-empty for all $x$ and so the agent always has at least one prediction to make.</p>
<p>Clearly such an $\H$ cannot be finite because $\X^*$ is infinite, i.e. there is at least one $\o\in\H$ for every $x\in\X^*$ (Simple proof: Suppose $\H$ were finite. Construct finite $x$ s.t. $x\not\sqsubset\o$ for all $\o\in\H$).  (Note that such an $\H$ need not be equal to $\X^\infty$. For example, $\H = \set{x`00000\dots \mid x\in\X^*}$ where $x`00000\dots$ is $x$ appended with infinite $0$s. This $\H$ does not include sequences with other limiting behavior, e.g. the binary digits of Pi.)</p>
<p>However, $\H$ can be too big. Suppose $\H=\X^\infty$. Then $\dom{\H}{x} = \X^\infty$ for all $x\in\X^*$. This hypothesis set can never be narrowed down to anything, i.e. there is never information gain. An agent with this hypothesis set remains maximally uncertain always, and so it is not useful.</p>
<p>Even if $\H$ is a strict subset of $\X^\infty$ but contains every finite data sequence (so that $\dom{\H}{x}$ is non-empty for all $x$), it&rsquo;s usefulness is still dubious. In general $\abs{\dom{\H}{x}}=\infty$ for all $x$ (the cardinality of $\dom{\H}{x}$ is always infinite; disregarding different sizes of infinity), so we cannot compare to what extent we&rsquo;ve narrowed down $\H$ further by observing $x$ rather than $y$. That is to say, $\abs{\dom{\H}{x}}/\abs{\H} = \infty/\infty$ is indeterminate. Furthermore, $\dom{\H}{x}$ will contain every finite data sequence starting with $x$, so there is no tangible sense in which the agent&rsquo;s predictions at finite time are narrowed down.</p>
<p>The problem of measuring narrowing-down of infinite possibility sets is resolved by choosing a measure $\mu$ on $\H$. However a new problem arises: how to choose $\mu$. My purpose in presenting Bayesian information theory as narrowing down possibility spaces is to give meaning to probability values. Now, we&rsquo;ve reintroduced an arbitrary measure $\mu$.</p>
<p>There is a sort of middle ground that also serves as a bridge between the usual probabilistic conception of Bayesian inference I introduced in 





  
    <a href="https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/">Deconstructing Bayesian Inference</a>
  


 and the hypothesis set conception, via algorithmic information theory.</p>
<h2 id="shannon-equivalence">Shannon Equivalence</h2>
<p>I defined the quantities of information above using the set cardinality function $\abs{\cdot}$ to measure the sizes of sets (called the <a href="https://en.wikipedia.org/wiki/Counting_measure"target="_blank">counting measure</a>). In general, the size of a set can be defined with a <strong>measure</strong>, which is a function from subsets to non-negative real numbers. So a measure $\mu$ on $\H$ has the type signature $\mu : 2^\H \to \mb{R}_{\geq 0}$ (though technically we need to restrict ourselves to <em>measurable</em> subsets of $\H$, see my <a href="http://zhat.io/articles/primer-probability-theory#primer-to-measure-theory"target="_blank">primer to measure theory</a> for details). Furthermore, if we choose measure $\mu$ s.t. $\mu(\H) = 1$, then $\mu$ is called a <strong>probability measure</strong> (or a <strong>normalized measure</strong>). See my <a href="http://zhat.io/articles/primer-probability-theory#definitions"target="_blank">primer to probability theory</a> for details.</p>
<p>If we replace $\abs{\cdot}$ with probability measure $\mu(\cdot)$ everywhere in the quantities of information defined above, then we get the usual Shannon definitions:</p>
<ul>
<li>$h_\mu(A) = -\lg \mu(A)$</li>
<li>$h_\mu(A \mid B) = -\lg \par{\frac{\mu(A \cap B)}{\mu(B)}}$</li>
<li>$i_\mu(A, B) = \lg\par{\frac{\mu(A\cap B)}{\mu(A)\mu(B)}}$</li>
<li>$\mb{H}_\mu(\mf{A}) = \sum_{A\in\mf{A}} \mu(A) h_\mu(A)$</li>
<li>$\mb{H}_\mu(\mf{A}\mid B) = \sum_{A\in\mf{A}} \mu(A\mid B) h_\mu(A\mid B)$</li>
<li>$\mb{H}_\mu(\mf{A}\mid\mf{B}) = -\sum_{A\in\mf{A}}\sum_{B\in\mf{B}} \mu(A\cap B) h_\mu(A\mid B)$</li>
<li>$\mb{I}_\mu(\mf{A}, \mf{B}) = \sum_{A\in\mf{A}}\sum_{B\in\mf{B}} \mu(A\cap B) i_\mu(A, B)$</li>
</ul>
<p>where $A,B\subseteq\H$ and $\mf{A},\mf{B}$ are two partitions of $\H$. Note that $\abs{\H}$ disappears because it becomes $\mu(\H) = 1$.</p>
<h2 id="algorithmic-randomness">Algorithmic Randomness</h2>
<p>In 





  
    <a href="https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/">Deconstructing Bayesian Inference</a>
  


, I defined Bayesian inference as a manipulation of probability measures on $\X^\infty$. Given a set $\M$ of measures $\mu$ on $\X^\infty$, and a prior $p$ on $\M$, what is the corresponding &ldquo;possibility space narrowing-down&rdquo; perspective?</p>
<p>I supposed that hypothesis probabilities $\mu(\o_{1:n})$ for $\mu\in\M$ represented irreducible uncertainty about the world, whereas subjective probabilities $p(\o_{1:n})$ represented a mixture of reducible and irreducible uncertainty, where knowing which hypothesis $\mu$ is true maximally reduces your uncertainty.</p>
<p>I also introduced a distinction between randomness and non-determinism. Randomness can be defined as incompressibility, and non-determinism refers to the output of a mathematical construct (such as a function) being not uniquely determined by givens (such as an input). Probabilities can measure both, which can lead to confusion.</p>
<p>Let&rsquo;s suppose here that hypothesis probabilities always measure randomness. Assuming $\mu\in\M$ is computable, we can precisely define what it means for an infinite sequence $\o\in\X^\infty$ to &ldquo;look like&rdquo; a typical sequence randomly drawn from $\mu$. Call such &ldquo;typical looking&rdquo; sequences <strong>$\mu$-typical</strong> (or <strong>$\mu$-random</strong>). Formally, $\o\in\X^\infty$ is $\mu$-typical iff the optimal compression rate of $\o$ (via monotone algorithmic complexity $Km$) is achieved with arithmetic coding using $\mu$. (See <a href="https://www.springer.com/gp/book/9781489984456"target="_blank">Li &amp; VitÃ¡nyi</a>, 3rd edition, theorem 4.5.3 on page 318, which states, $\sup_n\left\{\lg(1/\mu(\o_{1:n})) - Km(\o_{1:n})\right\} &lt; \infty$.)</p>
<p>Let $\h\up{\mu}\subseteq\X^\infty$ be the set of all infinite sequences which are $\mu$-typical, called the <strong>$\mu$-typical set</strong>. The $\mu$-probability of drawing a $\mu$-typical sequence is 1, i.e. $\mu(\h\up{\mu})=1$, and the $\mu$-probability of drawing a $\mu$-atypical sequence is 0. You can think of $\h\up{\mu}$ as $\X^\infty$ with a $\mu$-measure 0 subset subtracted from it (specifically $\h\up{\mu}$ is the smallest constructable $\mu$-measure 1 subset of $\X^\infty$).</p>
<p>The prior $p$ on $\M$ induces a subjective data distribution: $p(\o_{1:n}) = \sum_{\mu\in\M} p(\mu)\mu(\o_{1:n})$. So long as $p$ is computable, there is a $p$-typical set $\h\up{p}$.</p>
<p>Let $\H = \h\up{p}$ and $\mf{H} = \set{\h\up{\mu} \mid \mu\in\M}$, where $\H$ is an agent&rsquo;s hypothesis set (set of sequence predictions) and $\mf{H}$ is a set of compound hypotheses, where $\bigcup \mf{H} = \H$. I&rsquo;m dropping the requirement that $\mf{H}$ be a proper partition of $\O$ (i.e. all $\h\in\mf{H}$ are mutually disjoint), in which case we call $\mf{H}$ a cover of $\O$.</p>
<p>Now, Bayesian inference with hypothesis set (of measures) $\M$ and prior $p$, and Shannon quantities using these measures, corresponds to Bayesian inference with $\H$ and $\mf{H}$ as I outlined above. The restriction $\dom{\H}{x}$ is the typical set for the conditional measure $p(\cdot \mid x)$, and likewise for $\h\up{\mu}\in\mf{H}$, the restriction $\dom{\h\up{\mu}}{x}$ is the typical set for the conditional measure $\mu(\cdot \mid x)$.</p>
<p>The prior $p(\mu)$ is simply the relative size of $\h\up{\mu}$ within $\H$, given by $p(\h\up{\mu})$. The prior encodes how much information we would gain if all other hypotheses were ruled out: $\H\tr\h\up{\mu}$, quantified by $h_p(\h\up{\mu}) = -\lg p(\h\up{\mu})$.</p>
<p>The posterior $p(\mu\mid x)$ is simply the relative size of $\dom{\h\up{\mu}}{x}$ within $\dom{\H}{x}$, given by $p(\dom{\h\up{\mu}}{x}) = p(\h\up{\mu} \cap \dom{\H}{x})$. The posterior encodes how much information we would gain if all other hypotheses were ruled out (after observing $x$): $\dom{\H}{x}\tr\dom{\h\up{\mu}}{x}$, quantified by $h_p(\h\up{\mu} \mid \dom{\H}{x}) = -\lg p(\dom{\h\up{\mu}}{x})/p(\dom{\H}{x})$.</p>
<p>Furthermore, $\H\tr\dom{\H}{x}$ is quantified by $h_p(\dom{\H}{x}) = -\lg p(\dom{\H}{x}) = -\lg p(x)$, and $\h\up{\mu}\tr\dom{\h\up{\mu}}{x}$ is quantified by $h_\mu(\dom{\h\up{\mu}}{x}) = -\lg \mu(\dom{\h\up{\mu}}{x}) = -\lg \mu(x)$.</p>
<p>The mysterious &ldquo;information gained about whether hypothesis $\mu$ is true&rdquo; becomes $\frac{\H \tr \dom{\H}{x}}{\h\up{\mu} \tr \dom{\h\up{\mu}}{x}}$, quantified by $i_p(\h\up{\mu}, \dom{\H}{x}) = \lg\frac{p(\dom{\h\up{\mu}}{x})}{p(\h\up{\mu})p(\dom{\H}{x})} = \lg\frac{p(\mu,x)}{p(\mu)p(x)}$ which is the pointwise mutual information between hypothesis $\mu$ and data $x$.</p>
<p>Finally, the information gained about which hypothesis is true is summarized by the quantity</p>
<p>$$\mb{I}_p(\mf{H}, \mf{X}_\abs{x}) = \sum_{\h\up{\mu}\in\mf{H}}\sum_{\dom{\H}{x}} \mu(\dom{\h}{x}) i_p(\h\up{\mu}, \dom{\H}{x}) = \mb{E}_p\left[i(H,X_{1:\abs{x}})\right] = I(H, X_{1:\abs{x}})\,,$$</p>
<p>where $H$ is the random variable corresponding to choice of hypothesis $\mu$ sampled from $p(\mu)$ and $X_{1:\abs{x}}$ is the random variable corresponding to choice of length-$\abs{x}$ data sampled from $p(x)$. This quantity is sometimes called <strong>Bayesian surprise</strong> (see <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2860069/"target="_blank">ref 1</a> and <a href="https://www.fil.ion.ucl.ac.uk/~karl/The%20free-energy%20principle%20A%20unified%20brain%20theory.pdf"target="_blank">ref 2</a>), though more commonly &ldquo;surprise&rdquo; refers to the total info gain $h_\mu(\dom{\H}{x})$ (<a href="https://en.wikipedia.org/wiki/Information_content"target="_blank">ref 1</a>, <a href="https://psyarxiv.com/b4jm6/"target="_blank">ref 2</a>).</p>
<h3 id="posterior-consistency">Posterior consistency</h3>
<p>When $\mf{H}$ is a strict partition, this is a special (and desirable) case in Bayesian inference, called <strong>posterior consistency</strong>. Informally, posterior consistency is when posterior probabilities converge to one-hot (or Dirac delta) distributions (almost surely). Posterior consistency is a property of a particular set of hypotheses $\M$, and is invariant to prior probabilities (assuming suppose on $\M$).</p>
<p>If $\M$ has the posterior consistency property, then the corresponding $\mf{H}$ will be <em>nearly</em> a partition, where $\mu(\h\up{\mu}\cap\h\up{\nu}))=\nu(\h\up{\mu}\cap\h\up{\nu}))=0$, for $\mu,\nu\in\M$.</p>
<p>Posterior inconsistency in $\M$ corresponds to $\mf{H}$ which has overlapping compound hypotheses (of non-zero measure).</p>
<p>Why do we want $\mf{H}$ to be a partition? So then information gained about one hypothesis $\mu$, corresponding to compound hypothesis $\h\up{\mu}$, necessarily implies information loss about all other hypotheses. If compound hypotheses overlap, then you can become more certain about multiple hypotheses at the same time, and in the infinite data limit, many hypotheses may remain (and we have not achieved our goal of prediction certainty by narrowing down to one hypothesis).</p>
<h3 id="posterior-convergence">Posterior convergence</h3>
<p>For any typical set $\h\up{\mu}$, there are actually infinitely many computable measures with the same typical set. Why is that? For any $\mu$, a new measure $\mu'$ can be constructed that assigns different probabilities than $\mu$ to finite sequences, e.g. $\mu'(\o_{1:n}) \neq \mu(\o_{1:n})$, while preserving the limiting compression rates:</p>
<p>$$\lim_{n\to\infty} \frac{1}{n}\lg\par{\frac{1}{\mu(\o_{1:n})}} = \lim_{n\to\infty} \frac{1}{n}\lg\par{\frac{1}{\mu'(\o_{1:n})}}$$</p>
<p>A difference in probabilities of finite sequence $\o_{1:n}$ corresponds to a constant offset in compression length (according to Shannon):</p>
<p>$$\abs{\lg\par{\frac{1}{\mu(\o_{1:n})}} - \lg\par{\frac{1}{\mu'(\o_{1:n})}}} = C &lt; \infty$$</p>
<p>Any finite difference becomes negligible as $n\to\infty$. $\mu$ is said to have <strong>posterior convergence</strong> to $\mu'$.</p>
<p>This is a strange predicament, because it implies that there are infinitely many probability measures that essentially encode the same randomness. That is to say, the measurement of randomness of finite sequences is not uniquely determined. This actually falls in line with the results of algorithmic information theory, where optimal compression length (Kolmogorov complexity) depends on choice of programming language (universal Turing machine), and is also arbitrary for that reason.</p>
<p>It is the infinite sequences which have a unique quantity of randomness, in a sense. Non-computable infinite sequences have a compressed length that is also infinite, but often a finite compression rate: $\limsup_{n\to\infty}\frac{Km(\o_{1:n})}{n}$. Moreover, $\o$ has a unique limiting data posterior, i.e.</p>
<p>$$<br>
\lim_{n\to\infty} \mu(\o_n \mid \o_{&lt;n}) - \mu'(\o_n \mid \o_{&lt;n}) = 0\,,<br>
$$</p>
<p>(almost surely), for any two measures $\mu$ and $\mu'$ which have the same typical set $\h\up{\mu}$. Solomonoff&rsquo;s universal data distribution $\xi$ (the mixture of all semicomputable semimeasures, see 





  
    <a href="https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/#solomonoff-induction">Deconstructing Bayesian Inference#solomonoff-induction</a>
  


) has posterior convergence to all such $\mu$. In fact, Solomonoff&rsquo;s mixture can be used as a test for whether $\o$ is $\mu$-typical by observing if $\lim_{n\to\infty} \xi(\o_n \mid \o_{&lt;n}) - \mu(\o_n \mid \o_{&lt;n}) = 0$ (almost surely).</p>
<p>The takeaway here is that if compound hypothesis $\h$ is the typical set for some measure $\mu$, then if we don&rsquo;t specify any particular measure, the limiting information gain $h_\H(\dom{\h}{x&rsquo;y} \mid \dom{\h}{x})$ as $\abs{x}\to\infty$ converges to something unique. So if we have infinite hypothesis sets and don&rsquo;t want to arbitrarily choose a measure, not all hope is lost.</p>
<h1 id="optimal-compression">Optimal Compression</h1>
<p>As we&rsquo;ve seen, Bayesian information theory is mathematically equivalent to Shannon&rsquo;s information theory, where a probability measure $\mu$ is used to measure the sizes of hypothesis sets (sets of predictions).</p>
<p>However, what is the connection between narrowing down hypothesis sets and optimal compression? Given probability measure $\mu$ on $\H$, the $\mu$-probability of finite observation $x\in\X^*$ is $\mu(\dom{\H}{x})$. (We can abuse notation and write $\mu(x)$ where $x$ is shorthand for $\dom{\H}{x}$ when given as the argument to $\mu$.) Then is $h_\mu(\dom{\H}{x})$ the optimal compressed length of $x$?</p>
<p>For finite strings, optimal compression isn&rsquo;t a well defined notion. According to algorithmic information theory, we use the shortest program that outputs $x$ as the compressed representation of $x$, but the length of that shortest program depends on our arbitrary choice of programming language. We can achieve an encoded length of approximately $h_\mu(\dom{\H}{x})$ by using <a href="https://en.wikipedia.org/wiki/Arithmetic_coding"target="_blank">arithmetic coding</a>, with $\mu$ as the provided measure (ignoring the length of the arithmetic decoder program itself).</p>
<p>Shannon&rsquo;s information theory operates in the domain of random data, and provides optimal code lengths <em>in expectation</em>. In the Bayesian information theory I&rsquo;ve outlined above, we are not working with randomness, but non-determinism (the agent&rsquo;s predictions are not uniquely determined). However, as the data length goes to infinity, these two conceptions of probability become intertwined.</p>
<p>Let $\o\in\X^\infty$ be an infinite sequence, and an agent observed the finite prefix $\o_{1:n}$. If the agent has a hypothesis set $\H$ with probability measure $\mu$, the agent can use arithmetic coding w.r.t. $\mu$ to achieve compressed length $-\lg \mu(\o_{1:n})$, and limiting compression rate</p>
<p>$$<br>
\limsup_{n\to\infty} \frac{1}{n}\lg\par{\frac{1}{\mu(\o_{1:n})}}\,.<br>
$$</p>
<p>If $\o$ is $\mu$-typical, than this will be the optimal compression rate achievable (according to algorithmic information theory). However, if $\o$ is not $\mu$-typical then the agent&rsquo;s compression rate will be worse than the optimum.</p>
<p>Let $\X = \set{0,1}$. For each bit of data $\o_n$, the approximate compressed length of that bit is $-\lg\mu(\o_n \mid \o_{&lt;n})$. So if the agent gains more than 1 bit of information from $\o_n$, arithmetic coding w.r.t. $\mu$ will actually assign more than one bit to $\o_n$ in the compressed representation. If the agent&rsquo;s info gain remains high in the long run, this &ldquo;compression&rdquo; of $\o$ will end up being longer than $\o$ itself (specifically, the compression of $\o_{1:n}$ will be longer than $n$ as $n\to\infty$). This gives us a precise sense about whether the agent is doing a good job at predicting the part of $\o$ that can be predicted: If the agent&rsquo;s $\mu$-compression rate is better than the length of the data itself then the agent is predicting the data at least better than random.</p>
<p>A mixture distribution can be viewed as a hedge against bad compression. Suppose $\o$ is not $\mu$-typical. If instead of using $\mu$ to predict $\o$, we had a set of distributions $\M$ of which $\mu$ is a member, and we use the mixture $p = \sum_{\nu\in\M} w_\nu \nu$ to predict $\o$. If $\o$ is typical w.r.t. at least one $\nu\in\M$, then $\o$ is also $p$-typical. The difference between using $\nu$ and $p$ to compress $\o$ is</p>
<p>$$<br>
\limsup_{n\to\infty} \lg\par{\frac{1}{p(\o_{1:n})}} - \lg\par{\frac{1}{\nu(\o_{1:n})}} \,,<br>
$$</p>
<p>which is a constant cost in compression length, and becomes negligible in the long run as $n\to\infty$, i.e. the compression rate using $p$ and $\nu$ is the same. So using a mixture to compress $\o$ may incur additional cost (extra bits) initially, in the long run it is no worse than using the &ldquo;true&rdquo; hypothesis $\nu$ (there may be more than one &ldquo;true&rdquo; hypothesis in $\M$). A good strategy is then to make $\M$ as large as possible. This is the premise behind Solomonoff induction, where $\M$ is the set of all semicomputable semimeasures.</p>
</article><section class="article labels"><a class="tag" href=https://danabo.github.io/blog/tags/epistemology/>epistemology</a><a class="tag" href=https://danabo.github.io/blog/tags/information/>information</a></section>
</div><nav id="TableOfContents">
  <ol>
    <li><a href="#information-and-finite-possibilities">Information and finite possibilities</a>
      <ol>
        <li><a href="#compound-hypotheses">Compound hypotheses</a></li>
        <li><a href="#information-gain">Information Gain</a>
          <ol>
            <li><a href="#useful-identities">Useful Identities</a></li>
          </ol>
        </li>
        <li><a href="#other-information-quantities">Other Information Quantities</a>
          <ol>
            <li><a href="#entropy">Entropy</a></li>
            <li><a href="#mutual-information">Mutual Information</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#infinite-possibilities">Infinite possibilities</a>
      <ol>
        <li><a href="#shannon-equivalence">Shannon Equivalence</a></li>
        <li><a href="#algorithmic-randomness">Algorithmic Randomness</a>
          <ol>
            <li><a href="#posterior-consistency">Posterior consistency</a></li>
            <li><a href="#posterior-convergence">Posterior convergence</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#optimal-compression">Optimal Compression</a></li>
  </ol>
</nav></div>
<div class="article bottom"><section class="article navigation"><p><a class="link" href="https://danabo.github.io/blog/posts/modular-neural-networks/"><span class="iconfont icon-article"></span>Modular Neural Networks</a></p><p><a class="link" href="https://danabo.github.io/blog/posts/deconstructing-bayesian-inference/"><span class="iconfont icon-article"></span>Deconstructing Bayesian Inference</a></p></section></div></section><section id="footer"><div class="footer-wrap">
    <p class="copyright">Â©2021 Daniel Abolafia.</p>
    <p class="powerby"><span>Powered&nbsp;by&nbsp;</span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span>&nbsp;&amp;&nbsp;</span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p></div></section><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ 
                tex2jax: { 
                    inlineMath: [['$','$'], ['\\(','\\)']] 
                },

                "HTML-CSS": {
                    preferredFont: "TeX",
                    availableFonts: ["TeX"]
                }
            });
        </script></body>

</html>