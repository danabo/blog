<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.80.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>Physical Information&nbsp;&ndash;&nbsp;Dan&#39;s Notepad</title><link rel="stylesheet" href="https://danabo.github.io/blog/css/core.min.ca865499b26624a6a7b7e7c6a09b8ef4db427d2fe9ad2ca79f6ba8b23433dbbb302163fdcbf2d6c0dbb66e7472f15ff1.css" integrity="sha384-yoZUmbJmJKant&#43;fGoJuO9NtCfS/prSynn2uosjQz27swIWP9y/LWwNu2bnRy8V/x"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Physical Information" /><body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="https://danabo.github.io/blog/"><span class="site name">Dan's Notepad</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="https://danabo.github.io/blog/tags/">Tags</a><a class="nav item" href=""></a><a class="nav item" href="https://zhat%2eio/"target="_blank">Zhat</a></nav></div></span></div><div class="site slogan"><span class="title">A window into my second brain</span></div></section><section id="content"><div class="article-outer">
    <div class="article-container"><section class="article header">
    <h1 class="article title">Physical Information</h1><p class="article date">May 14, 2021<span class="lastmod"> â€¢ edited May 17, 2021</span></p></section><article class="article markdown-body"><p>$$<br>
\newcommand{\0}{\mathrm{false}}<br>
\newcommand{\1}{\mathrm{true}}<br>
\newcommand{\mb}{\mathbb}<br>
\newcommand{\mc}{\mathcal}<br>
\newcommand{\mf}{\mathfrak}<br>
\newcommand{\and}{\wedge}<br>
\newcommand{\or}{\vee}<br>
\newcommand{\es}{\emptyset}<br>
\newcommand{\a}{\alpha}<br>
\newcommand{\t}{\tau}<br>
\newcommand{\T}{\Theta}<br>
\newcommand{\D}{\Delta}<br>
\newcommand{\o}{\omega}<br>
\newcommand{\O}{\Omega}<br>
\newcommand{\x}{\xi}<br>
\newcommand{\z}{\zeta}<br>
\newcommand{\fa}{\forall}<br>
\newcommand{\ex}{\exists}<br>
\newcommand{\X}{\mc{X}}<br>
\newcommand{\Y}{\mc{Y}}<br>
\newcommand{\Z}{\mc{Z}}<br>
\newcommand{\P}{\Psi}<br>
\newcommand{\y}{\psi}<br>
\newcommand{\p}{\phi}<br>
\newcommand{\l}{\lambda}<br>
\newcommand{\B}{\mb{B}}<br>
\newcommand{\m}{\times}<br>
\newcommand{\N}{\mb{N}}<br>
\newcommand{\R}{\mb{R}}<br>
\newcommand{\I}{\mb{I}}<br>
\newcommand{\H}{\mb{H}}<br>
\newcommand{\e}{\varepsilon}<br>
\newcommand{\Env}{\mf{E}}<br>
\newcommand{\expt}[2]{\mb{E}_{#1}\left[#2\right]}<br>
\newcommand{\set}[1]{\left\{#1\right\}}<br>
\newcommand{\par}[1]{\left(#1\right)}<br>
\newcommand{\vtup}[1]{\left\langle#1\right\rangle}<br>
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}<br>
\newcommand{\inv}[1]{{#1}^{-1}}<br>
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}<br>
\newcommand{\dom}[1]{_{\mid #1}}<br>
\newcommand{\df}{\overset{\mathrm{def}}{=}}<br>
\newcommand{\M}{\mc{M}}<br>
\newcommand{\up}[1]{^{(#1)}}<br>
\newcommand{\Dt}{{\Delta t}}<br>
\newcommand{\tr}{\rightarrowtail}<br>
\newcommand{\tx}{\prec}<br>
\newcommand{\qed}{\ \ \blacksquare}<br>
\newcommand{\c}{\overline}<br>
\newcommand{\A}{\mf{A}}<br>
\newcommand{\cA}{\c{\mf{A}}}<br>
\newcommand{\dg}{\dagger}<br>
\require{cancel}<br>
$$</p>
<p>$\newcommand{\sys}[2]{\left[#2\right]_{#1}}$</p>
<p>I will apply to abstract physics the same information algebra I introduced in <a href="https://danabo.github.io/blog/posts/bayesian-information-theory/#defining-information">Bayesian information theory#defining-information</a> and further developed in <a href="https://danabo.github.io/blog/posts/information-algebra/">Information Algebra</a>. <em>Bayesian</em> information is just information from the perspective of an agent that may have or not have particular information. Below, I will introduce the notion of a physical system having or not having information about itself or other systems (whether or not it has <em>agenty</em> attributes), and the same information algebra will apply. The only difference is a shift from the 1st person to 3rd person perspective.</p>
<h1 id="information-preliminaries">Information Preliminaries</h1>
<p>For sets $\O$ and $A \subseteq \O$,</p>
<p>$$<br>
\O\tr \A<br>
$$</p>
<p><em>is</em> information. This denotes the narrowing down of possibility space $\O$ to possibility space $A$ containing the <em>true</em> possibility $\o^*\in A$.</p>
<p>The information $\O\tr \A$ implies a domain restriction. For some other set $B \subseteq \O$,</p>
<p>$$<br>
B\dom{A} \df B \cap A<br>
$$</p>
<p>is the domain restriction operation on $B$, which makes clear which set is the domain and which set is being restricted.</p>
<p>Let $\mf{P}$ be a partition of $\O$. Then</p>
<p>$$<br>
\begin{aligned}<br>
\mf{P}\dom{A} &amp;\df \set{p\dom{A} \mid p\in\mf{P}} \\<br>
&amp;= \set{p\cap A \mid p\in\mf{P}}<br>
\end{aligned}<br>
$$</p>
<p>is the domain restriction of partition $\mf{P}$ to domain $A$, s.t. $\bigcup\mf{P} = A$.</p>
<h1 id="information-theory-of-systems">Information Theory Of Systems</h1>
<p>I will use the abstraction of physics that I introduced in <a href="https://danabo.github.io/blog/posts/causality-for-physics/#abstract-physics">Causality For Physics#abstract-physics</a>. Let $\O$ be a set of possible states and $\t_\Dt : \O\to\O$, $\Dt\in\R$, be a family of bijective time-evolution functions on $\O$. In general, time-evolution forms the group $(\set{\t_\Dt \mid \Dt\in\R}, \circ)$, where $\t_{\Dt+\Dt'} = \t_\Dt\circ\t_{\Dt'}$ and $\t_{-\Dt}=\t^{-1}_\Dt$, and $\t_0:\o\mapsto\o$ is the identity function.</p>
<p>I will regard $\O$ as the state-space of an entire universe (i.e. a closed system). The universe may contain any number of systems labeled &ldquo;A&rdquo;, &ldquo;B&rdquo;, &ldquo;C&rdquo;, etc., with respective state-spaces $A, B, C,\dots$, so that $\O\subseteq A\m B\m C\m \dots$ and states are tuples, $\o = (a, b, c, \dots) \in \O$. Then the time-evolution function</p>
<p>$$\t_\Dt : (a, b, c, \dots) \mapsto \t_\Dt(a, b, c, \dots)$$</p>
<p>jointly time-evolves all the systems simultaneously, which allows $\t_\Dt$ to incorporate arbitrary interactions between systems. This also means that the time evolution of, say, system A, depends on not just A&rsquo;s state, but the state of all systems, i.e. the universe&rsquo;s state $\o$.</p>
<p>There is an alternative way to describe systems using partitions. Let $\mf{A}, \mf{B}, \mf{C},\dots$ each be a partition on $\O$. Partition $\mc{A}$ is a representation of system A&rsquo;s state space, partition $\mf{B}$ is a representation of system B&rsquo;s state space, and so on. I&rsquo;ll denote elements of a partition with lowercase letters, e.g. $a\in\mf{A},\ b\in\mf{B},\ c\in\mf{C},\ \dots$</p>
<p>In the state-space view, $a\in A$ is a featureless element of which universal states $\o$ are composed. In the partition view, on the other hand, $a\in\mf{A}$ is a subset of $\O$, corresponding to all the states of the universe that are indistinguishable to system A, i.e. the set of all universal states $\o\in\O$ for which system A is in the same state $a$. You can think of $\mf{A}$ as the set of equivalence classes for the relation &ldquo;same state from system A&rsquo;s perspective&rdquo;. Let $\sys{\mf{A}}{\o}$ be the equivalence class containing $\o$, i.e. $\sys{\mf{A}}{\o} = a\in\mf{A}$ s.t. $\o\in a$.</p>
<p>From here on I will treat the states of systems as subsets of $\O$, and the state spaces of systems will be partitions of $\O$.</p>
<h2 id="systems-have-information">Systems Have Information</h2>
<p>Suppose the universe is in state $\o\up{t}\in\O$ at time $t$. Then system A, with state space $\mf{A}$, is in state $a\up{t} = \sys{\mf{A}}{\o\up{t}}$. From system A&rsquo;s perspective, $a\up{t}$ is the set of states the universe can be in.</p>
<p>System A has the information $\O \tr a\up{t}$, which reads &ldquo;$\O$ is narrowed down to $a\up{t}$.&rdquo; System A possesses this information in a purely physical sense. System A need not have awareness or understanding that it posses information, or even the capacity for awareness or understanding of anything. Merely as a physical description, I define any system with state space $\mf{A}$ to have the information $\O \tr \sys{\mf{A}}{\o\up{t}}$ at time $t$.</p>
<p>

  <img src="https://danabo.github.io/blog/Pasted%20image%2020210514153113.png" alt="">
<br>
As a corollary, the universe has the information $\O\tr\set{\o\up{t}}$ at time $t$. The universe can be viewed as the <em>supersystem</em> with state space $\mf{O} = \set{\set{\o} \mid \o\in\O}$, the singleton partition. In this sense, the universe has total information, corresponding to narrowing down to a single state.</p>
<p>This definition of what it means for a system to have information will allow us to talk about what information the system has about itself and other systems at various times, as well as the information the system gains or losses about them as time evolves.</p>
<h2 id="interactions">Interactions</h2>
<p>If $a\up{t}$ is the state of system A at time $t$, then $\t_{\Dt}(a\up{t})$ is NOT the time-evolution of system A&rsquo;s state. A&rsquo;s state at time $t+\Dt$ is $a\up{t+\Dt}=\sys{\mf{A}}{\o\up{t+\Dt}}$ where $\o\up{t+\Dt} = \t_\Dt(\o\up{t})$. If $\t_{\Dt}(a\up{t}) \neq a\up{t+\Dt}$, then system A <strong>interacted</strong> with another system (or the environment) in the time interval $(t, t+\Dt)$. Note that by necessity, $\o\up{t+\Dt} \in \t_{\Dt}(a\up{t})\cap a\up{t+\Dt}$.</p>
<p>

  <img src="https://danabo.github.io/blog/Pasted%20image%2020210514153146.png" alt="">
<br>
Then what is $\t_{\Dt}(a\up{t})$? At time $t$, system A has information not just about time $t$, but about all other points in time. Specifically, at time $t$, system A has the information $\O\tr \t_{\Dt}(a\up{t})$ about time $t+\Dt$. It is important to distinguish between the time when a system has information and the time it has information about. So at time $t+\Dt$, system A has information $\O\tr \t_{-\Dt}(a\up{t+\Dt})$ about time $t$.</p>
<p>If $\t_{\Dt}(a\up{t}) \neq a\up{t+\Dt}$, then system A does not have complete information about its own future state, which is the necessary result of interaction. Furthermore, $\t_{\Dt}(a\up{t}) \neq a\up{t+\Dt} \iff \t_{-\Dt}(a\up{t+\Dt}) \neq a\up{t}$, and so after system A interacted, it has forgotten information about its previous state at time $t$. That is to say, interaction causes a system to lose information about its past.</p>
<p>

  <img src="https://danabo.github.io/blog/Pasted%20image%2020210514153301.png" alt="">
</p>
<h1 id="conservation-of-information">Conservation Of Information</h1>
<p>Recapping <a href="https://danabo.github.io/blog/posts/information-algebra/">Information Algebra</a>, suppose we are given some measure $\mu$ on $\O$. This measure need not be normalized. Then for measureable set $R\subseteq\O$, the quantity of the information $\O\tr R$ is given by</p>
<p>$$<br>
h(\O\tr R) = h(R) = \lg\par{\frac{\mu(\O)}{\mu(R)}}\,,<br>
$$</p>
<p>where $h(R)$ is the <a href="https://en.wikipedia.org/wiki/Information_content"target="_blank">information content</a> (or pointwise entropy) of $R$, and $h(\O\tr R)$ is my own shorthand notation to make it clear what information is being quantified.</p>
<p><strong>Conservation of information</strong> is the property of any bijective time-evolution, whereby the information $\O\tr \t_\Dt(a\up{t})$ is enough to recover the information $\O\tr a\up{t}$, because $a\up{t} = \t^{-1}_\Dt(\t_\Dt(a\up{t}))$, for all $\Dt\in\R$. That is to say, time-evolution of arbitrary state sets $R\subseteq\O$ does not destroy the information $\O\tr R$ (this is distinct from the time-evolution of systems which, as we saw, can lose information).</p>
<p><strong>Conservation of information quantity</strong> is a property of <strong>measure-preserving</strong> time-evolution. Let $\mu$ additionally be a $\t_\Dt$-<a href="https://en.wikipedia.org/wiki/Invariant_measure"target="_blank">invariant measure</a> on $\O$, i.e. $\mu(\t_\Dt^{-1}(A)) = \mu(A)$ for all measurable $A\subseteq \O$ and for all $\Dt\in\R$. Because $\t_\Dt$ is bijective, <a href="https://encyclopediaofmath.org/wiki/Invariant_measure"target="_blank">this is equivalent to</a> requiring $\mu(\t_\Dt(A)) = \mu(A)$. Then $h(\O\tr a\up{t}) = h(\O\tr \t_\Dt(a\up{t}))$ for all $t, \Dt\in\R$.</p>
<p>Conservation of information quantity is a stronger property that requires a $\t$-invariant measure in addition to bijective time-evolution. In classical mechanics, <a href="https://en.wikipedia.org/wiki/Liouville%27s_theorem_%28Hamiltonian%29"target="_blank">Liouville&rsquo;s theorem</a> shows that any Newtonian time-evolution preserves uniform measures on phase space. This result is sometimes referred to as <em>conservation of information</em> by physicists, who are referring to conservation of information quantity within my nomenclature. For details, see</p>
<ul>
<li><a href="https://physicstravelguide.com/theorems/liouvilles_theorem"target="_blank">Liouville&rsquo;s theorem - physicstravelguide.com</a> (Davis &amp; Schwichtenberg)</li>
<li><a href="https://theoreticalminimum.com/courses/statistical-mechanics/2013/spring/lecture-1"target="_blank">Entropy and conservation of information - theoreticalminimum.com</a> - (Susskind)</li>
<li><a href="http://philsci-archive.pitt.edu/15985/1/gibbsliouville.pdf"target="_blank">On the Gibbs-Liouville theorem in classical mechanics</a> (Henriksson)</li>
<li><a href="https://arxiv.org/abs/2004.11569"target="_blank">Hamiltonian mechanics is conservation of information entropy</a> (Carcassi &amp; Aidala)</li>
</ul>
<h1 id="information-about-systems">Information About Systems</h1>
<p>Suppose there are at least two systems, A and B, with respective state spaces $\mf{A}$ and $\mf{B}$. Let system A be in state $a\up{t}$ at time $t$. System A (at $t$) has the information $\O\tr \t_{\Dt}(a\up{t})$ about time $t+\Dt$. What information does A have about B&rsquo;s state at time $t+\Dt$?</p>
<p>In <a href="https://danabo.github.io/blog/posts/information-algebra/#mutual-information">Information Algebra#mutual-information</a>, I showed that pointwise mutual information (PMI) quantifies &ldquo;information about&rdquo;. Specifically,</p>
<p>$$<br>
i(a\up{t}, b) = i(b, a\up{t}) = \lg\par{\frac{\mu(a\up{t}\cap b)\mu(\O)}{\mu(a\up{t})\mu(b)}}<br>
$$</p>
<p>quantifies how much information system A, being in state $a\up{t}$, has about whether system B is in state $b\in\mf{B}$ at time $t$.</p>
<p>

  <img src="https://danabo.github.io/blog/Pasted%20image%2020210514153829.png" alt="">
<br>
If $i(a\up{t}, b) = 0$, then $a\up{t}$ and $b$ are <strong>orthogonal</strong> states. State spaces $\mf{A}$ and $\mf{B}$ are called orthogonal iff $i(a,b) = 0$ for all $a\in\mf{A}$ and $b\in\mf{B}$. Usually, systems are defined to be orthogonal, meaning that the state of one system can be chosen totally independently of the state of the other. Non-orthogonal systems have information about each other <em>by construction</em>, regardless of their mutual time-evolution. On the other hand, orthogonal systems always have zero information about each other at the present moment. That is to say, $i(a\up{t}, b) = 0$ for all $\mf{B}$, i.e. system A has no information about the state of system B at time $t$. However, system A may have information about system B&rsquo;s past or future state.</p>
<p>$\O\tr \t_\Dt(a\up{t})$ is the information system A has <em>at time $t$</em> about time $t+\Dt$. Then it follows that $i(\t_\Dt(a\up{t}), b)$ is the quantity of information system A has, <em>at time $t$</em>, about whether system B is in state $b$ <em>at time $t+\Dt$</em>. If $\t_\Dt(a\up{t}) \in b$, then $i(\t_\Dt(a\up{t}), b) = h(b) = h(\O\tr b)$ and system A (at time $t$) has certainty that system B is in state $b$ at time $t+\Dt$. On the other hand, if $b \in \t_\Dt(a\up{t})$, then $i(\t_\Dt(a\up{t}), b) = h(\t_\Dt(a\up{t})) = h(\O\tr \t_\Dt(a\up{t}))$ which is just the total quantity of information that system A has about time $t+\Dt$.</p>
<p>To fully lay out what information system A has (at time $t$) about system B&rsquo;s state at time $t+\Dt$, we need to look at all the quantities of information A has about every state $b\in\mf{B}$, which can be given as the vector</p>
<p>$$<br>
\vtup{i(\t_\Dt(a\up{t}), b) \mid b\in\mf{B}}\,.<br>
$$</p>
<p>Note that $i(\t_\Dt(a\up{t}), b)$ can be negative (and has no lower bound), which means that system A has needs more quantity of information than $h(\O\tr b)$ to have certainty that system B is in state $b$. If system A has information that rules out state $b$ with certainty, i.e. $b\cap \t_\Dt(a\up{t}) = \es$, then $i(\t_\Dt(a\up{t}), b) = -\infty$. <a href="https://danabo.github.io/blog/posts/information-algebra/#mutual-information">Information Algebra#mutual-information</a> goes into further detail about the interpretation of PMI quantities.</p>
<h2 id="information-gain">Information Gain</h2>
<p>The difference</p>
<p>$$<br>
i(\t_{t'-t-\Dt}(a\up{t+\Dt}), b) - i(\t_{t'-t}(a\up{t}), b)<br>
$$</p>
<p>is <strong>information gain</strong>, i.e. the information system A gained (or lost if negative) about whether system B is in state $b$ at time $t'$ due to the time-evolution of system A&rsquo;s state over the time interval $(t,t+\Dt)$.</p>
<p>Likewise, $i(\t_{t'-t-\Dt}(a\up{t+\Dt}), a) - i(\t_{t'-t}(a\up{t}), a)$ is system A&rsquo;s information gain (or loss) about itself, specifically whether it is in state $a$ at time $t'$.</p>
<h1 id="sum-conservation-laws">Sum-Conservation Laws</h1>
<p>Let $\O\tr R$ be the information some unspecified system at some unspecified time has about some other time. Then that system has the quantity of information $h(\O\tr R)$.</p>
<p>Let $b\in\mf{B}$ be a possible state of system B. Suppose $i(b, R) &lt; h(\O\tr R)$. Then you might ask, &ldquo;where did the remaining information go?&rdquo; That is to say, does it make sense to think that the complete quantity of information $h(\O\tr R)$ should be divided among information about various things? Could we then have the information about everything add up to $h(\O\tr R)$, analogous to how conservation of mass and energy in physics results in reactions or interactions s.t. the energies and masses of the outputs add up to the input energy and mass?</p>
<p>Information does not quite work like this. For instance, the sum of the vector $\vtup{i(\t_\Dt(a\up{t}), b) \mid b\in\mf{B}}$ need not be $h(R)$. This is evident when you consider that it&rsquo;s possible for the system with information $\O\tr R$ to have the information quantity $h(R)$ about $b\in\mf{B}$ and about $b'\in\mf{B}$, if both $b$ and $b'$ are contained within $R$. Then at least two entries in the vector are each $h(R)$. Furthermore, entries can be arbitrarily negative, and even $-\infty$ as we saw above.</p>
<p>I have found two ways to achieve something like a sum-conservation law for information quantity. One way uses pointwise quantities, and the other way uses expected quantities, i.e. entropy and mutual information.</p>
<h2 id="pointwise">Pointwise</h2>
<p>Suppose $i(b, R) &lt; h(\O\tr R)$. If we &ldquo;shrank&rdquo; $b$ down by intersecting with some other set $c$ such that $b \cap c \in R$, then we&rsquo;d have  $i(b\cap c, R) = h(\O\tr R)$. Writing $i(b\cap c, R)$ as a sum involving $i(b, R)$ gives us a sum-conservation law:</p>
<p>$$<br>
\begin{aligned}<br>
i(b \cap c, R) &amp;= i(b, R) + i(c, R) - i(b, c, R) \\<br>
&amp;= i(b, R) + i(c, R) - i(b, c) + i(b, c \mid R) \\<br>
&amp;= h(\O\tr R)\,.<br>
\end{aligned}<br>
$$</p>
<p>This is easy to check using the definition of PMI and $i(b, c, R)\df i(b, c) - i(b, c \mid R)$, and</p>
<p>$$<br>
i(b, c \mid R) \df \lg\par{\frac{\mu(b\cap c\cap R)\mu(R)}{\mu(b\cap R)\mu(c \cap R)}}\,.<br>
$$</p>
<p>If $c$ is chosen such that $b$ and $c$ are orthogonal, i.e. $i(b,c)=0$, and plugging in $h(\O\tr R)$ for $i(b \cap c, R)$, then we have the simpler form</p>
<p>$$<br>
h(\O\tr R) = i(b, R) + i(c, R) + i(b, c \mid R)\,,<br>
$$</p>
<p>which does not involve the information quantity $i(b,c)$ due to redundancy between the choice of $b$ and $c$.</p>
<p>Note that the &ldquo;$i$&rdquo; quantities may be negative, but $h(\O\tr R)$ is always positive. Thus at least one of these terms is positive.</p>
<p>It is possible for the information $\O\tr R$ to not contain information (or contain negative information) about $b$ or $c$. Then all of the positive information quantity goes into the term $i(b, c \mid R)$, which can be thought of as the quantity of information $\O\tr b$ has about whether $c$ is the case, given the smaller state space $R$.</p>
<p>

  <img src="https://danabo.github.io/blog/Pasted%20image%2020210514153229.png" alt="">
</p>
<h3 id="information-about-more-than-two-sets">Information about more than two sets</h3>
<p>Suppose $i(b\cap c, R) &lt; h(\O\tr R)$. We can repeat the process above again by choosing a third set $a$ s.t. $i(a\cap b\cap c, R) = h(\O\tr R)$. In general, there is an $n$-way relationship.</p>
<p>Let $x_1,\dots,x_n,R \subseteq \O$ be arbitrary sets. Then,</p>
<p>$$<br>
i(x_1\cap\dots\cap x_n,R) = i(x_1\cap\dots\cap x_{n-1},R) + i(x_n, R \mid x_1\cap\dots\cap x_{n-1})\,.<br>
$$</p>
<p>Using the identity</p>
<p>$$<br>
\begin{aligned}<br>
&amp; i(x_{n-k+1}, \dots, x_n, R \mid x_1\cap\dots\cap x_{n-k}) \\<br>
&amp;\quad\,\, = \,\,i(x_{n-k+1}, \dots, x_n, R \mid x_1\cap\dots\cap x_{n-k-1}) \\<br>
&amp;\qquad\quad- i(x_{n-k}, \dots, x_n, R \mid x_1\cap\dots\cap x_{n-k-1})<br>
\end{aligned}<br>
$$</p>
<p>we can recursively expanding out these terms to produce an expression for $i(x_1\cap\dots\cap x_n,R)$ entirely composed of non-conditional PMI terms, which can be further expanded out using</p>
<p>$$<br>
i(x_1, \dots, x_n) \df i(x_1, \dots, x_{n-1}) - i(x_1, \dots, x_{n-1} \mid x_n)\,.<br>
$$</p>
<p>For case of three sets $a,b,c$, we have</p>
<p>$$<br>
\begin{aligned}<br>
&amp; i(a\cap b\cap c,R) \\<br>
&amp;\quad\,=\, i(a, R) + i(b, R) + i(c, R) \\<br>
&amp;\qquad\,\,- i(a, b, R) - i(a, c, R) - i(b, c, R) \\<br>
&amp;\qquad\,\,+ i(a,b,c,R)<br>
\end{aligned}<br>
$$</p>
<p>where</p>
<p>$$<br>
\begin{aligned}<br>
i(a,b,c,R) &amp;= i(a,b,c) - i(a,b,c\mid R)\\<br>
&amp;= i(a,b) - i(a,b \mid c) - i(a,b \mid R) + i(a,b \mid c\cap R)\,.<br>
\end{aligned}<br>
$$</p>
<p>If $a,b,c$ are all mutually orthogonal, i.e. $i(a,b) = i(a,c) = i(b,c) = i(a,b,c) = 0$, then</p>
<p>$$<br>
\begin{aligned}<br>
&amp; i(a\cap b\cap c,R) \\<br>
&amp;\quad\,=\, i(a, R) + i(b, R) + i(c, R) \\<br>
&amp;\qquad\,\,+ i(a,b \mid R) + i(b,c \mid R) + i(a,b \mid c\cap R)\,.<br>
\end{aligned}<br>
$$</p>
<h2 id="expectation">Expectation</h2>
<p>It would be nice if there was a single quantity describing $\mf{B}\tr\mf{B}\dom{R}$, the narrowing down of partition $\mf{B}$ to domain $R$, analogous to $h(\O\tr R)$ for individual sets. I will motivate such a quantity from a few special cases.</p>
<p>If $P\subseteq 2^\O$ is some set of sets, then let $\mu(P) = \mu(\bigcup P)$. Then for partition $\mf{B}$ of $\O$, we have $\mu(\mf{B}) = \mu(\bigcup\mf{B}) = \mu(\O)$. Let&rsquo;s define the quantity $h(\mf{B}\tr \mf{B}\dom{R})$ on a few special cases.</p>
<p>Consider $\mf{B}\tr \set{b}$ for some $b\in\mf{B}$.<br>


  <img src="https://danabo.github.io/blog/Pasted%20image%2020210514154831.png" alt="">
</p>
<p>Let</p>
<p>$$<br>
\begin{aligned}<br>
h(\mf{B}\tr \set{b}) &amp;= \lg\par{\frac{\mu(\mf{B})}{\mu\set{b}}} \\<br>
&amp;= \lg\par{\frac{\mu(\O)}{\mu(b)}} \\<br>
&amp;= h(\O\tr b)\,.<br>
\end{aligned}<br>
$$</p>
<p>Then the quantity of information due to narrowing down the partition $\mf{B}$ to one of its elements $b$ is equal to the quantity of information due to narrowing down the possibility space $\O$ to $b$.</p>
<p>In general, let $\mf{B}' \subseteq \mf{B}$. Then</p>
<p>$$<br>
\begin{aligned}<br>
h(\mf{B}\tr \mf{B}') &amp;= \lg\par{\frac{\mu(\mf{B})}{\mu(\mf{B}')}} \\<br>
&amp;= \lg\par{\frac{\mu(\O)}{\mu(\bigcup \mf{B}')}} \\<br>
&amp;= h\par{\O\tr \bigcup \mf{B}'}\,.<br>
\end{aligned}<br>
$$</p>
<p>

  <img src="https://danabo.github.io/blog/Pasted%20image%2020210514154938.png" alt="">
<br>
One more special case: suppose $\mf{B}$ and $\mf{B}\dom{R}$ are <strong>uniform</strong>, meaning that $\mu(b) = \mu(b')$ or $0$ for all $b,b'\in\mf{B}$, and likewise for $\mf{B}\dom{R}$ (which may contain the empty set).</p>
<p>Then</p>
<p>$$<br>
\begin{aligned}<br>
h(\mf{B}\tr \mf{B}\dom{R}) &amp;= h(\O\tr b) - h(R \tr b\cap R)\\<br>
&amp;= h(\O\tr R) - h(b \tr b\cap R)\\<br>
&amp;= i(b, \bigcup \mf{B}\dom{R}) \\<br>
&amp;= i(b, R)<br>
\end{aligned}<br>
$$</p>
<p>for some $b\in\mf{B}$ s.t. $b\cap R \neq \es$. If $R$ reduces each $b\in\mf{B}$ by the same amount, then $i(b, R) = 0$ for all $b\in\mf{B}$ (i.e. $R$ and $b$ are orthogonal), and so $h(\mf{B}\tr \mf{B}\dom{R}) = 0$, indicating that $\O\tr R$ contains no information about the partition $\mf{B}$.</p>
<p>

  <img src="https://danabo.github.io/blog/Pasted%20image%2020210514155029.png" alt="">


  <img src="https://danabo.github.io/blog/Pasted%20image%2020210514155002.png" alt="">
<br>
Let $\hat{\mf{B}}\dom{R} = \set{b\in\mf{B} \mid b\cap R \neq \es}$ be the subset of $\mf{B}$ containing elements that have non-zero intersection with $R$. We&rsquo;d like $h(\mf{B}\tr \mf{B}\dom{R}) = h(\mf{B}\tr \hat{\mf{B}}\dom{R}) = \lg\par{\frac{\mu(\mf{B})}{\mu(\hat{\mf{B}}\dom{R})}}$. Notice that $\mu(b)/\mu(\bigcup\hat{\mf{B}}\dom{R}) = \mu(b\cap R)/\mu(R)$, which gives us</p>
<p>$$<br>
\mu(\bigcup\hat{\mf{B}}\dom{R}) = \mu(b)\frac{\mu(R)}{\mu(b\cap R)}\,.<br>
$$</p>
<p>Plugging in, we get $\lg\par{\frac{\mu(\mf{B})}{\mu(\hat{\mf{B}}\dom{R})}} = \lg\par{\frac{\mu(\O)}{\mu(b)\frac{\mu(R)}{\mu(b\cap R)}}} = i(b, R)$.</p>
<hr>
<p>In the more general case $\mf{B}$ and $\mf{B}\dom{R}$ are not uniform partitions. We will need some kind of averaging operation over $i(b,R)$ for $b\in\mf{B}$ that reduces to our special cases above, and zeros out all the $i(b,R) = -\infty$ terms where $b\cap R = \es$. Taking the expectation w.r.t. $\mu(\cdot \mid R)$ fulfills both requirements.</p>
<p>Let</p>
<p>$$<br>
\expt{x\in\mf{X}}{f(x)} = \frac{1}{\mu(\bigcup\mf{X})} \sum_{x\in\mf{X}} \mu(x)\,f(b)<br>
$$</p>
<p>for discrete $\mf{X}$ and</p>
<p>$$<br>
\expt{x\in\mf{X}}{f(x)} = \frac{1}{\mu(\bigcup\mf{X})}\int_{x\in\mf{X}} \rho(x)\,f(b)\,\mathrm{d}x<br>
$$</p>
<p>for continuous $\mf{X}$, where $\rho$ is the density function for measure $\mu$. These quantities are normalized by $\mu(\bigcup\mf{X})$ where $\bigcup\mf{X}$ is the domain of the partition $\mf{X}$.</p>
<p>Define the mutual information between partitions $\mf{A}$ and $\mf{B}$ as</p>
<p>$$<br>
\I(\mf{A}, \mf{B}) \df \expt{a\in\mf{A},b\in\mf{B}}{i(a,b)}\,,<br>
$$</p>
<p>as well as the conditional mutual information</p>
<p>$$<br>
\I(\mf{A}, \mf{B} \mid R) \df \expt{a\dom{R}\in\mf{A}\dom{R},b\dom{R}\in\mf{B}\dom{R}}{i(a,b\mid R)}\,.<br>
$$</p>
<p>Define the mutual information between partition $\mf{B}$ and set $R$ as</p>
<p>$$<br>
\I(\mf{B}, R) \df \expt{b\dom{R}\in\mf{B}\dom{R}}{i(b,R)}\,.<br>
$$</p>
<p>In the discrete case,</p>
<p>$$<br>
\I(\mf{B}, R) = \sum_{b\in\mf{B}} \frac{\mu(b\cap R)}{\mu(R)}i(b,R)\,.<br>
$$</p>
<p>In general, define</p>
<p>$$<br>
h(\mf{B}\tr \mf{B}\dom{R}) \df \I(\mf{B}, R)\,.<br>
$$</p>
<p>Let $\mf{C}$ be the state space of some other system. Taking the expectation of the pointwise sum-conservation law from above, we get</p>
<p>$$<br>
\begin{aligned}<br>
\I(\mf{B}\otimes\mf{C}, R) &amp;= \I(\mf{B}, R) + \I(\mf{C}, R) - \I(\mf{B}, \mf{C}, R) \\<br>
&amp;= \I(\mf{B}, R) + \I(\mf{C}, R) - \I(\mf{B}, \mf{C}) + \I(\mf{B}, \mf{C} \mid R)\,,<br>
\end{aligned}<br>
$$</p>
<p>where</p>
<p>$$<br>
\mf{B}\otimes\mf{C}\df \set{b\cap c \mid b\in\mf{B} \and c\in\mf{C}}<br>
$$</p>
<p>is the <strong>partition product</strong> of $\mf{B}$ and $\mf{C}$, i.e. the intersection of all pairs of elements of $\mf{B}$ and $\mf{C}$.</p>
<p>Each &ldquo;$\I$&rdquo; in the expression $\I(\mf{B}, R) + \I(\mf{C}, R) - \I(\mf{B}, \mf{C}) + \I(\mf{B}, \mf{C} \mid R)$ is always positive (though 3-way &ldquo;$\I$&rdquo; can be negative). If $\mf{B}$ and $\mf{C}$ are orthogonal, i.e. $\I(\mf{B}, \mf{C}) = 0$, then</p>
<p>$$<br>
\I(\mf{B}\otimes\mf{C}, R) = \I(\mf{B}, R) + \I(\mf{C}, R) + \I(\mf{B}, \mf{C} \mid R)\,,<br>
$$</p>
<p>which is a decomposition of $\I(\mf{B}\otimes\mf{C}, R)$ into positive terms.</p>
<h3 id="environments">Environments</h3>
<p>Let $\mf{B}$ be some state space. A <strong>partition complement</strong> of $\mf{B}$, denoted $\c{\mf{B}}$, satisfies the following properties:</p>
<ol start="0">
<li>$\c{\mf{B}}$ is a partition of $\O$.</li>
<li>$\I(\mf{B}, \c{\mf{B}}) = 0$,</li>
<li>$\mf{B}\otimes\c{\mf{B}} = \mf{O} = \set{\set{\o}\mid\o\in\O}$.</li>
</ol>
<p>In general, $\mf{B}$ does not have a unique complement, and may not have any complement. For example, if $\O = \set{\o_1, \o_2, \o_3}$ and $\mf{B} = \set{\set{\o_1, \o_2}, \set{\o_3}}$, then it is easy to check that $\mf{B}$ has no complement.</p>
<p>If each element of $\mf{B}$ is infinite, then $\mf{B}$ always has a complement. If each element of $\mf{B}$ is finite, then it has a complement if each element has the same cardinality.</p>
<p>We can regard a complement $\c{\mf{B}}$ as the <strong>environment</strong> of system B, which is everything in the universe outside of system B.</p>
<p>Assuming $\mf{B}$ has a complement $\c{\mf{B}}$, then the sum-conservation law from above is</p>
<p>$$<br>
\I(\mf{B}\otimes\c{\mf{B}}, R) = \I(\mf{B}, R) + \I(\c{\mf{B}}, R) + \I(\mf{B}, \c{\mf{B}} \mid R)\,.<br>
$$</p>
<p>A useful identity here is that $\I(\mf{O}, R) = h(\O\tr R)$. Since $\mf{B}\otimes\c{\mf{B}} = \mf{O}$, then we have</p>
<p>$$<br>
h(\O\tr R) = \I(\mf{B}, R) + \I(\c{\mf{B}}, R) + \I(\mf{B}, \c{\mf{B}} \mid R)\,.<br>
$$</p>
<hr>
<p><em><strong>Proof</strong></em> that $\I(\mf{O}, R) = h(\O\tr R)$.</p>
<p>Let $\mf{X}$ be a partition of $\O$ s.t. for each $x\in\mf{X}$, either $x\cap R = \es$ or $x$. That is to say, there is some subset $Y \subseteq \mf{X}$ s.t. $R = \bigcup Y$. Then $Y = \mf{X}\dom{R}$.</p>
<p>$$<br>
\begin{aligned}<br>
\I(\mf{X}, R) &amp;= \sum_{x\in\mf{X}} \mu(x \mid R) \lg\par{\frac{\mu(x\cap R)\mu(\O)}{\mu(x)\mu(R)}} \\<br>
&amp;= \sum_{x\in \mf{X}\dom{R}} \mu(x \mid R) \lg\par{\frac{\cancel{\mu(x)}\mu(\O)}{\cancel{\mu(x)}\mu(R)}} \\<br>
&amp;= \lg\par{\frac{\mu(\O)}{\mu(R)}} \\<br>
&amp;= h(\O\tr R)\,.<br>
\end{aligned}<br>
$$</p>
<p>Since $\mf{O}$ is the singleton partition, it satisfies the condition above for all $R\subseteq \O$. Thus $\I(\mf{O}, R) = h(\O\tr R)$. $\qed$</p>
<hr>
<p>Combining conservation of information quantity,</p>
<p>$$<br>
h(\O\tr R) = h(\O\tr \t_\Dt(R))\,,<br>
$$</p>
<p>with the sum-conservation law,  we get</p>
<p>$$<br>
\begin{aligned}<br>
&amp; \I(\mf{B}, R) + \I(\c{\mf{B}}, R) + \I(\mf{B}, \c{\mf{B}} \mid R) \\<br>
=\,\, &amp; \I(\mf{B}, \t_\Dt(R)) + \I(\c{\mf{B}}, \t_\Dt(R)) + \I(\mf{B}, \c{\mf{B}} \mid \t_\Dt(R))\,.<br>
\end{aligned}<br>
$$</p>
<p>Consider system A with state space $\A$, and its environment with state space $\cA$. At time $t$, system A is in state $a\up{t}\in\mf{A}$ and has the information $\O\tr a\up{t}$ about time $t$. Since $\I(\A, a\up{t}) = h(\O\tr a\up{t})$, then $\I(\cA, a\up{t}) + \I(\A, \cA \mid a\up{t}) = 0$.</p>
<p>Then our conservation law reduces to</p>
<p>$$<br>
\begin{aligned}<br>
\I(\A, a\up{t}) &amp;= \I(\A, \t_\Dt(a\up{t})) + \I(\cA, \t_\Dt(a\up{t})) + \I(\A, \cA \mid \t_\Dt(a\up{t})) \\<br>
&amp;= h(\O\tr a\up{t}) \,.<br>
\end{aligned}<br>
$$</p>
<p>If $\I(\A, \t_\Dt(a\up{t})) = h(\O\tr \t_\Dt(a\up{t})) = h(\O\tr a\up{t})$, then $\I(\cA, \t_\Dt(a\up{t})) + \I(\A, \cA \mid \t_\Dt(a\up{t})) = 0$ and system A (at time $t$) has no information about the environment at time $t+\Dt$. Since all the terms are positive, for system A to have information about the future environment, system A must have less than complete information about its own future state.</p>
<p>A tricky case to be aware of is when $\I(\A, \t_\Dt(a\up{t})) = 0$ and $\I(\cA, \t_\Dt(a\up{t})) = 0$. This would seem to be saying that system A has no information about its own future state and the environment&rsquo;s future state. It would then seem that the information quantity $h(\O\tr a\up{t})$ simply disappeared and was not conserved. That quantity went into the third term, $\I(\A, \cA \mid \t_\Dt(a\up{t})) = h(\O\tr a\up{t})$, which indicates that system A becomes highly correlated with its environment in the future.</p>
<p>

  <figure>
    
    <img src="https://danabo.github.io/blog/Pasted%20image%2020210514160217.png" alt="">
    <figcaption>$R=a\in\A$. Then $\I(\A,R) = h(\O\tr R)$ and $\I(\cA,R) = 0$, $\I(\A,\cA \mid R) = 0$.</figcaption>
  </figure>
<br>


  <figure>
    
    <img src="https://danabo.github.io/blog/Pasted%20image%2020210514160232.png" alt="">
    <figcaption>$R=a^\dg\in\cA$. Then $\I(\cA,R) = h(\O\tr R)$ and $\I(\A,R) = 0$, $\I(\A,\cA \mid R) = 0$.</figcaption>
  </figure>
<br>


  <figure>
    
    <img src="https://danabo.github.io/blog/Pasted%20image%2020210514160247.png" alt="">
    <figcaption>$\I(\A,R)$ and $\I(\cA,R)$ are both non-zero, and $\I(\A,\cA \mid R) = 0$, meaning $\A$ and $\cA$ are still orthogonal when restricted to the domain $R$.</figcaption>
  </figure>
<br>


  <figure>
    
    <img src="https://danabo.github.io/blog/Pasted%20image%2020210514160256.png" alt="">
    <figcaption>$\I(\A,R) = 0$ and $\I(\cA,R) = 0$, since restricting either partition to the domain $R$ still tells you nothing about the other partition. However, $\I(\A,\cA \mid R) = h(\O\tr R)$, meaning $\A$ and $\cA$ restricted to the domain $R$ are maximally redundant, i.e. given $R$ and some $a\in\mf{A}$, you can uniquely determine $a^\dg\in\mf{A}$, and vice versa.</figcaption>
  </figure>
</p>
</article><section class="article labels"><a class="tag" href=https://danabo.github.io/blog/tags/physics/>physics</a><a class="tag" href=https://danabo.github.io/blog/tags/information/>information</a></section>
</div><nav id="TableOfContents">
  <ol>
    <li><a href="#information-preliminaries">Information Preliminaries</a></li>
    <li><a href="#information-theory-of-systems">Information Theory Of Systems</a>
      <ol>
        <li><a href="#systems-have-information">Systems Have Information</a></li>
        <li><a href="#interactions">Interactions</a></li>
      </ol>
    </li>
    <li><a href="#conservation-of-information">Conservation Of Information</a></li>
    <li><a href="#information-about-systems">Information About Systems</a>
      <ol>
        <li><a href="#information-gain">Information Gain</a></li>
      </ol>
    </li>
    <li><a href="#sum-conservation-laws">Sum-Conservation Laws</a>
      <ol>
        <li><a href="#pointwise">Pointwise</a>
          <ol>
            <li><a href="#information-about-more-than-two-sets">Information about more than two sets</a></li>
          </ol>
        </li>
        <li><a href="#expectation">Expectation</a>
          <ol>
            <li><a href="#environments">Environments</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>
</nav></div>
<div class="article bottom"><section class="article navigation"><p><a class="link" href="https://danabo.github.io/blog/posts/information-algebra/"><span class="iconfont icon-article"></span>Information Algebra</a></p></section></div></section><section id="footer"><div class="footer-wrap">
    <p class="copyright">Â©2021 Daniel Abolafia.</p>
    <p class="powerby"><span>Powered&nbsp;by&nbsp;</span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span>&nbsp;&amp;&nbsp;</span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p></div></section><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ 
                tex2jax: { 
                    inlineMath: [['$','$'], ['\\(','\\)']] 
                },

                "HTML-CSS": {
                    preferredFont: "TeX",
                    availableFonts: ["TeX"]
                }
            });
        </script></body>

</html>